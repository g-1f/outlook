{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import random\n",
    "import time\n",
    "\n",
    "# System prompt for the portfolio manager\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "You must respond in the following XML format:\n",
    "\n",
    "<macro state>\n",
    "...\n",
    "</macro state>\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<positioning>\n",
    "...\n",
    "</positioning>\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Environment Definition\n",
    "# =============================================================================\n",
    "\n",
    "class MacroTradingEnv:\n",
    "    \"\"\"\n",
    "    Environment for training a macro trading agent\n",
    "    Handles market state, position tracking, and reward calculation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, window_size: int = 7):\n",
    "        self.df = df.copy()\n",
    "        self.position = 0.0  # -1 (full short) to 1 (full long)\n",
    "        self.current_step = 0\n",
    "        self.window_size = window_size\n",
    "        self.headline_window = deque(maxlen=window_size)\n",
    "        self.action_history = deque(maxlen=5)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['headline', 'returns']\n",
    "        for col in required_cols:\n",
    "            assert col in self.df.columns, f\"DataFrame must contain '{col}' column\"\n",
    "        \n",
    "        # Initialize action history with zeros\n",
    "        for _ in range(5):\n",
    "            self.action_history.append(0.0)\n",
    "\n",
    "    def reset(self, random_start: bool = True):\n",
    "        \"\"\"Reset environment, optionally to a random starting point\"\"\"\n",
    "        if random_start:\n",
    "            # Ensure we have enough data ahead for a full episode\n",
    "            max_start = len(self.df) - 30\n",
    "            self.current_step = random.randint(self.window_size, max_start) if max_start > self.window_size else self.window_size\n",
    "        else:\n",
    "            self.current_step = self.window_size\n",
    "        \n",
    "        # Reset state\n",
    "        self.position = 0.0\n",
    "        self.headline_window.clear()\n",
    "        self.action_history.clear()\n",
    "        \n",
    "        # Initialize headline window with past headlines\n",
    "        for i in range(self.window_size):\n",
    "            idx = self.current_step - i - 1\n",
    "            if idx >= 0:\n",
    "                self.headline_window.appendleft(self.df.iloc[idx]['headline'])\n",
    "            else:\n",
    "                self.headline_window.appendleft(\"No headline available\")\n",
    "        \n",
    "        # Initialize action history with zeros\n",
    "        for _ in range(5):\n",
    "            self.action_history.append(0.0)\n",
    "            \n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, new_position: float) -> Tuple[Dict, float, bool, Dict]:\n",
    "        \"\"\"Execute position adjustment and return (state, reward, done, info)\"\"\"\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            return self.get_state(), 0.0, True, {'status': 'completed'}\n",
    "        \n",
    "        # Calculate position change\n",
    "        position_change = new_position - self.position\n",
    "        transaction_cost = abs(position_change) * 0.001  # 0.1% friction\n",
    "        \n",
    "        # Update position\n",
    "        self.position = new_position\n",
    "        \n",
    "        # Calculate return (using pre-calculated returns from dataframe)\n",
    "        next_return = self.df.iloc[self.current_step]['returns']\n",
    "        position_return = next_return * self.position\n",
    "        \n",
    "        # Move to next day\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Update headline window and action history\n",
    "        if self.current_step < len(self.df):\n",
    "            self.headline_window.append(self.df.iloc[self.current_step]['headline'])\n",
    "        self.action_history.append(new_position)\n",
    "        \n",
    "        # Calculate reward (return minus transaction cost)\n",
    "        reward = position_return - transaction_cost\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.current_step >= len(self.df) - 1)\n",
    "        \n",
    "        info = {\n",
    "            'return': position_return,\n",
    "            'transaction_cost': transaction_cost,\n",
    "            'position_change': position_change\n",
    "        }\n",
    "        \n",
    "        return self.get_state(), reward, done, info\n",
    "\n",
    "    def get_state(self) -> Dict:\n",
    "        \"\"\"Return current environment state dictionary\"\"\"\n",
    "        if self.current_step >= len(self.df):\n",
    "            self.current_step = len(self.df) - 1\n",
    "            \n",
    "        current_row = self.df.iloc[self.current_step]\n",
    "        \n",
    "        # Create context dictionary with all technical indicators\n",
    "        context = {}\n",
    "        for col in current_row.index:\n",
    "            # Skip specific columns\n",
    "            if col not in ['headline', 'returns', 'date']:\n",
    "                context[col] = current_row[col]\n",
    "        \n",
    "        return {\n",
    "            'market_context': context,\n",
    "            'headlines': list(self.headline_window),\n",
    "            'position': self.position,\n",
    "            'action_history': list(self.action_history)\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# 2. LLM Trading Agent\n",
    "# =============================================================================\n",
    "\n",
    "class LLMTradingAgent:\n",
    "    \"\"\"\n",
    "    Trading agent that uses a language model to make decisions based on market context\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"facebook/opt-350m\"):\n",
    "        \"\"\"Initialize with a smaller model that works reliably\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize tokenizer and model with trust_remote_code=True\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Generation parameters\n",
    "        self.generation_kwargs = {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"do_sample\": True,\n",
    "        }\n",
    "    \n",
    "    def format_state(self, state: Dict) -> str:\n",
    "        \"\"\"Create prompt from current state\"\"\"\n",
    "        # Format market context\n",
    "        context_str = []\n",
    "        for k, v in state['market_context'].items():\n",
    "            # Format number with appropriate precision\n",
    "            if isinstance(v, (int, float)):\n",
    "                if abs(v) < 0.01:\n",
    "                    formatted_value = f\"{v:.6f}\"\n",
    "                elif abs(v) < 1:\n",
    "                    formatted_value = f\"{v:.4f}\"\n",
    "                else:\n",
    "                    formatted_value = f\"{v:.2f}\"\n",
    "            else:\n",
    "                formatted_value = str(v)\n",
    "                \n",
    "            context_str.append(f\"{k}: {formatted_value}\")\n",
    "        \n",
    "        # Format headlines\n",
    "        headlines_str = \"\\n\".join([f\"- {h}\" for h in state['headlines']])\n",
    "        \n",
    "        # Format previous positions\n",
    "        positions_str = \", \".join([f\"{pos:.2f}\" for pos in state['action_history']])\n",
    "        \n",
    "        # Combine all context\n",
    "        prompt = f\"{SYSTEM_PROMPT.strip()}\\n\\n\"\n",
    "        prompt += \"Market Context:\\n\"\n",
    "        prompt += \", \".join(context_str) + \"\\n\\n\"\n",
    "        prompt += f\"Current Position: {state['position']:.2f}\\n\\n\"\n",
    "        prompt += \"Recent Headlines:\\n\"\n",
    "        prompt += headlines_str + \"\\n\\n\"\n",
    "        prompt += f\"Previous Positions: [{positions_str}]\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def extract_positioning(self, text: str) -> float:\n",
    "        \"\"\"Extract positioning value from XML response\"\"\"\n",
    "        try:\n",
    "            match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "            if match:\n",
    "                position_str = match.group(1).strip()\n",
    "                # Try to extract a float from the text\n",
    "                try:\n",
    "                    # First look for float patterns\n",
    "                    float_pattern = r\"[-+]?\\d*\\.\\d+|\\d+\"\n",
    "                    float_match = re.search(float_pattern, position_str)\n",
    "                    if float_match:\n",
    "                        return float(float_match.group())\n",
    "                    else:\n",
    "                        return float(position_str)\n",
    "                except ValueError:\n",
    "                    print(f\"Could not convert position to float: {position_str}\")\n",
    "                    return 0.0\n",
    "            return 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting position: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def check_format(self, text: str) -> bool:\n",
    "        \"\"\"Check if response follows the required XML format\"\"\"\n",
    "        pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "        return bool(re.search(pattern, text, re.DOTALL))\n",
    "\n",
    "    def predict(self, state: Dict) -> Tuple[float, str]:\n",
    "        \"\"\"Generate trading decision based on current state\"\"\"\n",
    "        prompt = self.format_state(state)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            **self.generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract positioning\n",
    "        position = self.extract_positioning(response)\n",
    "        position = np.clip(position, -1.0, 1.0)\n",
    "        \n",
    "        return position, response\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom PPO Implementation for LLM Trading\n",
    "# =============================================================================\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.responses = []  # Store LLM text responses\n",
    "        self.format_rewards = []  # Store format-specific rewards\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done, response, format_reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)\n",
    "        self.responses.append(response)\n",
    "        self.format_rewards.append(format_reward)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Sample a batch of experiences\"\"\"\n",
    "        if len(self.states) < self.batch_size:\n",
    "            indices = range(len(self.states))\n",
    "        else:\n",
    "            indices = np.random.choice(len(self.states), self.batch_size, replace=False)\n",
    "            \n",
    "        states = [self.states[i] for i in indices]\n",
    "        actions = [self.actions[i] for i in indices]\n",
    "        rewards = [self.rewards[i] for i in indices]\n",
    "        next_states = [self.next_states[i] for i in indices]\n",
    "        dones = [self.dones[i] for i in indices]\n",
    "        responses = [self.responses[i] for i in indices]\n",
    "        format_rewards = [self.format_rewards[i] for i in indices]\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, responses, format_rewards\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.next_states.clear()\n",
    "        self.dones.clear()\n",
    "        self.responses.clear()\n",
    "        self.format_rewards.clear()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "class LLMPPOTrainer:\n",
    "    \"\"\"Custom PPO implementation for training LLM agents\"\"\"\n",
    "    \n",
    "    def __init__(self, agent, env, learning_rate=3e-5, gamma=0.99, epsilon=0.2, \n",
    "                 value_coef=0.5, entropy_coef=0.01, format_reward_weight=0.5):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon  # PPO clipping parameter\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.format_reward_weight = format_reward_weight\n",
    "        self.memory = Memory()\n",
    "        \n",
    "        # Create value network (simple MLP)\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(len(self.vectorize_state(env.get_state())), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(self.agent.device)\n",
    "        \n",
    "        # Create policy network (simple MLP)\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(len(self.vectorize_state(env.get_state())), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()  # Outputs between -1 and 1\n",
    "        ).to(self.agent.device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=self.lr)\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        \n",
    "    def vectorize_state(self, state):\n",
    "        \"\"\"Convert state dictionary to vector for neural network input\"\"\"\n",
    "        # Extract features from state dictionary\n",
    "        vector = []\n",
    "        \n",
    "        # Add position\n",
    "        vector.append(state['position'])\n",
    "        \n",
    "        # Add market context features\n",
    "        for key, value in state['market_context'].items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                vector.append(value)\n",
    "        \n",
    "        # Add action history\n",
    "        vector.extend(state['action_history'])\n",
    "        \n",
    "        # Include sentiment from headlines if available\n",
    "        if 'llm_sentiment' in state['market_context']:\n",
    "            vector.append(state['market_context']['llm_sentiment'])\n",
    "            \n",
    "        return np.array(vector, dtype=np.float32)\n",
    "    \n",
    "    def compute_format_reward(self, response: str) -> float:\n",
    "        \"\"\"Calculate reward for formatting according to required XML structure\"\"\"\n",
    "        # Check overall structure\n",
    "        has_correct_format = self.agent.check_format(response)\n",
    "        \n",
    "        # Check individual tags\n",
    "        has_macro_state = \"<macro state>\" in response and \"</macro state>\" in response\n",
    "        has_reasoning = \"<reasoning>\" in response and \"</reasoning>\" in response\n",
    "        has_positioning = \"<positioning>\" in response and \"</positioning>\" in response\n",
    "        \n",
    "        # Calculate format reward component\n",
    "        if has_correct_format:\n",
    "            return 0.5  # Full format reward\n",
    "        elif has_macro_state and has_reasoning and has_positioning:\n",
    "            return 0.3  # Tags exist but not in correct order/format\n",
    "        elif (has_macro_state and has_reasoning) or (has_macro_state and has_positioning) or (has_reasoning and has_positioning):\n",
    "            return 0.1  # Some tags exist\n",
    "        else:\n",
    "            return -0.2  # Format completely wrong\n",
    "    \n",
    "    def collect_rollout(self, num_episodes=5, max_steps=21):\n",
    "        \"\"\"Collect experience by running episodes\"\"\"\n",
    "        total_rewards = []\n",
    "        \n",
    "        for _ in tqdm(range(num_episodes), desc=\"Collecting experience\"):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                # Get LLM-based action \n",
    "                position, response = self.agent.predict(state)\n",
    "                \n",
    "                # Get neural network-based value estimate\n",
    "                state_vector = self.vectorize_state(state)\n",
    "                state_tensor = torch.FloatTensor(state_vector).unsqueeze(0).to(self.agent.device)\n",
    "                value = self.value_net(state_tensor).item()\n",
    "                \n",
    "                # Calculate format reward\n",
    "                format_reward = self.compute_format_reward(response)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, _ = self.env.step(position)\n",
    "                \n",
    "                # Combine environment reward and format reward\n",
    "                combined_reward = reward + self.format_reward_weight * format_reward\n",
    "                \n",
    "                # Store experience\n",
    "                self.memory.add(state, position, combined_reward, next_state, done, response, format_reward)\n",
    "                \n",
    "                episode_reward += combined_reward\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            total_rewards.append(episode_reward)\n",
    "            \n",
    "        return np.mean(total_rewards)\n",
    "    \n",
    "    def update_policy(self, num_epochs=10):\n",
    "        \"\"\"Update policy and value networks using PPO\"\"\"\n",
    "        if len(self.memory) == 0:\n",
    "            return\n",
    "            \n",
    "        for _ in range(num_epochs):\n",
    "            # Sample batch of experiences\n",
    "            states, actions, rewards, next_states, dones, responses, format_rewards = self.memory.sample()\n",
    "            \n",
    "            # Convert to tensors\n",
    "            state_vectors = [self.vectorize_state(s) for s in states]\n",
    "            state_tensors = torch.FloatTensor(state_vectors).to(self.agent.device)\n",
    "            action_tensors = torch.FloatTensor(actions).unsqueeze(1).to(self.agent.device)\n",
    "            reward_tensors = torch.FloatTensor(rewards).unsqueeze(1).to(self.agent.device)\n",
    "            \n",
    "            # Compute advantages\n",
    "            with torch.no_grad():\n",
    "                values = self.value_net(state_tensors)\n",
    "                next_state_vectors = [self.vectorize_state(s) for s in next_states]\n",
    "                next_state_tensors = torch.FloatTensor(next_state_vectors).to(self.agent.device)\n",
    "                next_values = self.value_net(next_state_tensors)\n",
    "                \n",
    "                # Compute returns and advantages\n",
    "                returns = []\n",
    "                advantages = []\n",
    "                for r, d, v, nv in zip(rewards, dones, values, next_values):\n",
    "                    if d:\n",
    "                        R = r\n",
    "                    else:\n",
    "                        R = r + self.gamma * nv.item()\n",
    "                    adv = R - v.item()\n",
    "                    returns.append(R)\n",
    "                    advantages.append(adv)\n",
    "                    \n",
    "                returns_tensor = torch.FloatTensor(returns).unsqueeze(1).to(self.agent.device)\n",
    "                advantages_tensor = torch.FloatTensor(advantages).unsqueeze(1).to(self.agent.device)\n",
    "            \n",
    "            # Normalize advantages\n",
    "            if len(advantages_tensor) > 1:\n",
    "                advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
    "            \n",
    "            # Policy loss\n",
    "            old_actions = self.policy_net(state_tensors).detach()\n",
    "            new_actions = self.policy_net(state_tensors)\n",
    "            \n",
    "            # Get probability ratio\n",
    "            ratio = torch.exp(new_actions - old_actions)\n",
    "            \n",
    "            # PPO loss\n",
    "            clip_adv = torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon) * advantages_tensor\n",
    "            policy_loss = -torch.min(ratio * advantages_tensor, clip_adv).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = nn.MSELoss()(self.value_net(state_tensors), returns_tensor)\n",
    "            \n",
    "            # Update policy network\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            # Update value network\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "    \n",
    "    def train(self, num_iterations=50, num_episodes_per_iter=5, update_epochs=10):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        all_rewards = []\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            # Collect experiences\n",
    "            mean_reward = self.collect_rollout(num_episodes=num_episodes_per_iter)\n",
    "            \n",
    "            # Update policy\n",
    "            self.update_policy(num_epochs=update_epochs)\n",
    "            \n",
    "            # Clear memory after update\n",
    "            self.memory.clear()\n",
    "            \n",
    "            all_rewards.append(mean_reward)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Iteration {iteration+1}/{num_iterations}, Mean Reward: {mean_reward:.4f}\")\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                self.save_models(f\"models/iteration_{iteration+1}\")\n",
    "                \n",
    "                # Plot rewards\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(all_rewards)\n",
    "                plt.title(\"Mean Episode Rewards\")\n",
    "                plt.xlabel(\"Iteration\")\n",
    "                plt.ylabel(\"Reward\")\n",
    "                plt.savefig(f\"models/rewards_iter_{iteration+1}.png\")\n",
    "                plt.close()\n",
    "                \n",
    "        return all_rewards\n",
    "    \n",
    "    def save_models(self, path_prefix):\n",
    "        \"\"\"Save both the LLM and neural networks\"\"\"\n",
    "        # Save policy network\n",
    "        torch.save(self.policy_net.state_dict(), f\"{path_prefix}_policy.pt\")\n",
    "        \n",
    "        # Save value network\n",
    "        torch.save(self.value_net.state_dict(), f\"{path_prefix}_value.pt\")\n",
    "        \n",
    "        # Save LLM model\n",
    "        self.agent.model.save_pretrained(f\"{path_prefix}_llm\")\n",
    "        self.agent.tokenizer.save_pretrained(f\"{path_prefix}_llm\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Main Training Function\n",
    "# =============================================================================\n",
    "\n",
    "def train_macro_trader(df, model_name=\"facebook/opt-350m\", num_iterations=50):\n",
    "    \"\"\"Train a macro trading model with PPO and the custom environment\"\"\"\n",
    "    \n",
    "    # Initialize environment and agent\n",
    "    env = MacroTradingEnv(df)\n",
    "    agent = LLMTradingAgent(model_name=model_name)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = LLMPPOTrainer(agent, env)\n",
    "    \n",
    "    # Train the agent\n",
    "    rewards = trainer.train(num_iterations=num_iterations, num_episodes_per_iter=5)\n",
    "    \n",
    "    # Save final model\n",
    "    trainer.save_models(\"models/final_model\")\n",
    "    \n",
    "    return agent, env, rewards\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_csv('your_data.csv')\n",
    "    \n",
    "    # Ensure 'returns' column exists\n",
    "    if 'returns' not in df.columns:\n",
    "        df['returns'] = df['close'].pct_change().shift(-1)\n",
    "    \n",
    "    # Train macro trader\n",
    "    agent, env, rewards = train_macro_trader(df, num_iterations=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
