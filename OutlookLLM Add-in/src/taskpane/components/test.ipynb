{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVespaManager\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Manages ingestion into Vespa. Uses JSON for simple trackers (pending, success, posted IDs)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    and Parquet for detailed failed payload tracking. Uses GET for status checks.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Internal status constants\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mVespaManager\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m FAILURE_STATUS_FAILED \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m FAILURE_STATUS_PERMANENT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpermanent_failure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     15\u001b[0m     schema_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     16\u001b[0m     env: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     17\u001b[0m     s3_bucket_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m---> 18\u001b[0m     creds: \u001b[43mDict\u001b[49m,\n\u001b[1;32m     19\u001b[0m     s3_tracking_prefix: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     20\u001b[0m     max_reingest_attempts: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     21\u001b[0m     ingest_calls_per_minute: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m     22\u001b[0m     status_calls_per_minute: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m     23\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     24\u001b[0m     limiter_time_period: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m,\n\u001b[1;32m     25\u001b[0m     verify_ssl: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m     s3_client: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m     http_session: Optional[aiohttp\u001b[38;5;241m.\u001b[39mClientSession] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m ):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m schema_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s3_bucket_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s3_tracking_prefix:\n\u001b[1;32m     30\u001b[0m          \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema_id, env, s3_bucket_id, and s3_tracking_prefix are required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "def _default_giveup_condition(e: Exception) -> bool:\n",
    "    \"\"\"Backoff giveup condition: stop on 4xx errors except 429.\"\"\"\n",
    "    if isinstance(e, aiohttp.ClientResponseError):\n",
    "        return 400 <= e.status < 500 and e.status != 429\n",
    "    return True\n",
    "\n",
    "@backoff.on_exception(backoff.expo, aiohttp.ClientError, max_tries=5, giveup=_default_giveup_condition)\n",
    "async def post_with_retry(session: aiohttp.ClientSession, url: str, limiter: AsyncLimiter, json_payload: Any, headers: Dict[str, str], verify_ssl: bool) -> aiohttp.ClientResponse:\n",
    "    \"\"\"Performs an HTTP POST request with rate limiting and backoff/retry logic.\"\"\"\n",
    "    async with limiter:\n",
    "        response = await session.post(url, json=json_payload, headers=headers, ssl=None if verify_ssl else False)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "@backoff.on_exception(backoff.expo, aiohttp.ClientError, max_tries=5, giveup=_default_giveup_condition)\n",
    "async def get_with_retry(session: aiohttp.ClientSession, url: str, limiter: AsyncLimiter, headers: Dict[str, str], verify_ssl: bool) -> aiohttp.ClientResponse:\n",
    "    \"\"\"Performs an HTTP GET request with rate limiting and backoff/retry logic.\"\"\"\n",
    "    async with limiter:\n",
    "        response = await session.get(url, headers=headers, ssl=None if verify_ssl else False)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "class VespaManager:\n",
    "    \"\"\"\n",
    "    Manages ingestion into Vespa. Uses JSON for simple trackers (pending, success, posted IDs)\n",
    "    and Parquet for detailed failed payload tracking. Uses GET for status checks.\n",
    "    \"\"\"\n",
    "    STATUS_SUCCESS = \"SUCCESS\"; STATUS_FAILURE = \"FAILURE\"; STATUS_PENDING = \"PENDING\"\n",
    "    STATUS_DELETED = \"DELETED\"; STATUS_NOT_FOUND = \"NOT_FOUND\"; STATUS_ERROR = \"ERROR\"\n",
    "    FAILURE_STATUS_FAILED = \"failed\"\n",
    "    FAILURE_STATUS_PERMANENT = \"permanent_failure\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema_id: str,\n",
    "        env: str,\n",
    "        s3_bucket_id: str,\n",
    "        creds: Dict,\n",
    "        s3_tracking_prefix: str,\n",
    "        max_reingest_attempts: int = 3,\n",
    "        ingest_calls_per_minute: int = 600,\n",
    "        status_calls_per_minute: int = 300,\n",
    "        batch_size: int = 100,\n",
    "        limiter_time_period: int = 60,\n",
    "        verify_ssl: bool = True,\n",
    "        s3_client: Optional[Any] = None,\n",
    "        http_session: Optional[aiohttp.ClientSession] = None,\n",
    "    ):\n",
    "        if not schema_id or not env or not s3_bucket_id or not s3_tracking_prefix:\n",
    "             raise ValueError(\"schema_id, env, s3_bucket_id, and s3_tracking_prefix are required.\")\n",
    "\n",
    "        self.schema_id = schema_id\n",
    "        self.env = env\n",
    "        self.s3 = s3_client if s3_client else S3(creds, s3_bucket_id)\n",
    "        self.s3_tracking_prefix = s3_tracking_prefix.strip('/')\n",
    "\n",
    "        self.max_reingest_attempts = max_reingest_attempts\n",
    "        self.batch_size = batch_size\n",
    "        self.verify_ssl = verify_ssl\n",
    "\n",
    "        self.ingest_limiter = AsyncLimiter(ingest_calls_per_minute, period=limiter_time_period)\n",
    "        self.status_limiter = AsyncLimiter(status_calls_per_minute, period=limiter_time_period)\n",
    "\n",
    "        # S3 paths: JSON for simple trackers, Parquet for failures\n",
    "        self.pending_tracker_path = f\"{self.s3_tracking_prefix}/pending_documents.json\"\n",
    "        self.success_tracker_path = f\"{self.s3_tracking_prefix}/success_documents.json\"\n",
    "        self.failed_payload_path = f\"{self.s3_tracking_prefix}/failed_payloads.parquet\"\n",
    "        self.posted_native_ids_path = f\"{self.s3_tracking_prefix}/posted_native_ids.json\"\n",
    "\n",
    "        self._external_session = http_session is not None\n",
    "        self._session = http_session if self._external_session else aiohttp.ClientSession(headers=create_headers())\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Closes the internally managed aiohttp ClientSession.\"\"\"\n",
    "        if self._session and not self._external_session and not self._session.closed:\n",
    "            await self._session.close()\n",
    "\n",
    "    def _read_s3_json(self, file_path: str, default: Any = None) -> Any:\n",
    "        \"\"\"Reads/parses JSON from S3, returns default on error/not found.\"\"\"\n",
    "        try:\n",
    "            if self.s3.file_exists(file_path):\n",
    "                content = self.s3.read_file(file_path)\n",
    "                return json.loads(content) if content else (default if default is not None else {})\n",
    "            else: return default if default is not None else {}\n",
    "        except (json.JSONDecodeError, Exception) as e:\n",
    "            print(f\"Warning: Error reading/parsing S3 JSON {file_path}: {e}. Returning default.\")\n",
    "            return default if default is not None else {}\n",
    "\n",
    "    def _write_s3_json(self, file_path: str, data: Any):\n",
    "        \"\"\"Writes Python object as formatted JSON to S3.\"\"\"\n",
    "        try:\n",
    "            self.s3.write_file(file_path, json.dumps(data, indent=2))\n",
    "        except Exception as e: print(f\"Error writing JSON to S3 file {file_path}: {e}\")\n",
    "\n",
    "    def _read_failed_payloads_parquet(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Reads failed payloads parquet into {nativeId: {payload:..., attempts:..., error:..., status:...}}.\"\"\"\n",
    "        df = self.s3.read_parquet(self.failed_payload_path)\n",
    "        failed_dict = {}\n",
    "        required_cols = ['nativeId', 'payload_str', 'ingest_attempts', 'last_error', 'status']\n",
    "        if df.empty or not all(col in df.columns for col in required_cols):\n",
    "            return failed_dict\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            native_id = row['nativeId']\n",
    "            try: payload_dict = json.loads(row.get('payload_str', '{}'))\n",
    "            except (json.JSONDecodeError, TypeError): payload_dict = {}\n",
    "            failed_dict[native_id] = {\n",
    "                'payload': payload_dict,\n",
    "                'ingest_attempts': int(row.get('ingest_attempts', 0)),\n",
    "                'last_error': row.get('last_error', ''),\n",
    "                'status': row.get('status', self.FAILURE_STATUS_FAILED)\n",
    "            }\n",
    "        return failed_dict\n",
    "\n",
    "    def _write_failed_payloads_parquet(self, failed_payloads_dict: Dict[str, Dict]):\n",
    "        \"\"\"Converts failed payloads dict to DataFrame and writes as parquet.\"\"\"\n",
    "        expected_cols = ['nativeId', 'payload_str', 'ingest_attempts', 'last_error', 'status']\n",
    "        if not failed_payloads_dict:\n",
    "            df = pd.DataFrame(columns=expected_cols)\n",
    "            self.s3.write_parquet(self.failed_payload_path, df) # Write empty DF with schema\n",
    "            return\n",
    "\n",
    "        data_for_df = []\n",
    "        for native_id, data in failed_payloads_dict.items():\n",
    "            try: payload_str = json.dumps(data.get('payload', {}))\n",
    "            except TypeError: payload_str = \"{}\"\n",
    "            data_for_df.append({\n",
    "                'nativeId': native_id, 'payload_str': payload_str,\n",
    "                'ingest_attempts': data.get('ingest_attempts', 0),\n",
    "                'last_error': data.get('last_error', ''),\n",
    "                'status': data.get('status', self.FAILURE_STATUS_FAILED)\n",
    "            })\n",
    "        df = pd.DataFrame(data_for_df, columns=expected_cols)\n",
    "        self.s3.write_parquet(self.failed_payload_path, df)\n",
    "\n",
    "    def _has_failed_payloads(self) -> bool:\n",
    "         \"\"\"Checks if any failures are tracked in the parquet file.\"\"\"\n",
    "         failed_dict = self._read_failed_payloads_parquet()\n",
    "         return bool(failed_dict)\n",
    "\n",
    "    def _get_pending_tracker(self) -> Dict[str, str]:\n",
    "        return self._read_s3_json(self.pending_tracker_path, default={})\n",
    "    def _get_success_tracker(self) -> Dict[str, str]:\n",
    "        return self._read_s3_json(self.success_tracker_path, default={})\n",
    "    def _get_failed_payloads(self) -> Dict[str, Dict]:\n",
    "        return self._read_failed_payloads_parquet()\n",
    "    def _get_posted_native_ids(self) -> Set[str]:\n",
    "        id_list = self._read_s3_json(self.posted_native_ids_path, default=[])\n",
    "        return set(id_list) if isinstance(id_list, list) else set()\n",
    "\n",
    "    async def ingest_in_vespa(self, ingest_payload: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Ingests a list of payload dictionaries into Vespa.\n",
    "        Handles filtering, batching, POST requests, state tracking, and saving state.\n",
    "        \"\"\"\n",
    "        if not ingest_payload: return\n",
    "        print(f\"Starting ingestion for {len(ingest_payload)} payloads.\")\n",
    "\n",
    "        # Load current state\n",
    "        pending_docs = self._get_pending_tracker()\n",
    "        success_docs = self._get_success_tracker()\n",
    "        failed_payloads = self._get_failed_payloads() # Dict from parquet\n",
    "        posted_native_ids = self._get_posted_native_ids()\n",
    "\n",
    "        payloads_to_process = []\n",
    "        skipped_count = 0\n",
    "        for payload in ingest_payload:\n",
    "            native_id = payload.get(\"nativeId\")\n",
    "            if native_id in success_docs or native_id in posted_native_ids:\n",
    "                skipped_count += 1\n",
    "            else:\n",
    "                payloads_to_process.append(payload)\n",
    "\n",
    "        if skipped_count > 0: print(f\"Skipped {skipped_count} already processed/posted payloads.\")\n",
    "        if not payloads_to_process: print(\"No new payloads to process.\"); return\n",
    "        print(f\"Processing {len(payloads_to_process)} new or previously failed payloads.\")\n",
    "\n",
    "        ingest_url = create_url(self.env, f\"{self.schema_id}/ingest\")\n",
    "        num_batches = (len(payloads_to_process) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        for i in range(0, len(payloads_to_process), self.batch_size):\n",
    "            batch = payloads_to_process[i : i + self.batch_size]\n",
    "            batch_num = i // self.batch_size + 1\n",
    "            print(f\"Processing batch {batch_num}/{num_batches}...\")\n",
    "\n",
    "            response = None\n",
    "            try:\n",
    "                response = await post_with_retry(\n",
    "                    session=self._session, url=ingest_url, limiter=self.ingest_limiter,\n",
    "                    json_payload=batch, headers=create_headers(), verify_ssl=self.verify_ssl,\n",
    "                )\n",
    "                response_json = await response.json(content_type=None)\n",
    "\n",
    "                api_success_docs = response_json.get(\"successDocs\", [])\n",
    "                api_failed_docs = response_json.get(\"failedDocs\", [])\n",
    "\n",
    "                for success_info in api_success_docs:\n",
    "                    native_id = success_info.get(\"nativeId\"); doc_id = success_info.get(\"documentId\")\n",
    "                    if native_id and doc_id:\n",
    "                        pending_docs[native_id] = doc_id\n",
    "                        posted_native_ids.add(native_id)\n",
    "                        if native_id in failed_payloads: del failed_payloads[native_id] # Remove from failures\n",
    "                    else: print(f\"Warning: Could not process success info: {success_info}\")\n",
    "\n",
    "                for failure_info in api_failed_docs:\n",
    "                    native_id = failure_info.get(\"nativeId\"); error_msg = failure_info.get(\"error\")\n",
    "                    original_payload = batch_origin_map.get(native_id)\n",
    "                    if native_id and original_payload:\n",
    "                         self._update_failed_payloads_dict(native_id, original_payload, error_msg, failed_payloads)\n",
    "                         posted_native_ids.add(native_id) # Mark as posted even on failure\n",
    "                    else: print(f\"Warning: Could not link failed post back. Info: {failure_info}\")\n",
    "\n",
    "            except (aiohttp.ClientError, asyncio.TimeoutError) as http_err:\n",
    "                print(f\"HTTP Error processing batch {batch_num}: {http_err}\")\n",
    "                error_msg = f\"Batch POST failed permanently: {http_err}\"\n",
    "                for payload in batch: # Iterate original batch items\n",
    "                    native_id = payload.get('nativeId')\n",
    "                    if native_id:\n",
    "                        self._update_failed_payloads_dict(native_id, payload, error_msg, failed_payloads)\n",
    "                        posted_native_ids.add(native_id)\n",
    "            except json.JSONDecodeError as json_err:\n",
    "                 print(f\"JSON Error processing batch {batch_num} response: {json_err}\")\n",
    "                 error_msg = f\"Failed to parse Vespa response: {json_err} (Status: {response.status if response else 'N/A'})\"\n",
    "                 for payload in batch:\n",
    "                      native_id = payload.get('nativeId')\n",
    "                      if native_id:\n",
    "                           self._update_failed_payloads_dict(native_id, payload, error_msg, failed_payloads)\n",
    "                           posted_native_ids.add(native_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected Error processing batch {batch_num}: {e}\")\n",
    "                error_msg = f\"Unexpected batch error: {e}\"\n",
    "                for payload in batch:\n",
    "                     native_id = payload.get('nativeId')\n",
    "                     if native_id:\n",
    "                          self._update_failed_payloads_dict(native_id, payload, error_msg, failed_payloads)\n",
    "                          posted_native_ids.add(native_id)\n",
    "            finally:\n",
    "                 if response: response.release()\n",
    "\n",
    "        print(\"Ingestion loop finished. Saving trackers...\")\n",
    "        self._write_s3_json(self.pending_tracker_path, pending_docs)\n",
    "        self._write_failed_payloads_parquet(failed_payloads) # Writes parquet\n",
    "        self._write_s3_json(self.posted_native_ids_path, sorted(list(posted_native_ids))) # Writes JSON list\n",
    "\n",
    "\n",
    "    def _update_failed_payloads_dict(\n",
    "        self, native_id: str, payload_dict: Dict[str, Any], error_message: str, failed_payloads_master_dict: Dict[str, Dict]\n",
    "    ):\n",
    "        \"\"\"Updates the in-memory failed payloads dict, handles retries and status.\"\"\"\n",
    "        entry = failed_payloads_master_dict.get(native_id, {})\n",
    "        entry['payload'] = payload_dict\n",
    "        entry['ingest_attempts'] = entry.get('ingest_attempts', 0) + 1\n",
    "        entry['last_error'] = str(error_message)\n",
    "        if entry['ingest_attempts'] > self.max_reingest_attempts:\n",
    "            if entry.get('status') != self.FAILURE_STATUS_PERMANENT:\n",
    "                 print(f\"Max attempts reached for {native_id}. Marking as permanent failure.\")\n",
    "                 entry['status'] = self.FAILURE_STATUS_PERMANENT\n",
    "        else:\n",
    "            entry['status'] = self.FAILURE_STATUS_FAILED\n",
    "        failed_payloads_master_dict[native_id] = entry\n",
    "\n",
    "\n",
    "    async def _get_single_doc_status(self, document_id: str) -> Tuple[str, Optional[str]]:\n",
    "        \"\"\"Fetches status for a single document ID using GET with retry.\"\"\"\n",
    "        status_url = create_url(self.env, f\"v1/status/{self.schema_id}/{document_id}\")\n",
    "        response = None\n",
    "        try:\n",
    "            response = await get_with_retry(\n",
    "                session=self._session, url=status_url, limiter=self.status_limiter,\n",
    "                headers=create_headers(), verify_ssl=self.verify_ssl,\n",
    "            )\n",
    "            status_json = await response.json(content_type=None)\n",
    "            vespa_status = status_json.get(\"status\") # Check actual Vespa status strings\n",
    "            # --- Map Vespa status strings ---\n",
    "            if vespa_status == \"SUCCEEDED\": return document_id, self.STATUS_SUCCESS\n",
    "            elif vespa_status == \"FAILED\": return document_id, self.STATUS_FAILURE\n",
    "            else: return document_id, self.STATUS_PENDING\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            if e.status == 404: return document_id, self.STATUS_NOT_FOUND\n",
    "            else: print(f\"HTTP Error status {document_id}: {e}\"); return document_id, self.STATUS_ERROR\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError) as err:\n",
    "            print(f\"Error status {document_id}: {err}\"); return document_id, self.STATUS_ERROR\n",
    "        except Exception as e:\n",
    "             print(f\"Unexpected error status {document_id}: {e}\"); return document_id, self.STATUS_ERROR\n",
    "        finally:\n",
    "            if response: response.release()\n",
    "\n",
    "\n",
    "    async def update_ingestion_status(self) -> bool:\n",
    "        \"\"\"Checks status via GET per doc, updates trackers, returns if all processed.\"\"\"\n",
    "        print(\"Starting ingestion status update via GET...\")\n",
    "        pending_docs = self._get_pending_tracker()\n",
    "        success_docs = self._get_success_tracker()\n",
    "        failed_payloads = self._get_failed_payloads()\n",
    "\n",
    "        if not pending_docs: print(\"No documents pending status check.\"); return True\n",
    "\n",
    "        print(f\"Checking status for {len(pending_docs)} pending documents concurrently...\")\n",
    "        tasks = [self._get_single_doc_status(doc_id) for doc_id in pending_docs.values()]\n",
    "        status_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        all_processed_terminal = True\n",
    "        native_id_map = {v: k for k, v in pending_docs.items()}\n",
    "        needs_failed_save = False\n",
    "\n",
    "        for document_id, current_status in status_results:\n",
    "            native_id = native_id_map.get(document_id)\n",
    "            if not native_id: continue\n",
    "\n",
    "            if current_status == self.STATUS_SUCCESS:\n",
    "                success_docs[native_id] = document_id\n",
    "                if native_id in pending_docs: del pending_docs[native_id]\n",
    "                if native_id in failed_payloads: del failed_payloads[native_id]; needs_failed_save = True\n",
    "            elif current_status == self.STATUS_FAILURE:\n",
    "                error_msg = \"Ingestion confirmed FAILURE via status API\"\n",
    "                if native_id not in failed_payloads:\n",
    "                     failed_payloads[native_id] = {'payload': {}, 'ingest_attempts': 0, 'last_error': '', 'status': self.FAILURE_STATUS_FAILED}\n",
    "                     print(f\"Warning: Adding missing entry for {native_id} to failures due to status API result.\")\n",
    "                self._update_failed_payloads_dict(native_id, failed_payloads[native_id].get('payload',{}), error_msg, failed_payloads)\n",
    "                needs_failed_save = True\n",
    "                if native_id in pending_docs: del pending_docs[native_id]\n",
    "            elif current_status in [self.STATUS_DELETED, self.STATUS_NOT_FOUND]:\n",
    "                 if native_id in pending_docs: del pending_docs[native_id]\n",
    "                 if native_id in failed_payloads: del failed_payloads[native_id]; needs_failed_save = True\n",
    "            elif current_status == self.STATUS_ERROR:\n",
    "                 all_processed_terminal = False\n",
    "            else: # Pending\n",
    "                 all_processed_terminal = False\n",
    "\n",
    "        print(\"Status update finished. Saving trackers...\")\n",
    "        self._write_s3_json(self.pending_tracker_path, pending_docs)\n",
    "        self._write_s3_json(self.success_tracker_path, success_docs)\n",
    "        if needs_failed_save:\n",
    "             self._write_failed_payloads_parquet(failed_payloads)\n",
    "\n",
    "        return all_processed_terminal\n",
    "\n",
    "    async def reingest_documents(self):\n",
    "        \"\"\"Loads non-permanent failed payloads from parquet and attempts re-ingestion.\"\"\"\n",
    "        print(\"Starting re-ingestion process...\")\n",
    "        failed_payloads_dict = self._get_failed_payloads()\n",
    "        if not failed_payloads_dict: print(\"No failed payloads found.\"); return\n",
    "\n",
    "        payloads_to_reingest = []\n",
    "        for native_id, data in failed_payloads_dict.items():\n",
    "            if data.get('status') != self.FAILURE_STATUS_PERMANENT:\n",
    "                actual_payload = data.get('payload', {})\n",
    "                if 'nativeId' not in actual_payload:\n",
    "                     actual_payload['nativeId'] = native_id\n",
    "                payloads_to_reingest.append(actual_payload)\n",
    "\n",
    "        if not payloads_to_reingest: print(\"No non-permanent failed payloads found to re-ingest.\"); return\n",
    "\n",
    "        print(f\"Attempting re-ingestion for {len(payloads_to_reingest)} documents.\")\n",
    "        await self.ingest_in_vespa(payloads_to_reingest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Polling Function (Remains the same structure) ---\n",
    "\n",
    "async def poll_ingest_update(\n",
    "    start_date: dt.date,\n",
    "    end_date: dt.date,\n",
    "    schema_id: str,\n",
    "    env: str,\n",
    "    s3_bucket_id: str,\n",
    "    s3_tracking_prefix: str,\n",
    "    s3_client: Optional[Any] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Polls data, ingests into Vespa, monitors status via GET, uses mixed tracking storage,\n",
    "    and retries failures until completion or max attempts.\n",
    "    \"\"\"\n",
    "    creds = get_s3_credentials()\n",
    "    manager = None\n",
    "\n",
    "    try:\n",
    "        manager = VespaManager(\n",
    "            schema_id=schema_id, env=env, s3_bucket_id=s3_bucket_id, creds=creds,\n",
    "            s3_tracking_prefix=s3_tracking_prefix, s3_client=s3_client,\n",
    "        )\n",
    "\n",
    "        print(f\"Loading data for date range: {start_date} to {end_date}\")\n",
    "        payload_list = load_earnings_call_data_vespa(start_date, end_date)\n",
    "        if not payload_list: print(\"No payloads generated.\"); return\n",
    "\n",
    "        # --- Preprocessing ---\n",
    "        temp_s3 = S3(creds, bucket_id=\"fluenta1\")\n",
    "        try:\n",
    "             sedol_map_str = temp_s3.read_file(\"sedol_to_id_db_company_fe.json\")\n",
    "             sedol_map = json.loads(sedol_map_str) if sedol_map_str else {}\n",
    "        except Exception as e: print(f\"Warning: Error loading sedol map: {e}\"); sedol_map = {}\n",
    "\n",
    "        print(\"Preprocessing payloads...\")\n",
    "        processed_payloads = []\n",
    "        for payload in payload_list:\n",
    "            try:\n",
    "                if \"fields\" not in payload: payload[\"fields\"] = {}\n",
    "                if \"sedols_s\" not in payload[\"fields\"]: payload[\"fields\"][\"sedols_s\"] = []\n",
    "                sedol = payload[\"fields\"][\"sedols_s\"][0] if payload[\"fields\"][\"sedols_s\"] else None\n",
    "                payload[\"fields\"][\"companyId_s\"] = sedol_map.get(sedol, \"\") if sedol else \"\"\n",
    "                payload[\"fields\"][\"event_time_s\"] = parse_ect_event_time(payload[\"fields\"].get(\"event_time_s\"))\n",
    "                payload[\"fields\"][\"document_date_s\"] = parse_ect_source_time(payload[\"fields\"].get(\"document_date_s\"))\n",
    "                # Ensure nativeId exists before adding to processed list\n",
    "                if \"nativeId\" in payload:\n",
    "                    processed_payloads.append(payload)\n",
    "                else:\n",
    "                    print(f\"Warning: Payload missing nativeId during preprocessing. Skipping.\")\n",
    "            except Exception as e: print(f\"Error preprocessing payload {payload.get('nativeId', 'N/A')}: {e}. Skipping.\")\n",
    "        if not processed_payloads: print(\"No payloads after preprocessing.\"); return\n",
    "\n",
    "        # --- Initial Ingestion ---\n",
    "        print(f\"\\n--- Starting Initial Ingestion of {len(processed_payloads)} payloads ---\")\n",
    "        await manager.ingest_in_vespa(processed_payloads) # Pass list of payload dicts\n",
    "\n",
    "        # --- Polling Loop ---\n",
    "        print(\"\\n--- Entering Status Polling and Re-ingestion Loop ---\")\n",
    "        poll_attempt = 0\n",
    "        max_poll_attempts = 10 # Safety break\n",
    "\n",
    "        while poll_attempt < max_poll_attempts:\n",
    "            poll_attempt += 1\n",
    "            print(f\"\\n--- Poll Cycle {poll_attempt}/{max_poll_attempts} ---\")\n",
    "            all_processed = False\n",
    "            try:\n",
    "                all_processed = await manager.update_ingestion_status()\n",
    "                await manager.reingest_documents()\n",
    "                has_failures = manager._has_failed_payloads()\n",
    "\n",
    "                if all_processed and not has_failures: print(\"\\nSUCCESS: All documents processed successfully.\"); break\n",
    "                elif all_processed and has_failures: print(\"STATUS: All pending processed, but failures remain (check permanent).\")\n",
    "                else: print(\"STATUS: Some documents are still pending.\")\n",
    "\n",
    "                if poll_attempt >= max_poll_attempts:\n",
    "                     print(f\"WARNING: Max poll attempts reached. Exiting loop.\")\n",
    "                     if has_failures: print(\"WARNING: Exiting with outstanding failures.\")\n",
    "                     if not all_processed: print(\"WARNING: Exiting with pending documents.\")\n",
    "                     break\n",
    "                print(f\"Waiting 20 seconds before next poll cycle...\")\n",
    "                await asyncio.sleep(20)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during poll cycle {poll_attempt}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                print(\"Waiting 60 seconds after error...\")\n",
    "                await asyncio.sleep(60)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"FATAL Error in ingestion process: {e}\")\n",
    "         traceback.print_exc()\n",
    "    finally:\n",
    "        if manager: await manager.close()\n",
    "        print(\"Ingestion process finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
