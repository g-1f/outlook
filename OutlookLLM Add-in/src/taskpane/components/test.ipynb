{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "\n",
    "class StockTradingEnvCustom:\n",
    "\n",
    "    def __init__(self, df, initial_amount=1e6, hmax=100, tech_indicator_list=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.tech_indicator_list = tech_indicator_list or []\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.cash = initial_amount\n",
    "        self.position = 0\n",
    "\n",
    "        self.data = self.df.iloc[self.current_step]\n",
    "        self.price = self.data[\"close\"]\n",
    "        self.news = self.data.get(\"news\", \"\")\n",
    "        self.state = [self.cash, self.price] + [\n",
    "            self.data[tech] for tech in self.tech_indicator_list\n",
    "        ]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.cash = self.initial_amount\n",
    "        self.position = 0\n",
    "        self.data = self.df.iloc[self.current_step]\n",
    "        self.price = self.data[\"close\"]\n",
    "        self.news = self.data.get(\"news\", \"\")\n",
    "        self.state = [self.cash, self.price] + [\n",
    "            self.data[tech] for tech in self.tech_indicator_list\n",
    "        ]\n",
    "        return self.state, self.news\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute the (modulated) action:\n",
    "          - action: integer representing shares to buy (positive) or sell (negative).\n",
    "        Updates cash and position, then moves to the next timestep.\n",
    "        Returns:\n",
    "          - new numerical state,\n",
    "          - new news snippet,\n",
    "          - reward (change in portfolio value),\n",
    "          - done flag,\n",
    "          - empty info dict.\n",
    "        \"\"\"\n",
    "        # Clip action to allowable range\n",
    "        action = int(np.clip(action, -self.hmax, self.hmax))\n",
    "        trade_cost = self.price * action\n",
    "        self.cash -= trade_cost\n",
    "        self.position += action\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.df):\n",
    "            self.done = True\n",
    "            new_state, new_news = None, \"\"\n",
    "        else:\n",
    "            self.data = self.df.iloc[self.current_step]\n",
    "            self.price = self.data[\"close\"]\n",
    "            new_news = self.data.get(\"news\", \"\")\n",
    "            new_state = [self.cash, self.price] + [\n",
    "                self.data[tech] for tech in self.tech_indicator_list\n",
    "            ]\n",
    "\n",
    "        portfolio_value = self.cash + self.position * self.price\n",
    "        reward = portfolio_value - self.initial_amount\n",
    "\n",
    "        return new_state, new_news, reward, self.done, {}\n",
    "\n",
    "\n",
    "def obs_and_news_to_prompt(obs, news):\n",
    "    \"\"\"\n",
    "    Build a composite prompt that includes both numerical observations and news text.\n",
    "    The prompt output three comma-separated numbers:\n",
    "      (action, recommendation score, risk score)\n",
    "    \"\"\"\n",
    "    obs_str = \", \".join([f\"{v:.2f}\" for v in obs])\n",
    "    prompt = (\n",
    "        f\"Observation: {obs_str}. News: {news} \"\n",
    "        \"As a financial expert, please output three numbers separated by commas: \"\n",
    "        \"the trading action (integer), a stock recommendation score (1-5), \"\n",
    "        \"and a risk assessment score (1-5).\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def parse_response_refined(response_text):\n",
    "    \"\"\"\n",
    "    Expect three comma-separated numbers:\n",
    "      - Raw action (integer)\n",
    "      - Recommendation score (float)\n",
    "      - Risk score (float)\n",
    "    Returns defaults (0, 3.0, 3.0) if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = response_text.strip().split(\",\")\n",
    "        if len(parts) < 3:\n",
    "            raise ValueError(\"Insufficient output values.\")\n",
    "        action = int(float(parts[0].strip()))\n",
    "        rec_score = float(parts[1].strip())\n",
    "        risk_score = float(parts[2].strip())\n",
    "    except Exception:\n",
    "        action, rec_score, risk_score = 0, 3.0, 3.0\n",
    "    return action, rec_score, risk_score\n",
    "\n",
    "\n",
    "def map_rec_score_to_factor(rec_score):\n",
    "    \"\"\"\n",
    "    Map the recommendation score (Sf) to an action modulation factor.\n",
    "      5 -> 1.10, 4 -> 1.05, 3 -> 1.00, 2 -> 0.95, 1 -> 0.90.\n",
    "    \"\"\"\n",
    "    if rec_score >= 5:\n",
    "        return 1.10\n",
    "    elif rec_score >= 4:\n",
    "        return 1.05\n",
    "    elif rec_score >= 3:\n",
    "        return 1.00\n",
    "    elif rec_score >= 2:\n",
    "        return 0.95\n",
    "    else:\n",
    "        return 0.90\n",
    "\n",
    "\n",
    "def map_risk_score_to_factor(risk_score):\n",
    "    \"\"\"\n",
    "    Map the risk score (Rf) to a reward adjustment factor.\n",
    "      5 -> 1.10, 4 -> 1.05, 3 -> 1.00, 2 -> 0.95, 1 -> 0.90.\n",
    "    \"\"\"\n",
    "    if risk_score >= 5:\n",
    "        return 1.10\n",
    "    elif risk_score >= 4:\n",
    "        return 1.05\n",
    "    elif risk_score >= 3:\n",
    "        return 1.00\n",
    "    elif risk_score >= 2:\n",
    "        return 0.95\n",
    "    else:\n",
    "        return 0.90\n",
    "\n",
    "\n",
    "def load_trading_data(path):\n",
    "    \"\"\"\n",
    "    Load trading data from CSV. The CSV should contain:\n",
    "      - \"close\" column,\n",
    "      - \"news\" column,\n",
    "      - (Optionally) technical indicator columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_trl_refined(env, model, tokenizer, ppo_trainer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        obs, news = env.reset()\n",
    "        done = False\n",
    "        epoch_reward = 0\n",
    "        prompts = []\n",
    "        responses = []\n",
    "        rewards = []\n",
    "\n",
    "        while not done:\n",
    "            prompt = obs_and_news_to_prompt(obs, news)\n",
    "            prompts.append(prompt)\n",
    "\n",
    "            # Tokenize and generate a response from Qwen2.5\n",
    "            query = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            response_ids = model.generate(**query, max_length=50)\n",
    "            response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "            responses.append(response_text)\n",
    "\n",
    "            # Parse the response to get raw action, recommendation score, and risk score\n",
    "            raw_action, rec_score, risk_score = parse_response_refined(response_text)\n",
    "            sf = map_rec_score_to_factor(rec_score)\n",
    "            modulated_action = int(round(sf * raw_action))\n",
    "\n",
    "            # Execute action in environment\n",
    "            next_obs, next_news, reward, done, _ = env.step(modulated_action)\n",
    "            rf = map_risk_score_to_factor(risk_score)\n",
    "            reward_adj = rf * reward\n",
    "\n",
    "            rewards.append(reward_adj)\n",
    "            epoch_reward += reward_adj\n",
    "\n",
    "            obs, news = next_obs, next_news if not done else (None, \"\")\n",
    "\n",
    "        # Tokenize prompts and responses as a batch for PPO update\n",
    "        query_batch = tokenizer(\n",
    "            prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        response_batch = tokenizer(\n",
    "            responses, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "\n",
    "        stats = ppo_trainer.step(query_batch, response_batch, rewards)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} - Total Adjusted Reward: {epoch_reward:.2f}, Stats: {stats}\"\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_path = \"train_data_2013_2018.csv\"\n",
    "    df = load_trading_data(data_path)\n",
    "\n",
    "    env = StockTradingEnvCustom(df)\n",
    "\n",
    "    model_name = \"Qwen-2.5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "\n",
    "    ppo_config = PPOConfig(\n",
    "        model_name=model_name,\n",
    "        num_train_epochs=3,\n",
    "        batch_size=1,\n",
    "        learning_rate=1e-5,\n",
    "        log_with=\"tensorboard\",\n",
    "    )\n",
    "\n",
    "    ppo_trainer = PPOTrainer(ppo_config, model, tokenizer)\n",
    "\n",
    "    num_epochs = 10\n",
    "    trained_model = train_trl_refined(\n",
    "        env, model, tokenizer, ppo_trainer, num_epochs=num_epochs\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    save_dir = \"trained_trl_qwen2.5_news_trading\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    trained_model.save_pretrained(save_dir)\n",
    "    print(\"Training finished and model saved in\", save_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
