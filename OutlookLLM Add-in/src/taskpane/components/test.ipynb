{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from collections import deque\n",
    "from datasets import Dataset\n",
    "import re\n",
    "\n",
    "# System prompt for the portfolio manager\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "Example:\n",
    "Market Context: PMI:52.6, inflation:5.1%, unemployment:3.8%\n",
    "\n",
    "<macro state>\n",
    "Economy is showing strength with PMI in expansion territory, but inflation remains elevated.\n",
    "</macro state>\n",
    "<reasoning>\n",
    "The PMI at 52.6 indicates expansion, which is positive for equities. However, inflation at 5.1% is above the Fed's target, suggesting possible rate hikes. Employment is strong at 3.8%, supporting consumer spending.\n",
    "</reasoning>\n",
    "<positioning>\n",
    "0.3\n",
    "</positioning>\n",
    "\n",
    "You must respond in the above XML format.\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data(df):\n",
    "    def prepare_prompt(df):\n",
    "        # Instead of creating a list of dictionaries, create the formatted prompt directly\n",
    "        # This avoids serialization issues when the Dataset is created\n",
    "        df['prompt'] = df.apply(lambda row: [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT.strip()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Market Context:{', '.join(f'{k}:{v}' for k, v in row.drop('date', errors='ignore').items())}\"\n",
    "            }\n",
    "        ], axis=1)\n",
    "        return df\n",
    "\n",
    "    df = prepare_prompt(df)\n",
    "    df['returns'] = df['close'].pct_change().shift(-1)\n",
    "    train_dataset = df[['prompt', 'returns']]\n",
    "    data = Dataset.from_pandas(train_dataset, preserve_index=False)\n",
    "    return data\n",
    "\n",
    "def extract_positioning(text):\n",
    "    try:\n",
    "        match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            # Extract the value and clean any whitespace\n",
    "            value = match.group(1).strip()\n",
    "            return float(value)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Positioning extraction error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def format_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    # Debug output for first few completions\n",
    "    for i, content in enumerate(completion_contents[:3]):\n",
    "        print(f\"Completion {i}: {content[:100]}...\")\n",
    "    \n",
    "    matches = [re.search(pattern, content, re.DOTALL) is not None for content in completion_contents]\n",
    "    # Stronger reward/penalty for format\n",
    "    return [3.0 if match else -2.0 for match in matches]\n",
    "\n",
    "def return_reward(prompts, completions, returns, **kwargs):\n",
    "    \"\"\"Main reward function combining multiple factors\"\"\"\n",
    "    rewards = []\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    for i, completion in enumerate(completion_contents):\n",
    "        try:\n",
    "            position = extract_positioning(completion)\n",
    "            # Only reward correct format and positioning\n",
    "            if re.search(pattern, completion, re.DOTALL):\n",
    "                rewards.append(position * returns[i % len(returns)])\n",
    "            else:\n",
    "                rewards.append(-1.0)  # Penalty for incorrect format\n",
    "        except Exception as e:\n",
    "            print(f\"Reward calculation error: {e} for completion: {completion[:100]}...\")\n",
    "            rewards.append(-1.0)\n",
    "    return rewards\n",
    "\n",
    "# Custom GRPO trainer with debugging\n",
    "class DebugGRPOTrainer(GRPOTrainer):\n",
    "    def _generate_and_score_completions(self, inputs):\n",
    "        # Print sample input to debug\n",
    "        if self.accelerator.is_main_process and self.state.global_step == 0:\n",
    "            print(\"\\n==== SAMPLE INPUT ====\")\n",
    "            print(f\"Input type: {type(inputs[0]['prompt'])}\")\n",
    "            print(f\"Input sample: {inputs[0]['prompt']}\")\n",
    "            \n",
    "        # Get original implementation result\n",
    "        result = super()._generate_and_score_completions(inputs)\n",
    "        \n",
    "        # Print sample completions in the first step\n",
    "        if self.accelerator.is_main_process and self.state.global_step == 0:\n",
    "            print(\"\\n==== SAMPLE COMPLETION ====\")\n",
    "            prompt_ids = result[\"prompt_ids\"][0]\n",
    "            completion_ids = result[\"completion_ids\"][0]\n",
    "            print(f\"Decoded prompt: {self.processing_class.decode(prompt_ids)}\")\n",
    "            print(f\"Decoded completion: {self.processing_class.decode(completion_ids)}\")\n",
    "            \n",
    "        return result\n",
    "\n",
    "# Load and prepare data\n",
    "train = pd.read_csv('../train.csv')\n",
    "data = prepare_data(train)\n",
    "\n",
    "# Model configuration\n",
    "model_path = './huggingface_mirror/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775'\n",
    "output_dir = \"outputs/Qwen-1.5B-GRPO-trader\"\n",
    "run_name = \"Qwen-1.5B-GRPO-trader\"\n",
    "\n",
    "# GRPO Configuration with increased lengths and more iterations\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=512,  # Increased from 256\n",
    "    max_completion_length=512,  # Increased from 256\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    temperature=0.7,  # Increased from 0.1 for more exploration\n",
    "    num_iterations=2,  # Use 2 iterations per batch\n",
    "    log_completions=True  # Enable logging completions for debugging\n",
    ")\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True  # Important for Qwen models\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True  # Important for Qwen models\n",
    ")\n",
    "\n",
    "# Ensure tokenizer has pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize our custom debug trainer\n",
    "trainer = DebugGRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        format_reward_func,\n",
    "        return_reward\n",
    "    ],\n",
    "    reward_weights=[1.0, 1.0],  # Equal weight for format and returns\n",
    "    args=grpo_config,\n",
    "    train_dataset=data,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
