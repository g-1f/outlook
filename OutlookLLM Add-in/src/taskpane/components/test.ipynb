{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOPortfolioManager:\n",
    "    \"\"\"\n",
    "    Portfolio manager using PPO to make macro-driven investment decisions\n",
    "    Optimized for multi-GPU training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\", output_dir=\"output/ppo_portfolio_manager\"):\n",
    "        # Setup for multi-GPU training\n",
    "        import os\n",
    "        from accelerate import Accelerator\n",
    "        \n",
    "        # Check available GPUs\n",
    "        self.num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Found {self.num_gpus} GPU(s)\")\n",
    "        \n",
    "        # Create accelerator for distributed training\n",
    "        self.accelerator = Accelerator(\n",
    "            gradient_accumulation_steps=4,\n",
    "            mixed_precision=\"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\",\n",
    "            log_with=\"tensorboard\"\n",
    "        )\n",
    "        \n",
    "        # Determine device allocation strategy\n",
    "        self.policy_device = 0  # First GPU for policy model (primary model)\n",
    "        self.ref_device = 1 if self.num_gpus > 1 else 0  # Second GPU for reference model if available\n",
    "        self.value_device = 0  # Value model on first GPU with policy model\n",
    "        self.reward_device = 1 if self.num_gpus > 1 else 0  # Reward model on second GPU if available\n",
    "        \n",
    "        # Multi-GPU optimized PPO configuration\n",
    "        self.ppo_config = PPOConfig(\n",
    "            # Larger per-device batch size since we have multiple GPUs\n",
    "            per_device_train_batch_size=4 if self.num_gpus > 1 else 2,\n",
    "            \n",
    "            # Gradient accumulation for effective larger batch sizes\n",
    "            gradient_accumulation_steps=4,\n",
    "            \n",
    "            # Enable mixed precision training\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            \n",
    "            # Memory optimizations\n",
    "            optimize_cuda_cache=True,\n",
    "            gradient_checkpointing=True,\n",
    "            \n",
    "            # DeepSpeed ZeRO optimization for multi-GPU\n",
    "            deepspeed={\n",
    "                \"zero_optimization\": {\n",
    "                    \"stage\": 2,\n",
    "                    \"offload_optimizer\": {\n",
    "                        \"device\": \"cpu\"\n",
    "                    },\n",
    "                    \"contiguous_gradients\": True,\n",
    "                    \"overlap_comm\": True\n",
    "                }\n",
    "            } if self.num_gpus > 1 else None,\n",
    "            \n",
    "            # Standard parameters\n",
    "            learning_rate=5e-5,\n",
    "            max_grad_norm=1.0,\n",
    "            num_train_epochs=3,\n",
    "            seed=42,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            output_dir=output_dir,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=500,\n",
    "            run_name=\"ppo_portfolio_manager\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=500,\n",
    "            \n",
    "            # PPO specific parameters\n",
    "            num_ppo_epochs=4,\n",
    "            gamma=0.99,\n",
    "            lam=0.95,\n",
    "            cliprange=0.2,\n",
    "            cliprange_value=0.2,\n",
    "            vf_coef=0.1,\n",
    "            kl_coef=0.05,\n",
    "            whiten_rewards=False,\n",
    "            temperature=0.7,\n",
    "            response_length=256,  # Reduced for memory efficiency\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load models with device mapping\n",
    "        from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "        \n",
    "        # Select precision\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        \n",
    "        # 1. Load policy model on first GPU\n",
    "        if self.num_gpus > 1:\n",
    "            # With multiple GPUs, we can use a specific device map\n",
    "            device_map = {0: [0, 1, 2, 3, 4, 5], 1: [6, 7, 8, 9, 10, 11]}  # Example distribution of layers\n",
    "            print(f\"Loading policy model with custom device map: {device_map}\")\n",
    "        else:\n",
    "            device_map = f\"cuda:{self.policy_device}\"\n",
    "            print(f\"Loading policy model on single GPU: {device_map}\")\n",
    "            \n",
    "        self.policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device_map,  # Spread across GPUs or use specific GPU\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # Set generation config\n",
    "        self.policy_model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "        self.policy_model.config.use_cache = not self.ppo_config.gradient_checkpointing\n",
    "        \n",
    "        # 2. Load reference model on second GPU if available\n",
    "        if self.num_gpus > 1:\n",
    "            print(f\"Loading reference model on GPU {self.ref_device}\")\n",
    "            self.ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=f\"cuda:{self.ref_device}\",  # Use second GPU\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        else:\n",
    "            # For single GPU, we'll use a frozen copy of the policy model\n",
    "            print(\"Using frozen copy of policy model as reference model\")\n",
    "            self.ref_model = None  # Will create a copy when needed\n",
    "        \n",
    "        # 3. Create a multi-GPU compatible value model\n",
    "        # For multi-GPU, link it to the policy model but place the value head on the same GPU\n",
    "        from transformers import AutoModelForCausalLMWithValueHead\n",
    "        \n",
    "        print(f\"Creating value model linked to policy model's backbone\")\n",
    "        self.value_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            self.policy_model,  # Use the existing policy model to share parameters\n",
    "            torch_dtype=dtype,\n",
    "        )\n",
    "        \n",
    "        # 4. Create a lightweight reward model on second GPU if available\n",
    "        if self.num_gpus > 1:\n",
    "            print(f\"Creating reward model on GPU {self.reward_device}\")\n",
    "            device = f\"cuda:{self.reward_device}\"\n",
    "        else:\n",
    "            print(f\"Creating reward model on the same GPU as policy\")\n",
    "            device = f\"cuda:{self.policy_device}\"\n",
    "            \n",
    "        class SimpleRewardModel(torch.nn.Module):\n",
    "            def __init__(self, device):\n",
    "                super().__init__()\n",
    "                self.device = device\n",
    "                self.reward_head = torch.nn.Linear(1, 1).to(device)\n",
    "                \n",
    "            def forward(self, input_ids, attention_mask=None):\n",
    "                # Return a dummy reward\n",
    "                return torch.ones((input_ids.shape[0], 1), device=input_ids.device)\n",
    "                \n",
    "            def to(self, device):\n",
    "                self.device = device\n",
    "                self.reward_head = self.reward_head.to(device)\n",
    "                return self\n",
    "        \n",
    "        self.reward_model = SimpleRewardModel(device)\n",
    "        \n",
    "        # Initialize trainer to None\n",
    "        self.trainer = None\n",
    "        \n",
    "        # Generation parameters\n",
    "        self.generation_kwargs = {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": True,\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "        \n",
    "        print(f\"Initialized PPO Portfolio Manager with {self.num_gpus} GPUs\")\n",
    "        print(f\"GPU Memory Usage - Primary GPU: {torch.cuda.memory_allocated(0)/1024**2:.1f}MB / {torch.cuda.get_device_properties(0).total_memory/1024**2:.1f}MB\")\n",
    "        if self.num_gpus > 1:\n",
    "            print(f\"GPU Memory Usage - Secondary GPU: {torch.cuda.memory_allocated(1)/1024**2:.1f}MB / {torch.cuda.get_device_properties(1).total_memory/1024**2:.1f}MB\")\n",
    "\n",
    "    def initialize_trainer(self, train_dataset=None, data_collator=None):\n",
    "        \"\"\"Initialize the PPO trainer for multi-GPU training\"\"\"\n",
    "        try:\n",
    "            # Create a dummy dataset if none provided\n",
    "            if train_dataset is None:\n",
    "                # Create minimal dataset\n",
    "                dummy_text = \"Example.\"\n",
    "                dummy_encoding = self.tokenizer(dummy_text, return_tensors=\"pt\")\n",
    "                dummy_ids = dummy_encoding.input_ids[0].cpu().numpy()\n",
    "                dummy_mask = dummy_encoding.attention_mask[0].cpu().numpy()\n",
    "                \n",
    "                dummy_data = {\n",
    "                    \"input_ids\": [dummy_ids] * 2,\n",
    "                    \"attention_mask\": [dummy_mask] * 2,\n",
    "                    \"rewards\": [0.0] * 2\n",
    "                }\n",
    "                train_dataset = Dataset.from_dict(dummy_data)\n",
    "            \n",
    "            # Create data collator if needed\n",
    "            if data_collator is None:\n",
    "                from transformers import DataCollatorWithPadding\n",
    "                data_collator = DataCollatorWithPadding(\n",
    "                    self.tokenizer, \n",
    "                    pad_to_multiple_of=8\n",
    "                )\n",
    "            \n",
    "            # Free up memory\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # For single GPU: create reference model if needed\n",
    "            if self.num_gpus <= 1 and self.ref_model is None:\n",
    "                from ..models import create_reference_model\n",
    "                print(\"Creating reference model from policy model\")\n",
    "                self.ref_model = create_reference_model(self.policy_model)\n",
    "            \n",
    "            # Initialize PPOTrainer with multi-GPU support\n",
    "            self.trainer = PPOTrainer(\n",
    "                args=self.ppo_config,\n",
    "                processing_class=self.tokenizer,\n",
    "                model=self.policy_model,\n",
    "                ref_model=self.ref_model,\n",
    "                reward_model=self.reward_model,\n",
    "                train_dataset=train_dataset,\n",
    "                value_model=self.value_model,\n",
    "                data_collator=data_collator\n",
    "            )\n",
    "            \n",
    "            print(\"PPOTrainer initialized successfully for multi-GPU training!\")\n",
    "            return self.trainer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing PPOTrainer: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingOrchestrator:\n",
    "    \"\"\"Manages the PPO training process optimized for multi-GPU setups\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MacroTradingEnv, agent: PPOPortfolioManager, output_dir=\"output/ppo_portfolio_manager\", \n",
    "                 use_sequential_training=False):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 500\n",
    "        self.episode_length = 10\n",
    "        self.output_dir = output_dir\n",
    "        self.use_sequential_training = use_sequential_training\n",
    "        \n",
    "        # Maximum sequence lengths\n",
    "        self.max_query_length = 512\n",
    "        self.max_response_length = 256\n",
    "        \n",
    "        # Determine GPU allocation\n",
    "        self.num_gpus = torch.cuda.device_count()\n",
    "        # For storing experiences, use second GPU if available (to avoid policy model GPU)\n",
    "        self.experience_device = 1 if self.num_gpus > 1 else 0\n",
    "        \n",
    "        # Create device contexts for balanced GPU usage\n",
    "        self.experience_context = f\"cuda:{self.experience_device}\"\n",
    "        \n",
    "        print(f\"Training orchestrator initialized with {self.num_gpus} GPUs\")\n",
    "        print(f\"Using GPU {self.experience_device} for experience collection\")\n",
    "    \n",
    "    def compute_format_reward(self, response_text: str) -> float:\n",
    "        \"\"\"Calculate reward for formatting according to required XML structure\"\"\"\n",
    "        # Keep existing implementation\n",
    "        has_correct_format = self.agent.check_format(response_text)\n",
    "        \n",
    "        has_macro_state = \"<macro state>\" in response_text and \"</macro state>\" in response_text\n",
    "        has_reasoning = \"<reasoning>\" in response_text and \"</reasoning>\" in response_text\n",
    "        has_positioning = \"<positioning>\" in response_text and \"</positioning>\" in response_text\n",
    "        \n",
    "        if has_correct_format:\n",
    "            return 0.5\n",
    "        elif has_macro_state and has_reasoning and has_positioning:\n",
    "            return 0.3\n",
    "        elif (has_macro_state and has_reasoning) or (has_macro_state and has_positioning) or (has_reasoning and has_positioning):\n",
    "            return 0.1\n",
    "        else:\n",
    "            return -0.2\n",
    "    \n",
    "    def print_gpu_memory_stats(self):\n",
    "        \"\"\"Print memory stats for all available GPUs\"\"\"\n",
    "        for i in range(self.num_gpus):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**2\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**2\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**2\n",
    "            print(f\"GPU {i}: {allocated:.1f}MB allocated, {reserved:.1f}MB reserved, {total:.1f}MB total ({allocated/total*100:.1f}%)\")\n",
    "    \n",
    "    def collect_experience(self, num_episodes=3):\n",
    "        \"\"\"Collect trading experience with multi-GPU optimization\"\"\"\n",
    "        all_episode_rewards = []\n",
    "        \n",
    "        # Clear experience collection GPU\n",
    "        with torch.cuda.device(self.experience_device):\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        for episode in tqdm(range(num_episodes), desc=\"Collecting experience\"):\n",
    "            state = self.env.reset(random_start=True)\n",
    "            episode_queries = []\n",
    "            episode_responses = []\n",
    "            episode_rewards = []\n",
    "            episode_response_texts = []\n",
    "            episode_format_rewards = []\n",
    "            \n",
    "            # Run one episode\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < self.episode_length:\n",
    "                # Periodically clear cache\n",
    "                if step % 3 == 0:\n",
    "                    with torch.cuda.device(self.experience_device):\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Get action from agent\n",
    "                position, response_text, query, response = self.agent.predict(state)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, info = self.env.step(position)\n",
    "                \n",
    "                # Calculate format reward\n",
    "                format_reward = self.compute_format_reward(response_text)\n",
    "                \n",
    "                # Combine rewards\n",
    "                total_reward = reward + format_reward\n",
    "                \n",
    "                # Move tensors to experience device (if different)\n",
    "                if self.experience_device != self.agent.policy_device:\n",
    "                    query = query.to(f\"cuda:{self.experience_device}\")\n",
    "                    response = response.to(f\"cuda:{self.experience_device}\")\n",
    "                \n",
    "                # Truncate tensors\n",
    "                query = self._truncate_tensor(query, self.max_query_length)\n",
    "                response = self._truncate_tensor(response, self.max_query_length + self.max_response_length)\n",
    "                \n",
    "                # Store experience\n",
    "                episode_queries.append(query)\n",
    "                episode_responses.append(response)\n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_response_texts.append(response_text)\n",
    "                episode_format_rewards.append(format_reward)\n",
    "                \n",
    "                # Move to next state\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            \n",
    "            # Calculate returns with discount\n",
    "            returns = self._calculate_returns(episode_rewards)\n",
    "            \n",
    "            # Manage buffer size\n",
    "            if len(self.buffer) + len(episode_rewards) > self.buffer_size:\n",
    "                self.buffer = self.buffer[-(self.buffer_size - len(episode_rewards)):]\n",
    "            \n",
    "            # Store episodes in buffer\n",
    "            for i in range(len(episode_rewards)):\n",
    "                self.buffer.append({\n",
    "                    'query': episode_queries[i],\n",
    "                    'response': episode_responses[i],\n",
    "                    'reward': returns[i],\n",
    "                    'raw_reward': episode_rewards[i],\n",
    "                    'response_text': episode_response_texts[i],\n",
    "                    'format_reward': episode_format_rewards[i]\n",
    "                })\n",
    "                \n",
    "            all_episode_rewards.extend(episode_rewards)\n",
    "            \n",
    "            # Print GPU stats after each episode\n",
    "            if self.num_gpus > 1:\n",
    "                self.print_gpu_memory_stats()\n",
    "        \n",
    "        return np.mean(all_episode_rewards) if all_episode_rewards else 0.0\n",
    "    \n",
    "    def _truncate_tensor(self, tensor, max_length):\n",
    "        \"\"\"Truncate tensor to maximum length to save memory\"\"\"\n",
    "        if tensor is None:\n",
    "            return tensor\n",
    "            \n",
    "        # Handle different tensor shapes\n",
    "        if len(tensor.shape) == 1:\n",
    "            return tensor[:min(tensor.shape[0], max_length)]\n",
    "        elif len(tensor.shape) == 2:\n",
    "            return tensor[:, :min(tensor.shape[1], max_length)]\n",
    "        elif len(tensor.shape) == 3:\n",
    "            return tensor[:, :, :min(tensor.shape[2], max_length)]\n",
    "        else:\n",
    "            return tensor\n",
    "\n",
    "    def _calculate_returns(self, rewards):\n",
    "        \"\"\"Calculate discounted returns\"\"\"\n",
    "        gamma = self.agent.ppo_config.gamma\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def create_training_dataset(self, queries, responses, rewards):\n",
    "        \"\"\"Create training dataset optimized for multi-GPU training\"\"\"\n",
    "        # Format data for the dataset\n",
    "        formatted_data = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "            \"rewards\": []\n",
    "        }\n",
    "        \n",
    "        # Limit examples for memory efficiency\n",
    "        max_examples = 50\n",
    "        if len(queries) > max_examples:\n",
    "            print(f\"Limiting to {max_examples} examples for memory efficiency (from {len(queries)} available)\")\n",
    "            indices = np.random.choice(len(queries), max_examples, replace=False)\n",
    "            queries = [queries[i] for i in indices]\n",
    "            responses = [responses[i] for i in indices]\n",
    "            rewards = [rewards[i] for i in indices]\n",
    "        \n",
    "        # Process each example\n",
    "        for i in range(len(queries)):\n",
    "            try:\n",
    "                # Get individual tensors\n",
    "                query = queries[i].cpu().detach()\n",
    "                response = responses[i].cpu().detach()\n",
    "                \n",
    "                # Truncate to save memory\n",
    "                query = self._truncate_tensor(query, self.max_query_length)\n",
    "                response = self._truncate_tensor(response, self.max_query_length + self.max_response_length)\n",
    "                \n",
    "                # Handle tensor shapes\n",
    "                if len(query.shape) > 1:\n",
    "                    query = query.flatten()\n",
    "                if len(response.shape) > 1:\n",
    "                    response = response.flatten()\n",
    "                \n",
    "                # Create attention mask\n",
    "                attention_mask = torch.ones_like(query, dtype=torch.long)\n",
    "                \n",
    "                # Add to formatted data\n",
    "                formatted_data[\"input_ids\"].append(query.numpy())\n",
    "                formatted_data[\"attention_mask\"].append(attention_mask.numpy())\n",
    "                \n",
    "                # Add reward\n",
    "                if i < len(rewards):\n",
    "                    reward_value = float(rewards[i])\n",
    "                else:\n",
    "                    reward_value = 0.0\n",
    "                formatted_data[\"rewards\"].append(reward_value)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing experience {i}: {e}\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = Dataset.from_dict(formatted_data)\n",
    "        print(f\"Created dataset with {len(dataset)} examples\")\n",
    "        return dataset\n",
    "\n",
    "    def train(self, num_iterations=20, num_episodes_per_iter=3, batch_size=4):\n",
    "        \"\"\"Training loop optimized for multi-GPU systems\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            # Clear memory on all GPUs\n",
    "            for i in range(self.num_gpus):\n",
    "                with torch.cuda.device(i):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            print(f\"\\n{'='*20} Iteration {iteration+1}/{num_iterations} {'='*20}\")\n",
    "            print(\"GPU memory status before collection:\")\n",
    "            self.print_gpu_memory_stats()\n",
    "            \n",
    "            # Collect new experience\n",
    "            avg_reward = self.collect_experience(num_episodes=num_episodes_per_iter)\n",
    "            \n",
    "            # Skip training if buffer is too small\n",
    "            if len(self.buffer) < batch_size:\n",
    "                print(f\"Buffer too small ({len(self.buffer)}), skipping training\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Clear memory before training\n",
    "                for i in range(self.num_gpus):\n",
    "                    with torch.cuda.device(i):\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Select experiences\n",
    "                if self.use_sequential_training:\n",
    "                    start_idx = max(0, len(self.buffer) - batch_size)\n",
    "                    batch = self.buffer[start_idx:]\n",
    "                else:\n",
    "                    batch_indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), replace=False)\n",
    "                    batch = [self.buffer[i] for i in batch_indices]\n",
    "                \n",
    "                print(f\"Using batch of size {len(batch)}\")\n",
    "                \n",
    "                # Extract components\n",
    "                queries = [item['query'] for item in batch]\n",
    "                responses = [item['response'] for item in batch]\n",
    "                rewards = [item['reward'] for item in batch]\n",
    "                \n",
    "                # Create dataset\n",
    "                train_dataset = self.create_training_dataset(queries, responses, rewards)\n",
    "                \n",
    "                # Create optimized data collator for multi-GPU\n",
    "                from transformers import DataCollatorWithPadding\n",
    "                data_collator = DataCollatorWithPadding(\n",
    "                    self.agent.tokenizer,\n",
    "                    pad_to_multiple_of=8\n",
    "                )\n",
    "                \n",
    "                # Clear references to free memory\n",
    "                del queries, responses\n",
    "                for i in range(self.num_gpus):\n",
    "                    with torch.cuda.device(i):\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Print GPU status before training\n",
    "                print(\"GPU memory status before training:\")\n",
    "                self.print_gpu_memory_stats()\n",
    "                \n",
    "                # Initialize trainer for this batch\n",
    "                self.agent.initialize_trainer(\n",
    "                    train_dataset=train_dataset,\n",
    "                    data_collator=data_collator\n",
    "                )\n",
    "                \n",
    "                print(\"Starting PPO training...\")\n",
    "                \n",
    "                # Temporary adjust training parameters\n",
    "                original_num_train_epochs = self.agent.ppo_config.num_train_epochs\n",
    "                self.agent.ppo_config.num_train_epochs = 1\n",
    "                \n",
    "                # Run training with automatic multi-GPU distribution\n",
    "                self.agent.trainer.train()\n",
    "                \n",
    "                # Restore settings\n",
    "                self.agent.ppo_config.num_train_epochs = original_num_train_epochs\n",
    "                \n",
    "                # Log sample\n",
    "                if len(batch) > 0:\n",
    "                    sample = batch[0]\n",
    "                    print(f\"\\nSample response: {sample['response_text'][:100]}...\")\n",
    "                    print(f\"Total reward: {sample['reward']:.4f}\")\n",
    "                \n",
    "                # Free memory after training\n",
    "                del self.agent.trainer\n",
    "                self.agent.trainer = None\n",
    "                for i in range(self.num_gpus):\n",
    "                    with torch.cuda.device(i):\n",
    "                        torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "                # Print GPU status after training\n",
    "                print(\"GPU memory status after training:\")\n",
    "                self.print_gpu_memory_stats()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "                # Free memory after error\n",
    "                if hasattr(self.agent, 'trainer') and self.agent.trainer is not None:\n",
    "                    del self.agent.trainer\n",
    "                    self.agent.trainer = None\n",
    "                for i in range(self.num_gpus):\n",
    "                    with torch.cuda.device(i):\n",
    "                        torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (iteration + 1) % 5 == 0:\n",
    "                try:\n",
    "                    save_path = f\"{self.output_dir}/checkpoint-{iteration+1}\"\n",
    "                    self.agent.policy_model.save_pretrained(save_path)\n",
    "                    self.agent.tokenizer.save_pretrained(save_path)\n",
    "                    print(f\"Model saved to {save_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving model: {e}\")\n",
    "            \n",
    "            # Logging\n",
    "            print(f\"\\nIteration {iteration+1}/{num_iterations}\")\n",
    "            print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "            print(f\"Buffer Size: {len(self.buffer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
