{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from collections import deque\n",
    "from datasets import Dataset\n",
    "import re\n",
    "\n",
    "# First, apply our monkey patch to fix the GRPOTrainer\n",
    "def apply_no_pop_fix():\n",
    "    \"\"\"\n",
    "    Apply a fix to prevent GRPOTrainer from popping messages from prompt.\n",
    "    Call this before creating your GRPOTrainer instance.\n",
    "    \"\"\"\n",
    "    from trl.data_utils import is_conversational\n",
    "    \n",
    "    # Store the original method\n",
    "    original_method = GRPOTrainer._generate_and_score_completions\n",
    "    \n",
    "    # Define our patched method\n",
    "    def patched_generate_and_score_completions(self, inputs):\n",
    "        device = self.accelerator.device\n",
    "        prompts = [x[\"prompt\"] for x in inputs]\n",
    "        \n",
    "        # Make a deep copy of prompts if conversational to prevent modification\n",
    "        is_conv = is_conversational(inputs[0])\n",
    "        if is_conv:\n",
    "            # Deep copy each prompt to prevent the original method from modifying them\n",
    "            copied_inputs = []\n",
    "            for inp in inputs:\n",
    "                copied_prompt = []\n",
    "                for message in inp[\"prompt\"]:\n",
    "                    copied_prompt.append({k: v for k, v in message.items()})\n",
    "                \n",
    "                copied_inp = {k: v for k, v in inp.items()}\n",
    "                copied_inp[\"prompt\"] = copied_prompt\n",
    "                copied_inputs.append(copied_inp)\n",
    "                \n",
    "            # Call original with copied inputs\n",
    "            result = original_method(self, copied_inputs)\n",
    "            return result\n",
    "            \n",
    "        # For non-conversational, just use the original method\n",
    "        return original_method(self, inputs)\n",
    "    \n",
    "    # Apply the monkey patch\n",
    "    GRPOTrainer._generate_and_score_completions = patched_generate_and_score_completions\n",
    "    \n",
    "    print(\"âœ… Applied fix to prevent GRPOTrainer from popping messages from prompt.\")\n",
    "    \n",
    "    return GRPOTrainer\n",
    "\n",
    "# Apply the fix\n",
    "FixedGRPOTrainer = apply_no_pop_fix()\n",
    "\n",
    "# System prompt for the portfolio manager\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "Example:\n",
    "Market Context: PMI:52.6, inflation:5.1%, unemployment:3.8%\n",
    "\n",
    "<macro state>\n",
    "Economy is showing strength with PMI in expansion territory, but inflation remains elevated.\n",
    "</macro state>\n",
    "<reasoning>\n",
    "The PMI at 52.6 indicates expansion, which is positive for equities. However, inflation at 5.1% is above the Fed's target, suggesting possible rate hikes. Employment is strong at 3.8%, supporting consumer spending.\n",
    "</reasoning>\n",
    "<positioning>\n",
    "0.3\n",
    "</positioning>\n",
    "\n",
    "You must respond in the above XML format.\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data(df):\n",
    "    def prepare_prompt(df):\n",
    "        df['prompt'] = df.apply(lambda row: [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT.strip()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Market Context:{', '.join(f'{k}:{v}' for k, v in row.drop('date', errors='ignore').items())}\"\n",
    "            }\n",
    "        ], axis=1)\n",
    "        return df\n",
    "\n",
    "    df = prepare_prompt(df)\n",
    "    df['returns'] = df['close'].pct_change().shift(-1)\n",
    "    train_dataset = df[['prompt', 'returns']]\n",
    "    data = Dataset.from_pandas(train_dataset, preserve_index=False)\n",
    "    return data\n",
    "\n",
    "def extract_positioning(text):\n",
    "    try:\n",
    "        match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            # Extract the value and clean any whitespace\n",
    "            value = match.group(1).strip()\n",
    "            return float(value)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Positioning extraction error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def format_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    # Debug output for first few completions\n",
    "    for i, content in enumerate(completion_contents[:3]):\n",
    "        print(f\"Completion {i}: {content[:100]}...\")\n",
    "    \n",
    "    matches = [re.search(pattern, content, re.DOTALL) is not None for content in completion_contents]\n",
    "    # Stronger reward/penalty for format\n",
    "    return [3.0 if match else -2.0 for match in matches]\n",
    "\n",
    "def return_reward(prompts, completions, returns, **kwargs):\n",
    "    \"\"\"Main reward function combining multiple factors\"\"\"\n",
    "    rewards = []\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    for i, completion in enumerate(completion_contents):\n",
    "        try:\n",
    "            position = extract_positioning(completion)\n",
    "            # Only reward correct format and positioning\n",
    "            if re.search(pattern, completion, re.DOTALL):\n",
    "                rewards.append(position * returns[i % len(returns)])\n",
    "            else:\n",
    "                rewards.append(-1.0)  # Penalty for incorrect format\n",
    "        except Exception as e:\n",
    "            print(f\"Reward calculation error: {e} for completion: {completion[:100]}...\")\n",
    "            rewards.append(-1.0)\n",
    "    return rewards\n",
    "\n",
    "# Load and prepare data\n",
    "train = pd.read_csv('../train.csv')\n",
    "data = prepare_data(train)\n",
    "\n",
    "# Model configuration\n",
    "model_path = './huggingface_mirror/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775'\n",
    "output_dir = \"outputs/Qwen-2.5-0.5B-GRPO-trader\"\n",
    "run_name = \"Qwen-2.5-0.5B-GRPO-trader\"\n",
    "\n",
    "# GRPO Configuration with increased lengths and more exploration\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=512,  # Increased from 256\n",
    "    max_completion_length=512,  # Increased from 256\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    temperature=0.7,  # Increased from 0.1 for better exploration\n",
    "    num_iterations=2,  # Now safe to use iterations > 1\n",
    "    reward_weights=[1.0, 1.0],  # Equal weight for format and returns\n",
    "    log_completions=True  # Enable logging completions for debugging\n",
    ")\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True  # Important for Qwen models\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True  # Important for Qwen models\n",
    ")\n",
    "\n",
    "# Ensure tokenizer has pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add simple callback to print sample completions at various steps\n",
    "class CompletionDebugCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 50 == 0:\n",
    "            print(f\"\\n=== Step {state.global_step} Sample Completions ===\")\n",
    "            # Generate a sample completion with the current model state\n",
    "            sample = data[0]\n",
    "            messages = sample['prompt']\n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            output = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            print(f\"Output: {output}\\n\")\n",
    "            \n",
    "            # Check if it matches the format\n",
    "            pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "            has_format = re.search(pattern, output, re.DOTALL) is not None\n",
    "            print(f\"Has correct format: {has_format}\")\n",
    "            \n",
    "            # Try to extract positioning\n",
    "            try:\n",
    "                position = extract_positioning(output)\n",
    "                print(f\"Extracted positioning: {position}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract positioning: {e}\")\n",
    "\n",
    "# Use our fixed trainer\n",
    "trainer = FixedGRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        format_reward_func,\n",
    "        return_reward\n",
    "    ],\n",
    "    args=grpo_config,\n",
    "    train_dataset=data,\n",
    ")\n",
    "\n",
    "# Add the callback\n",
    "trainer.add_callback(CompletionDebugCallback())\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
