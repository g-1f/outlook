{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModelAdapter:\n",
    "    \"\"\"\n",
    "    Base class for language model adapters.\n",
    "    \n",
    "    This abstract class defines the interface that all model adapters\n",
    "    should implement.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = \"BaseModel\"):\n",
    "        \"\"\"\n",
    "        Initialize the model adapter.\n",
    "        \n",
    "        Args:\n",
    "            model_type: Identifier for the model type\n",
    "        \"\"\"\n",
    "        self._type = model_type\n",
    "        \n",
    "    @property\n",
    "    def model_type(self) -> str:\n",
    "        \"\"\"Get the type identifier for this model adapter.\"\"\"\n",
    "        return self._type\n",
    "        \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Make a synchronous call to the language model.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt/question to send\n",
    "            stop: Optional stop sequences\n",
    "            **kwargs: Additional parameters\n",
    "            \n",
    "        Returns:\n",
    "            Model's response as a string\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement _call method\")\n",
    "        \n",
    "    async def _acall(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Make an asynchronous call to the language model.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt/question to send\n",
    "            stop: Optional stop sequences\n",
    "            **kwargs: Additional parameters\n",
    "            \n",
    "        Returns:\n",
    "            Model's response as a string\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement _acall method\")\n",
    "        \n",
    "    def invoke(self, input: str, config: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Invoke the model with the given input.\n",
    "        \n",
    "        Args:\n",
    "            input: The prompt to send to the model\n",
    "            config: Optional configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            The model's response\n",
    "        \"\"\"\n",
    "        # Extract parameters from config\n",
    "        config = config or {}\n",
    "        \n",
    "        # Make the call with config parameters\n",
    "        kwargs = {k: v for k, v in config.items()}\n",
    "        return self._call(prompt=input, **kwargs)\n",
    "\n",
    "    async def ainvoke(self, input: str, config: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronously invoke the model with the given input.\n",
    "        \n",
    "        Args:\n",
    "            input: The prompt to send to the model\n",
    "            config: Optional configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            The model's response\n",
    "        \"\"\"\n",
    "        # Extract parameters from config\n",
    "        config = config or {}\n",
    "        \n",
    "        # Make the call with config parameters\n",
    "        kwargs = {k: v for k, v in config.items()}\n",
    "        return await self._acall(prompt=input, **kwargs)\n",
    "        \n",
    "    def batch(\n",
    "        self, \n",
    "        inputs: List[str], \n",
    "        config: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process a batch of inputs.\n",
    "        \n",
    "        Args:\n",
    "            inputs: List of prompts to process\n",
    "            config: Optional configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            List of responses\n",
    "        \"\"\"\n",
    "        return [self.invoke(input, config) for input in inputs]\n",
    "\n",
    "    async def abatch(\n",
    "        self, \n",
    "        inputs: List[str], \n",
    "        config: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Asynchronously process a batch of inputs.\n",
    "        \n",
    "        Args:\n",
    "            inputs: List of prompts to process\n",
    "            config: Optional configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            List of responses\n",
    "        \"\"\"\n",
    "        return [await self.ainvoke(input, config) for input in inputs]\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    \"\"\"\n",
    "    LLM (Language Model) adapter for AWM infrastructure.\n",
    "    \n",
    "    This class provides an interface for interacting with language models\n",
    "    through the CMS API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cms: Optional[CMS] = None):\n",
    "        \"\"\"\n",
    "        Initialize the LLM adapter.\n",
    "        \n",
    "        Args:\n",
    "            cms: Optional pre-configured CMS client instance\n",
    "        \"\"\"\n",
    "        self.cms = cms\n",
    "        self._type = \"CMSLanguageModel\"\n",
    "        \n",
    "    @property\n",
    "    def model_type(self) -> str:\n",
    "        \"\"\"Get the type identifier for this model.\"\"\"\n",
    "        return self._type\n",
    "\n",
    "    @classmethod\n",
    "    def build_cms(\n",
    "        cls,\n",
    "        app_id: Optional[str] = None,\n",
    "        env: Optional[str] = None,\n",
    "        model_name: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        log_level: str = \"INFO\",\n",
    "        **kwargs: Any,\n",
    "    ) -> CMS:\n",
    "        \"\"\"\n",
    "        Build a CMS client with the specified configuration.\n",
    "        \n",
    "        This factory method creates a properly configured CMS client with the\n",
    "        given parameters and preference settings.\n",
    "        \n",
    "        Args:\n",
    "            app_id: Application identifier\n",
    "            env: Environment name (dev, uat, prod, etc.)\n",
    "            model_name: Name of the inference model to use\n",
    "            temperature: Temperature setting for generation\n",
    "            log_level: Logging level for the CMS client\n",
    "            **kwargs: Additional preference parameters\n",
    "            \n",
    "        Returns:\n",
    "            Configured CMS client\n",
    "        \"\"\"\n",
    "        # Get preferences with the specified model and temperature\n",
    "        preferences = get_preferences(\n",
    "            inference_model_name=model_name, \n",
    "            temperature=temperature, \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Create and return CMS client\n",
    "        return CMS(\n",
    "            app_id=app_id,\n",
    "            env=env,\n",
    "            preferences=preferences,\n",
    "            log_level=log_level\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def init(\n",
    "        cls,\n",
    "        app_id: str,\n",
    "        env: Optional[str] = None,\n",
    "        model_name: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        log_level: str = \"INFO\",\n",
    "        **kwargs: Any,\n",
    "    ) -> \"LLM\":\n",
    "        \"\"\"\n",
    "        Create a configured LLM adapter with a new CMS client.\n",
    "        \n",
    "        Factory method for creating an LLM adapter with sensible defaults.\n",
    "        \n",
    "        Args:\n",
    "            app_id: Application identifier (required)\n",
    "            env: Environment name\n",
    "            model_name: Name of the inference model\n",
    "            temperature: Temperature setting\n",
    "            log_level: Logging level\n",
    "            **kwargs: Additional preferences\n",
    "            \n",
    "        Returns:\n",
    "            Configured LLM adapter\n",
    "        \"\"\"\n",
    "        cms = cls.build_cms(\n",
    "            app_id=app_id,\n",
    "            env=env,\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            log_level=log_level,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        return cls(cms)\n",
    "        \n",
    "    # Stop tokens handling removed as it's unnecessary for API-based implementations\n",
    "        \n",
    "    def invoke(self, input: str, config: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Invoke the model with the given input.\n",
    "        \n",
    "        Args:\n",
    "            input: The prompt to send to the model\n",
    "            config: Optional configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            The model's response\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If CMS client is not initialized\n",
    "        \"\"\"\n",
    "        if self.cms is None:\n",
    "            raise ValueError(\"CMS client not initialized\")\n",
    "            \n",
    "        # Extract parameters from config\n",
    "        config = config or {}\n",
    "        stop = config.get(\"stop\")\n",
    "        user_sso = config.get(\"user_sso\")\n",
    "        \n",
    "        # Get authentication token if not provided\n",
    "        if not user_sso:\n",
    "            try:\n",
    "                user_sso = gs_auth.get_sso()\n",
    "                logger.debug(\"Using automatically retrieved sso token\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to get sso token: {str(e)}\")\n",
    "            \n",
    "        # Call the CMS inference API\n",
    "        answer, _, error = self.cms.inference(\n",
    "            user_sso=user_sso, \n",
    "            question=input\n",
    "        )\n",
    "\n",
    "        # Handle errors\n",
    "        if error is not None:\n",
    "            raise RuntimeError(f\"Error in inference: {error}\")\n",
    "\n",
    "        # Stop token handling is done server-side in the API\n",
    "            \n",
    "        return answer\n",
    "\n",
    "    async def ainvoke(self, input: str, config: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronously invoke the model with the given input.\n",
    "        \n",
    "        Args:\n",
    "            input: The prompt to send to the model\n",
    "            config: Optional configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            The model's response\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If CMS client is not initialized\n",
    "        \"\"\"\n",
    "        if self.cms is None:\n",
    "            raise ValueError(\"CMS client not initialized\")\n",
    "            \n",
    "        # Extract parameters from config\n",
    "        config = config or {}\n",
    "        stop = config.get(\"stop\")\n",
    "        user_sso = config.get(\"user_sso\")\n",
    "        \n",
    "        # Get authentication token if not provided\n",
    "        if not user_sso:\n",
    "            try:\n",
    "                user_sso = gs_auth.get_sso()\n",
    "                logger.debug(\"Using automatically retrieved sso token\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to get sso token: {str(e)}\")\n",
    "            \n",
    "        # Call the CMS inference API\n",
    "        answer, _, error = await self.cms.async_inference(\n",
    "            user_sso=user_sso, \n",
    "            question=input\n",
    "        )\n",
    "\n",
    "        # Handle errors\n",
    "        if error is not None:\n",
    "            raise RuntimeError(f\"Error in inference: {error}\")\n",
    "\n",
    "        # Stop token handling is done server-side in the API\n",
    "            \n",
    "        return answer\n",
    "        \n",
    "    def batch(\n",
    "        self, \n",
    "        inputs: List[str], \n",
    "        config: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process a batch of inputs.\n",
    "        \n",
    "        Args:\n",
    "            inputs: List of prompts to process\n",
    "            config: Optional configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            List of responses\n",
    "        \"\"\"\n",
    "        return [self.invoke(input, config) for input in inputs]\n",
    "\n",
    "    async def abatch(\n",
    "        self, \n",
    "        inputs: List[str], \n",
    "        config: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Asynchronously process a batch of inputs.\n",
    "        \n",
    "        Args:\n",
    "            inputs: List of prompts to process\n",
    "            config: Optional configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            List of responses\n",
    "        \"\"\"\n",
    "        return [await self.ainvoke(input, config) for input in inputs]\n",
    "        \n",
    "    def stream(self, input: str, config: Optional[Dict[str, Any]] = None) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Stream responses from the language model.\n",
    "        \n",
    "        Note: Streaming is not currently supported by the CMS API.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: This functionality is not supported\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"Streaming is not supported for this model\"\n",
    "        )\n",
    "        \n",
    "    async def astream(self, input: str, config: Optional[Dict[str, Any]] = None) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Asynchronously stream responses from the language model.\n",
    "        \n",
    "        Note: Streaming is not currently supported by the CMS API.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: This functionality is not supported\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"Async streaming is not supported for this model\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
