{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from collections import deque\n",
    "from datasets import Dataset\n",
    "\n",
    "# =============================================================================\n",
    "# 1. GRPO-Compatible Trading Environment\n",
    "# =============================================================================\n",
    "\n",
    "class GRPOTradingEnv:\n",
    "    def __init__(self, data: pd.DataFrame, initial_cash: float = 1e6):\n",
    "        self.data = data\n",
    "        self.initial_cash = initial_cash\n",
    "        self._reset_state()\n",
    "        \n",
    "    def _reset_state(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.position = 0.0\n",
    "        self.portfolio_values = [self.initial_cash]\n",
    "        self.news_window = deque(maxlen=7)\n",
    "        \n",
    "    def step(self, action: float) -> Tuple[float, Dict]:\n",
    "        prev_price = self.data.iloc[self.current_step]['close']\n",
    "        self.current_step += 1\n",
    "        current_price = self.data.iloc[self.current_step]['close']\n",
    "        \n",
    "        # Calculate returns\n",
    "        daily_return = (current_price/prev_price - 1) * self.position\n",
    "        new_value = self.cash + self.position * current_price\n",
    "        self.portfolio_values.append(new_value)\n",
    "        \n",
    "        return daily_return, {\n",
    "            'new_value': new_value,\n",
    "            'return': daily_return,\n",
    "            'position': self.position,\n",
    "            'step': self.current_step\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Reward Functions for GRPO\n",
    "# =============================================================================\n",
    "\n",
    "def risk_adjusted_reward(example):\n",
    "    \"\"\"Main reward function combining multiple factors\"\"\"\n",
    "    return {\n",
    "        'rewards': [example['return'] / (example['volatility'] + 1e-6) + 0.1 * example['position_persistence']]\n",
    "    }\n",
    "\n",
    "def position_stability_reward(example):\n",
    "    \"\"\"Reward for maintaining positions\"\"\"\n",
    "    return {'rewards': [0.5 * (1 - abs(example['position_change']))]}\n",
    "\n",
    "# =============================================================================\n",
    "# 3. GRPO Training Configuration\n",
    "# =============================================================================\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "output_dir = \"outputs/Qwen-0.5B-GRPO\"\n",
    "run_name = \"Qwen-0.5B-GRPO-trading\"\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    num_train_epochs=1,\n",
    "    max_grad_norm=0.1,\n",
    "    use_vllm=True,\n",
    "    vllm_gpu_memory_utilization=0.3,\n",
    "    bf16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Model & Dataset Preparation\n",
    "# =============================================================================\n",
    "\n",
    "def create_prompt(row: pd.Series, news_context: List[str]) -> str:\n",
    "    return f\"\"\"Trading Decision for {row['date']}\n",
    "Market Context:\n",
    "- Price: {row['close']:.2f}\n",
    "- 20D Volatility: {row['vol_20d']:.2%}\n",
    "- RSI(14): {row['rsi_14']:.1f}\n",
    "- Position: {row['position']:.2%}\n",
    "\n",
    "Recent News:\n",
    "{\"\\n\".join(news_context)}\n",
    "\n",
    "Output Format (JSON):\n",
    "{{\n",
    "  \"analysis\": \"...\",\n",
    "  \"position_target\": [-1.0 to 1.0],\n",
    "  \"confidence\": [0.0-1.0]\n",
    "}}\"\"\"\n",
    "\n",
    "def process_dataset(data: pd.DataFrame):\n",
    "    dataset = []\n",
    "    env = GRPOTradingEnv(data)\n",
    "    \n",
    "    for i in range(len(data)-1):\n",
    "        news_context = list(env.news_window)\n",
    "        prompt = create_prompt(data.iloc[i], news_context)\n",
    "        \n",
    "        # Store step data for reward calculation\n",
    "        dataset.append({\n",
    "            'prompt': prompt,\n",
    "            'date': data.iloc[i]['date'],\n",
    "            'current_price': data.iloc[i]['close'],\n",
    "            'next_price': data.iloc[i+1]['close'],\n",
    "            'position': env.position,\n",
    "            'volatility': data.iloc[i]['vol_20d']\n",
    "        })\n",
    "        \n",
    "        # Update environment\n",
    "        env.news_window.append(data.iloc[i]['news'])\n",
    "    \n",
    "    return Dataset.from_pandas(pd.DataFrame(dataset))\n",
    "\n",
    "# =============================================================================\n",
    "# 5. GRPO Training Setup\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    data = pd.read_csv('sp500_daily.csv', parse_dates=['date'])\n",
    "    dataset = process_dataset(data)\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize GRPO Trainer\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=grpo_config,\n",
    "        reward_funcs=[risk_adjusted_reward, position_stability_reward],\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
