{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def process_tools(tools):\n",
    "        \"\"\"\n",
    "        Process a list of tools into Gemini API compatible schemas.\n",
    "        \n",
    "        Args:\n",
    "            tools: List of tools that can be:\n",
    "                - Functions (schema extracted automatically)\n",
    "                - Tuples of (function, params_model, [response_model])\n",
    "                \n",
    "        Returns:\n",
    "            Tuple of (processed_tools, tool_callables)\n",
    "            - processed_tools: List of tool schemas\n",
    "            - tool_callables: Dict mapping tool names to callable functions\n",
    "        \"\"\"\n",
    "        import inspect\n",
    "        \n",
    "        processed_tools = []\n",
    "        tool_callables = {}\n",
    "        \n",
    "        if not tools:\n",
    "            return processed_tools, tool_callables\n",
    "        \n",
    "        for tool in tools:\n",
    "            if isinstance(tool, tuple) and len(tool) >= 2 and callable(tool[0]):\n",
    "                # Case: (function, params_model, [response_model]) tuple\n",
    "                function = tool[0]\n",
    "                params_model = tool[1]\n",
    "                response_model = tool[2] if len(tool) > 2 else None\n",
    "                \n",
    "                # Convert Pydantic models to schema\n",
    "                func_name = function.__name__\n",
    "                func_description = inspect.getdoc(function) or \"\"\n",
    "                \n",
    "                params_schema = GeminiUtils.pydantic_to_schema(params_model)\n",
    "                \n",
    "                # Create schema\n",
    "                schema = {\n",
    "                    \"name\": func_name,\n",
    "                    \"description\": func_description,\n",
    "                    \"parameters\": params_schema\n",
    "                }\n",
    "                \n",
    "                # Add response schema if provided\n",
    "                if response_model:\n",
    "                    response_schema = GeminiUtils.pydantic_to_schema(response_model)\n",
    "                    schema[\"response\"] = response_schema\n",
    "                \n",
    "                processed_tools.append(schema)\n",
    "                tool_callables[func_name] = function\n",
    "                \n",
    "            elif callable(tool):\n",
    "                # Case: Plain function\n",
    "                schema = GeminiUtils.python_to_function(tool)\n",
    "                processed_tools.append(schema)\n",
    "                tool_callables[schema[\"name\"]] = tool\n",
    "                \n",
    "            else:\n",
    "                logger.warning(f\"Ignoring unsupported tool type: {type(tool)}\")\n",
    "        \n",
    "        return processed_tools, tool_callables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @classmethod\n",
    "    def init(\n",
    "        cls,\n",
    "        app_id: str,\n",
    "        env: Optional[str] = None,\n",
    "        model_name: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n",
    "        tools: Optional[List[Union[Callable, Tuple[Callable, Type[BaseModel], Optional[Type[BaseModel]]]]]] = None,\n",
    "        provider_type: Literal[\"cms\", \"dsml\"] = \"cms\",\n",
    "        log_level: Optional[Literal[\"INFO\", \"DEBUG\"]] = \"INFO\",\n",
    "        **kwargs,\n",
    "    ) -> \"LLM\":\n",
    "        \"\"\"\n",
    "        Create and initialize an LLM adapter with default services.\n",
    "        \n",
    "        Args:\n",
    "            app_id: Application identifier\n",
    "            env: Environment name\n",
    "            model_name: Name of the language model to use\n",
    "            temperature: Temperature parameter for generation\n",
    "            reasoning_effort: Reasoning effort level\n",
    "            tools: Tools that can be used by the model. Can be:\n",
    "                - Functions (schema extracted automatically)\n",
    "                - Tuples of (function, params_model, response_model)\n",
    "            provider_type: Default provider type to use\n",
    "            log_level: Logging level\n",
    "            **kwargs: Additional configuration options\n",
    "            \n",
    "        Returns:\n",
    "            Configured LLM adapter\n",
    "        \"\"\"\n",
    "        # Get preferences\n",
    "        preferences = get_preferences(\n",
    "            inference_model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            reasoning_effort=reasoning_effort,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        # Process tools to standardized format using GeminiUtils\n",
    "        processed_tools, tool_callables = GeminiUtils.process_tools(tools)\n",
    "        \n",
    "        if provider_type == \"cms\":\n",
    "            # Create CMS service with processed tools\n",
    "            cms = ConversationManagementService(\n",
    "                app_id=app_id,\n",
    "                env=env,\n",
    "                preferences=preferences,\n",
    "                tools=processed_tools,\n",
    "                log_level=log_level,\n",
    "                **kwargs,\n",
    "            )\n",
    "            \n",
    "            # Register tool implementations\n",
    "            for name, func in tool_callables.items():\n",
    "                cms.tool_callables[name] = func\n",
    "            \n",
    "            # Create LLM instance\n",
    "            return cls(\n",
    "                app_id=app_id,\n",
    "                env=env,\n",
    "                cms=cms,\n",
    "                provider_type=provider_type,\n",
    "            )\n",
    "        elif provider_type == \"dsml\":\n",
    "            # DSML support would be implemented here\n",
    "            raise NotImplementedError(\"DSML provider not yet implemented\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider type: {provider_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
