{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class FlagTraderAgent:\n",
    "    def __init__(self, model_name, device='cuda:0'):\n",
    "        self.device = device\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # PPO Configuration\n",
    "        self.config = PPOConfig(\n",
    "            model_name=model_name,\n",
    "            learning_rate=5e-5,\n",
    "            batch_size=32,\n",
    "            mini_batch_size=4,\n",
    "            gradient_accumulation_steps=8,\n",
    "            num_train_epochs=3,\n",
    "            vf_coef=0.1,\n",
    "            kl_coef=0.05,\n",
    "            cliprange=0.2,\n",
    "            gamma=0.99,\n",
    "            lam=0.95,\n",
    "            output_dir=\"ppo_output\",\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        # Load policy model with value head (actor-critic)\n",
    "        self.policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            model_name, torch_dtype=torch.float16, device_map='cuda:0'\n",
    "        )\n",
    "\n",
    "        # Load reference model (for PPO's KL penalty)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, torch_dtype=torch.float16, device_map='cuda:0'\n",
    "        )\n",
    "\n",
    "        # Simple Reward Model (just returns reward as is)\n",
    "        class SimpleRewardModel(torch.nn.Module):\n",
    "            def forward(self, input_ids, attention_mask=None):\n",
    "                return torch.ones((input_ids.shape[0], 1), device=input_ids.device)\n",
    "\n",
    "        self.reward_model = SimpleRewardModel().to('cuda:0')\n",
    "\n",
    "        # Value model is embedded within policy_model (provided by trl)\n",
    "        self.value_model = self.policy_model\n",
    "\n",
    "        # PPO Trainer\n",
    "        self.trainer = PPOTrainer(\n",
    "            args=self.config,\n",
    "            model=self.policy_model,\n",
    "            ref_model=self.policy_model,  # using same as reference\n",
    "            reward_model=self.reward_model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            train_dataset=None,\n",
    "            value_model=self.value_model,\n",
    "            data_collator=None\n",
    "        )\n",
    "\n",
    "    def generate_prompt(self, state):\n",
    "        return f\"Trading Task:\\nState: {state}\\nActions:[Buy,Sell,Hold]\\nDecision:\"\n",
    "\n",
    "    def predict(self, state):\n",
    "        prompt = self.generate_prompt(state)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.policy_model.generate(**inputs, max_new_tokens=16, temperature=0.7, do_sample=True)\n",
    "        decision = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return decision.split(\"Decision:\")[-1].strip()\n",
    "\n",
    "class FlagTraderEnv(gym.Env):\n",
    "    def __init__(self, data, initial_balance=10000):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.initial_balance = initial_balance\n",
    "        self.action_space = spaces.Discrete(3)  # Buy, Hold, Sell\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=data.shape[1:])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.asset = 0\n",
    "        return self.data[self.current_step]\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_value = self.balance + self.asset * self.data[self.current_step, -1]\n",
    "        price = self.data[self.current_step, -1]\n",
    "\n",
    "        if action == 0 and self.balance >= price:  # Buy\n",
    "            self.asset += self.balance // price\n",
    "            self.balance %= price\n",
    "        elif action == 2 and self.asset > 0:  # Sell\n",
    "            self.balance += self.asset * price\n",
    "            self.asset = 0\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        next_state = self.data[self.current_step]\n",
    "\n",
    "        portfolio_value = self.balance + self.asset * self.data[self.current_step, -1]\n",
    "        reward = portfolio_value - prev_value  # return-based reward\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def action_str_to_num(action_str):\n",
    "    return {'Buy': 0, 'Hold': 1, 'Sell': 2}.get(action_str, 1)\n",
    "\n",
    "def orchestrate_training(agent, env, epochs=3, steps_per_epoch=200):\n",
    "    for epoch in range(epochs):\n",
    "        experiences = []\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in tqdm(range(steps_per_epoch), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            action_str = agent.predict(state)\n",
    "            action_num = action_str_to_num(action_str)\n",
    "            next_state, reward, done, _ = env.step(action_num)\n",
    "\n",
    "            query = agent.generate_prompt(state)\n",
    "            experiences.append({\n",
    "                \"query\": query,\n",
    "                \"response\": action_str,\n",
    "                \"reward\": reward\n",
    "            })\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "        # Convert experiences to Dataset\n",
    "        dataset = Dataset.from_list(experiences)\n",
    "        agent.trainer.train_dataset = dataset\n",
    "\n",
    "        # PPO Train step\n",
    "        agent.trainer.train()\n",
    "\n",
    "        avg_reward = total_reward / steps_per_epoch\n",
    "        print(f\"Epoch {epoch+1}: Avg Reward = {avg_reward:.2f}\")\n",
    "\n",
    "# Initialize random market data (replace with real data for real cases)\n",
    "market_data = np.random.rand(1000, 10)  # 10 features per timestep\n",
    "\n",
    "# Initialize agent and environment\n",
    "env = FlagTraderEnv(market_data)\n",
    "agent = FlagTraderAgent(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "# Run PPO training\n",
    "orchestrate_training(agent, env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
