{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Optional, Literal, Union\n",
    "from typing_extensions import TypeAlias, NotRequired\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "class Environment(str, Enum):\n",
    "    DEV = \"dev\"\n",
    "    UAT = \"uat\"\n",
    "    PROD = \"prod\"\n",
    "\n",
    "class PromptType(str, Enum):\n",
    "    ECT = \"ECT\"\n",
    "    CUSTOMER = \"CUSTOMER\"\n",
    "    SUPPLIER = \"SUPPLIER\"\n",
    "\n",
    "class EarningsMetadata(BaseModel):\n",
    "    quarter_s: str\n",
    "    year_s: str\n",
    "    company_s: str\n",
    "    doc_type_s: str\n",
    "    tags_s: List[str]\n",
    "    event_time_s: str\n",
    "    companyId_s: str\n",
    "    tickers_s: List[str]\n",
    "    cusips_s: List[str]\n",
    "    isins_s: List[str]\n",
    "    sedols_s: List[str]\n",
    "    document_date_s: str\n",
    "    objecturl: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        frozen = True\n",
    "\n",
    "class AnalysisSection(BaseModel):\n",
    "    key_points: List[str]\n",
    "    summary: str\n",
    "\n",
    "    class Config:\n",
    "        frozen = True\n",
    "\n",
    "class PromptRequest(TypedDict):\n",
    "    prompt: str\n",
    "    company: str\n",
    "    type: PromptType\n",
    "    bucket: int\n",
    "    model: str\n",
    "\n",
    "class PromptResponse(TypedDict):\n",
    "    result: Dict[str, AnalysisSection]\n",
    "    type: PromptType\n",
    "    heading: str\n",
    "    confidence: float\n",
    "    bucket: int\n",
    "\n",
    "class MetadataResponse(TypedDict):\n",
    "    company: str\n",
    "    period: str\n",
    "    date: str\n",
    "    ticker: NotRequired[str]\n",
    "\n",
    "class ECTSummaryResponse(TypedDict):\n",
    "    metadata: MetadataResponse\n",
    "    analysis: Dict[str, AnalysisSection]\n",
    "\n",
    "ErrorResponse: TypeAlias = Dict[Literal[\"error\"], str]\n",
    "ECTResponse: TypeAlias = Union[List[ECTSummaryResponse], List[ErrorResponse]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECTAnalyzer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        gssso_token: str, \n",
    "        config: Optional[ECTConfig] = None,\n",
    "        environment: Optional[str] = None\n",
    "    ):\n",
    "        self.gssso_token = gssso_token\n",
    "        self.config = config or get_config(environment)\n",
    "        self.llm = ConverseAILangchainLLM.from_defaults(\n",
    "            app_id=\"fluentai\",\n",
    "            model_name=self.config.model.name\n",
    "        )\n",
    "        ect_logger.setLevel(self.config.log_level)\n",
    "\n",
    "    @log_errors(ect_logger)\n",
    "    async def get_earnings_by_company_id(self, company_id: str) -> List[EarningsMetadata]:\n",
    "        ect_logger.info(f\"Fetching earnings for company: {company_id}\")\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(\n",
    "                self._build_url(company_id),\n",
    "                headers=self._get_headers(),\n",
    "                timeout=self.config.api.timeout\n",
    "            ) as response:\n",
    "                if response.status != 200:\n",
    "                    raise APIError(\n",
    "                        f\"Failed to fetch earnings data\", \n",
    "                        status_code=response.status\n",
    "                    )\n",
    "                    \n",
    "                content = await response.text()\n",
    "                documents = self._parse_documents(content)\n",
    "                \n",
    "                if not documents:\n",
    "                    raise DocumentNotFoundError(f\"No documents found for company {company_id}\")\n",
    "                    \n",
    "                ect_logger.info(f\"Found {len(documents)} documents\")\n",
    "                return documents\n",
    "\n",
    "    def generate_ect_prompts(\n",
    "        self, \n",
    "        ect: str, \n",
    "        id_bb_company: str, \n",
    "        html_prompt: str = \"\"\n",
    "    ) -> List[PromptRequest]:\n",
    "        items = [(k, v) for k, v in self.config.headings]\n",
    "        prompt_buckets = [\n",
    "            items[i:i + self.config.model.batch_size]\n",
    "            for i in range(0, len(items), self.config.model.batch_size)\n",
    "        ]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"prompt\": self._format_bucket_prompt(bucket, ect, html_prompt),\n",
    "                \"company\": id_bb_company,\n",
    "                \"type\": PromptType.ECT,\n",
    "                \"bucket\": bucket_id,\n",
    "                \"model\": self.config.model.name\n",
    "            }\n",
    "            for bucket_id, bucket in enumerate(prompt_buckets)\n",
    "        ]\n",
    "\n",
    "    async def execute_prompts(self, prompts: List[PromptRequest]) -> List[PromptResponse]:\n",
    "        responses: List[PromptResponse] = []\n",
    "        for prompt in prompts:\n",
    "            try:\n",
    "                response = JsonFormer(\n",
    "                    schema=AnalysisOutput,\n",
    "                    llm=self.llm\n",
    "                ).invoke(prompt[\"prompt\"])\n",
    "                \n",
    "                responses.append({\n",
    "                    \"result\": response.dict(),\n",
    "                    \"type\": prompt[\"type\"],\n",
    "                    \"heading\": prompt.get(\"heading\", \"\"),\n",
    "                    \"confidence\": response.confidence,\n",
    "                    \"bucket\": prompt.get(\"bucket\", 0),\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise PromptExecutionError(f\"Failed to execute prompt: {str(e)}\")\n",
    "                \n",
    "        return responses\n",
    "\n",
    "    async def get_ect_summary(self, company_id: str) -> ECTResponse:\n",
    "        try:\n",
    "            documents = await self.get_earnings_by_company_id(company_id)\n",
    "            latest_doc = max(\n",
    "                documents,\n",
    "                key=lambda x: datetime.strptime(x.document_date_s, \"%Y-%m-%d\")\n",
    "            )\n",
    "            \n",
    "            content = await self.get_earnings_content(latest_doc)\n",
    "            if not content:\n",
    "                return [{\"error\": \"Could not fetch earnings content\"}]\n",
    "            \n",
    "            prompts = self.generate_ect_prompts(content, company_id)\n",
    "            responses = await self.execute_prompts(prompts)\n",
    "            \n",
    "            analysis_sections = {\n",
    "                resp[\"heading\"]: resp[\"result\"]\n",
    "                for resp in responses\n",
    "                if resp[\"confidence\"] > self.config.model.confidence_threshold\n",
    "            }\n",
    "            \n",
    "            if not analysis_sections:\n",
    "                return [{\"error\": \"No high-confidence analysis generated\"}]\n",
    "            \n",
    "            return [{\n",
    "                \"metadata\": {\n",
    "                    \"company\": latest_doc.company_s,\n",
    "                    \"period\": f\"Q{latest_doc.quarter_s} {latest_doc.year_s}\",\n",
    "                    \"date\": latest_doc.document_date_s,\n",
    "                    \"ticker\": latest_doc.tickers_s[0] if latest_doc.tickers_s else None\n",
    "                },\n",
    "                \"analysis\": analysis_sections\n",
    "            }]\n",
    "            \n",
    "        except ECTError as e:\n",
    "            return [{\"error\": str(e)}]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
