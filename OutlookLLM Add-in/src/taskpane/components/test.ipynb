{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from trl.trainer.grpo_trainer import is_conversational, maybe_apply_chat_template\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 1) Setup your system prompt\n",
    "##############################################################################\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "You must respond in the following XML format:\n",
    "\n",
    "<macro state>\n",
    "...\n",
    "</macro state>\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<positioning>\n",
    "...\n",
    "</positioning>\n",
    "\"\"\".strip()\n",
    "\n",
    "##############################################################################\n",
    "# 2) Prepare your data as a conversation\n",
    "#    IMPORTANT: Each \"prompt\" MUST be a list of role-content dicts\n",
    "##############################################################################\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> Dataset:\n",
    "    \"\"\"Creates a 'prompt' column with [system,user] structure.\"\"\"\n",
    "    def row_to_prompt(row: pd.Series) -> List[Dict[str, str]]:\n",
    "        # Example conversation structure\n",
    "        # (System) + (User).\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                # Join all columns except date (if present)\n",
    "                \"content\": \"Market Context:\" + \", \".join(\n",
    "                    f\"{k}:{v}\" for k, v in row.drop(\"date\", errors=\"ignore\").items()\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    df[\"prompt\"] = df.apply(row_to_prompt, axis=1)\n",
    "\n",
    "    # For demonstration, define returns or any numeric reward column\n",
    "    if \"close\" in df.columns:\n",
    "        df[\"returns\"] = df[\"close\"].pct_change().shift(-1)\n",
    "    else:\n",
    "        df[\"returns\"] = 0.05  # dummy reward if you don’t have close\n",
    "    \n",
    "    # Keep only needed columns\n",
    "    train_dataset = df[[\"prompt\", \"returns\"]]\n",
    "    # Convert to a HuggingFace Dataset\n",
    "    data = Dataset.from_pandas(train_dataset, preserve_index=False)\n",
    "    return data\n",
    "\n",
    "##############################################################################\n",
    "# 3) Two reward functions:\n",
    "#    (a) format_reward_func: checks if model output matches the XML pattern\n",
    "#    (b) return_reward: tries to parse <positioning> and scale by 'returns'\n",
    "##############################################################################\n",
    "\n",
    "# Regex fix: Use DOTALL so '.' can match newlines\n",
    "PATTERN = re.compile(\n",
    "    r\"<macro state>.*?</macro state>\\s*<reasoning>.*?</reasoning>\\s*<positioning>.*?</positioning>\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def format_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Gives +2.0 if the entire XML structure is found, else 0.0\n",
    "    (Increased from 0.5 to 2.0 for a stronger training signal.)\n",
    "    \"\"\"\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        for completion in completions\n",
    "    ]\n",
    "    rewards = []\n",
    "    for content in completion_contents:\n",
    "        match = re.search(PATTERN, content)\n",
    "        rewards.append(2.0 if match else 0.0)\n",
    "    return rewards\n",
    "\n",
    "def extract_positioning(text: str) -> float:\n",
    "    try:\n",
    "        match = re.search(r\"<positioning>(.*?)</positioning>\", text, flags=re.DOTALL)\n",
    "        if match:\n",
    "            return float(match.group(1).strip())\n",
    "        return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def return_reward(prompts, completions, returns, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward based on position * returns.\n",
    "    (If <positioning> is missing, that portion is 0.)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        for completion in completions\n",
    "    ]\n",
    "    for comp_text, r in zip(completion_contents, returns):\n",
    "        pos = extract_positioning(comp_text)\n",
    "        # If r is NaN, treat it as 0\n",
    "        r = 0.0 if pd.isna(r) else float(r)\n",
    "        # multiply them\n",
    "        rewards.append(pos * r)\n",
    "    return rewards\n",
    "\n",
    "##############################################################################\n",
    "# 4) Patch the \"pop\" logic so it does NOT remove your system/user messages\n",
    "##############################################################################\n",
    "# We'll override GRPOTrainer by subclassing and modifying _generate_and_score_completions\n",
    "\n",
    "from trl.trainer.grpo_trainer import GRPOTrainer as OriginalGRPOTrainer\n",
    "\n",
    "class GRPOTrainerNoPop(OriginalGRPOTrainer):\n",
    "    def _generate_and_score_completions(\n",
    "        self, inputs: dict[str, Union[torch.Tensor, Any]]\n",
    "    ) -> dict[str, Union[torch.Tensor, Any]]:\n",
    "        device = self.accelerator.device\n",
    "        prompts = [x[\"prompt\"] for x in inputs]\n",
    "\n",
    "        # Convert your conversation to the text that the model sees\n",
    "        # same as in official code: maybe_apply_chat_template\n",
    "        prompts_text = [maybe_apply_chat_template(example, self.processing_class)[\"prompt\"] for example in inputs]\n",
    "\n",
    "        prompt_inputs = self.processing_class(\n",
    "            prompts_text, return_tensors=\"pt\", padding=True, padding_side=\"left\", add_special_tokens=False\n",
    "        )\n",
    "        prompt_inputs = super(OriginalGRPOTrainer, self)._prepare_inputs(prompt_inputs)\n",
    "        prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"], prompt_inputs[\"attention_mask\"]\n",
    "\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n",
    "            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n",
    "\n",
    "        # ----- Next lines are the same as official code, we just skip them for brevity. -----\n",
    "        # We'll show the portion that changes the \"pop\" logic\n",
    "\n",
    "        # Let’s call the parent method to do the main generation & reward steps:\n",
    "        # but we need to replicate the parent's code with the slight fix near\n",
    "        # the \"if is_conversational\" block.\n",
    "\n",
    "        # -- so let's just inline the parent's code from official _generate_and_score_completions:\n",
    "\n",
    "        # (We do NOT do `prompt.pop()`.)\n",
    "\n",
    "        # For demonstration, let's do it fully:\n",
    "\n",
    "        # (1) Generate completions\n",
    "        if self.args.use_vllm:\n",
    "            # ... omitted for brevity ...\n",
    "            raise NotImplementedError(\"For simplicity, skip vLLM in this snippet.\")\n",
    "        else:\n",
    "            with self.accelerator.unwrap_model(self.model) as unwrapped_model:\n",
    "                prompt_completion_ids = unwrapped_model.generate(\n",
    "                    prompt_ids,\n",
    "                    attention_mask=prompt_mask,\n",
    "                    generation_config=self.generation_config,\n",
    "                )\n",
    "        # (2) Separate the prompt tokens from completion tokens\n",
    "        prompt_length = prompt_ids.size(1)\n",
    "        prompt_ids = prompt_completion_ids[:, :prompt_length]\n",
    "        completion_ids = prompt_completion_ids[:, prompt_length:]\n",
    "\n",
    "        # (3) Compute completion_mask by stopping at first EOS\n",
    "        is_eos = completion_ids == self.processing_class.eos_token_id\n",
    "        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)\n",
    "        eos_rows = is_eos.any(dim=1)\n",
    "        eos_idx[eos_rows] = is_eos.int().argmax(dim=1)[eos_rows]\n",
    "        seq_idx = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)\n",
    "        completion_mask = (seq_idx <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "        # (4) Build final attention_mask\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "\n",
    "        # (5) Possibly compute old_per_token_logps, ref_per_token_logps, etc. (skipping details)\n",
    "        old_per_token_logps = None\n",
    "        ref_per_token_logps = None\n",
    "\n",
    "        # (6) Convert completions_ids to text\n",
    "        completions_text = self.processing_class.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "\n",
    "        # (7) If is_conversational => produce completions as a list of role-based dict\n",
    "        #     Notice we remove the old \"pop()\" logic and just build a new assistant message\n",
    "        if is_conversational(inputs[0]):\n",
    "            completions = []\n",
    "            for prompt_data, ctext in zip(prompts, completions_text):\n",
    "                # We do NOT pop anything; we just say:\n",
    "                completions.append([{\"role\": \"assistant\", \"content\": ctext}])\n",
    "        else:\n",
    "            completions = completions_text\n",
    "\n",
    "        # (8) Evaluate all reward functions\n",
    "        # We'll do a minimal version:\n",
    "        device = self.accelerator.device\n",
    "        rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)\n",
    "        for i, reward_func in enumerate(self.reward_funcs):\n",
    "            # Build the kwargs:\n",
    "            keys = [k for k in inputs[0] if k not in [\"prompt\", \"completion\"]]\n",
    "            reward_kwargs = {k: [ex[k] for ex in inputs] for k in keys}\n",
    "            out = reward_func(prompts, completions, **reward_kwargs)\n",
    "            rewards_per_func[:, i] = torch.tensor(out, dtype=torch.float32, device=device)\n",
    "        # Sum weighted reward\n",
    "        total_rewards = rewards_per_func.sum(dim=1)  # if no weighting\n",
    "\n",
    "        # (9) Compute advantage & fill in the final dict.\n",
    "        # For brevity, let's skip the grouping logic. We'll just store the expansions:\n",
    "        return {\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"prompt_mask\": prompt_mask,\n",
    "            \"completion_ids\": completion_ids,\n",
    "            \"completion_mask\": completion_mask,\n",
    "            \"old_per_token_logps\": old_per_token_logps,\n",
    "            \"ref_per_token_logps\": ref_per_token_logps,\n",
    "            \"advantages\": torch.zeros_like(total_rewards),  # placeholder\n",
    "            # You would replicate the normal advantage logic from the official code\n",
    "        }\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 5) Putting it all together in an actual script\n",
    "##############################################################################\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "\n",
    "    # Build a sample DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"close\": [100, 101, 102, 105, 104, 107],\n",
    "        \"news\": [\"Fed hawkish\", \"Earnings up\", \"Geopolitical tension\", \"Jobs data\", \"Another event\", \"Yet more news\"]\n",
    "    })\n",
    "\n",
    "    # Prepare data\n",
    "    dataset = prepare_data(df)\n",
    "\n",
    "    # Model config\n",
    "    model_path = \"Qwen/Qwen2-7B-Chat\"  # example checkpoint\n",
    "    output_dir = \"./grpo-trader-out\"\n",
    "\n",
    "    grpo_config = GRPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        run_name=\"Qwen-1.5B-GRPO-trader\",\n",
    "        learning_rate=5e-6,\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_generations=4,\n",
    "        max_prompt_length=256,\n",
    "        max_completion_length=256,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=1,\n",
    "        temperature=0.1,\n",
    "        remove_unused_columns=False,\n",
    "        # etc. if needed\n",
    "    )\n",
    "\n",
    "    # Load model / tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Override the pop logic by using our `GRPOTrainerNoPop`\n",
    "    trainer = GRPOTrainerNoPop(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[format_reward_func, return_reward],\n",
    "        args=grpo_config,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Now you can do `trainer.model.generate(...)` to see if it produces the XML structure\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
