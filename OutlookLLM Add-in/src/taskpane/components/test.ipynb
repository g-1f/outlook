{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRewardModel(torch.nn.Module):\n",
    "    \"\"\"Simple reward model with the score method required by PPOTrainer\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a dummy parameter so PyTorch considers this a proper module\n",
    "        self.reward_head = torch.nn.Linear(1, 1)\n",
    "        # Need to add base_model_prefix attribute for PolicyAndValueWrapper\n",
    "        self.base_model_prefix = \"transformer\"\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, output_hidden_states=False, \n",
    "               position_ids=None, token_type_ids=None, past_key_values=None, \n",
    "               head_mask=None, inputs_embeds=None, use_cache=None, return_dict=None, **kwargs):\n",
    "        \"\"\"Forward pass that accepts all standard transformer arguments\"\"\"\n",
    "        batch_size = input_ids.shape[0] if input_ids is not None else 1\n",
    "        seq_length = input_ids.shape[1] if input_ids is not None else 1\n",
    "        hidden_size = 768  # Standard hidden size for many models\n",
    "        \n",
    "        # Create fake hidden states\n",
    "        last_hidden_state = torch.ones((batch_size, seq_length, hidden_size), \n",
    "                                      device=input_ids.device if input_ids is not None else 'cpu')\n",
    "        \n",
    "        # Create a dummy Transformers output object with all expected attributes\n",
    "        class DummyOutput:\n",
    "            def __init__(self, hidden_states, last_hidden=None):\n",
    "                self.hidden_states = hidden_states\n",
    "                self.last_hidden_state = last_hidden\n",
    "        \n",
    "        # Create fake hidden states for all layers\n",
    "        if output_hidden_states:\n",
    "            # Create a tuple of tensors for hidden states from each layer\n",
    "            hidden_states = tuple(torch.ones((batch_size, seq_length, hidden_size), \n",
    "                                            device=input_ids.device if input_ids is not None else 'cpu') \n",
    "                                 for _ in range(12))\n",
    "            return DummyOutput(hidden_states, last_hidden_state)\n",
    "        \n",
    "        return last_hidden_state\n",
    "        \n",
    "    def score(self, hidden_states):\n",
    "        \"\"\"Score method required by PPOTrainer\"\"\"\n",
    "        # Just return positive rewards (ones) for all inputs\n",
    "        if len(hidden_states.shape) == 3:\n",
    "            # If it's a sequence of hidden states, use the last token\n",
    "            return torch.ones((hidden_states.shape[0], 1), device=hidden_states.device)\n",
    "        else:\n",
    "            # If it's already a per-sequence representation\n",
    "            return torch.ones((hidden_states.shape[0], 1), device=hidden_states.device)\n",
    "        \n",
    "    def to(self, device):\n",
    "        \"\"\"Move model to device\"\"\"\n",
    "        self.reward_head = self.reward_head.to(device)\n",
    "        return self\n",
    "        \n",
    "    # Add a transformer attribute to satisfy the base_model_prefix\n",
    "    @property\n",
    "    def transformer(self):\n",
    "        \"\"\"Add a transformer property that returns self to satisfy PolicyAndValueWrapper\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class CustomValueModel(torch.nn.Module):\n",
    "    \"\"\"Custom value model that matches the interfaces expected by PPOTrainer\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a simple value head\n",
    "        self.value_head = torch.nn.Linear(768, 1)\n",
    "        # This prefix is used by PolicyAndValueWrapper to get the transformer backbone\n",
    "        self.base_model_prefix = \"transformer\"\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, output_hidden_states=True,\n",
    "                position_ids=None, token_type_ids=None, past_key_values=None, \n",
    "                head_mask=None, inputs_embeds=None, use_cache=None, return_dict=None, **kwargs):\n",
    "        \"\"\"Forward pass that accepts all standard transformer arguments\"\"\"\n",
    "        batch_size = input_ids.shape[0] if input_ids is not None else 1\n",
    "        seq_length = input_ids.shape[1] if input_ids is not None else 1\n",
    "        hidden_size = 768\n",
    "        \n",
    "        # Create fake hidden states\n",
    "        last_hidden_state = torch.ones((batch_size, seq_length, hidden_size), \n",
    "                                      device=input_ids.device if input_ids is not None else 'cpu')\n",
    "        \n",
    "        # Create a dummy output structure\n",
    "        class DummyOutput:\n",
    "            def __init__(self, hidden_states, last_hidden=None):\n",
    "                self.hidden_states = hidden_states\n",
    "                self.last_hidden_state = last_hidden\n",
    "        \n",
    "        # Create a tuple of tensors for all layers' hidden states\n",
    "        hidden_states = tuple(torch.ones((batch_size, seq_length, hidden_size), \n",
    "                                      device=input_ids.device if input_ids is not None else 'cpu') \n",
    "                             for _ in range(12))\n",
    "        \n",
    "        return DummyOutput(hidden_states, last_hidden_state)\n",
    "        \n",
    "    def score(self, hidden_states):\n",
    "        \"\"\"Score method that returns value estimates\"\"\"\n",
    "        # Return constant values for simplicity\n",
    "        return torch.zeros((hidden_states.shape[0], 1), device=hidden_states.device)\n",
    "        \n",
    "    def to(self, device):\n",
    "        \"\"\"Move model to device\"\"\"\n",
    "        self.value_head = self.value_head.to(device)\n",
    "        return self\n",
    "        \n",
    "    # Create a transformer property that handles the base_model_prefix\n",
    "    @property\n",
    "    def transformer(self):\n",
    "        \"\"\"Returns self to satisfy the PPOTrainer's expectation for base_model_prefix\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "def initialize_trainer(self, train_dataset=None, data_collator=None):\n",
    "    \"\"\"Initialize the PPO trainer with properly implemented models\"\"\"\n",
    "    try:\n",
    "        # Create dummy dataset if none provided\n",
    "        if train_dataset is None:\n",
    "            # Create minimal dataset\n",
    "            dummy_text = \"Example.\"\n",
    "            dummy_encoding = self.tokenizer(dummy_text, return_tensors=\"pt\")\n",
    "            dummy_ids = dummy_encoding.input_ids[0].cpu().numpy()\n",
    "            dummy_mask = dummy_encoding.attention_mask[0].cpu().numpy()\n",
    "            \n",
    "            dummy_data = {\n",
    "                \"input_ids\": [dummy_ids] * 2,\n",
    "                \"attention_mask\": [dummy_mask] * 2,\n",
    "                \"rewards\": [0.0] * 2\n",
    "            }\n",
    "            train_dataset = Dataset.from_dict(dummy_data)\n",
    "        \n",
    "        # Create data collator if needed\n",
    "        if data_collator is None:\n",
    "            from transformers import DataCollatorWithPadding\n",
    "            data_collator = DataCollatorWithPadding(\n",
    "                self.tokenizer, \n",
    "                pad_to_multiple_of=8\n",
    "            )\n",
    "        \n",
    "        # Free up memory\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create simple reward and value models that have all required interfaces\n",
    "        print(\"Creating custom reward and value models with required interfaces\")\n",
    "        self.reward_model = SimpleRewardModel().to(self.device)\n",
    "        self.value_model = CustomValueModel().to(self.device)\n",
    "        \n",
    "        # Initialize PPOTrainer\n",
    "        self.trainer = PPOTrainer(\n",
    "            args=self.ppo_config,\n",
    "            processing_class=self.tokenizer,\n",
    "            model=self.policy_model,\n",
    "            ref_model=self.ref_model,\n",
    "            reward_model=self.reward_model,\n",
    "            train_dataset=train_dataset,\n",
    "            value_model=self.value_model,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        \n",
    "        print(\"PPOTrainer initialized successfully!\")\n",
    "        return self.trainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing PPOTrainer: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
