{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarningsFetcher:\n",
    "    \"\"\"\n",
    "    Fetches earnings call transcripts and metadata from financial data providers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 gssso_token: str, \n",
    "                 base_url: str,\n",
    "                 cache_enabled: bool = True,\n",
    "                 max_concurrent: int = 5):\n",
    "        \"\"\"Initialize earnings fetcher\"\"\"\n",
    "        self.request_handler = RequestHandler(\n",
    "            base_url=base_url,\n",
    "            auth_token=gssso_token,\n",
    "            timeout=30,\n",
    "            max_retries=3\n",
    "        )\n",
    "        self.cache_enabled = cache_enabled\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self._metadata_cache: Dict[str, List[EarningsMetadata]] = {}\n",
    "        self._semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Enter async context\"\"\"\n",
    "        await self.request_handler.__aenter__()\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Exit async context\"\"\"\n",
    "        await self.request_handler.__aexit__(exc_type, exc_val, exc_tb)\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"Close request handler and release resources\"\"\"\n",
    "        await self.request_handler.close()\n",
    "    \n",
    "    # ----- Public API Methods -----\n",
    "    \n",
    "    async def get_latest_earning(self, \n",
    "                                refinitiv_id: Union[str, List[str]]) -> List[EarningTranscript]:\n",
    "        \"\"\"Get the latest earning transcripts for company IDs\"\"\"\n",
    "        # Normalize input to list\n",
    "        company_ids = [refinitiv_id] if isinstance(refinitiv_id, str) else refinitiv_id\n",
    "        \n",
    "        if not company_ids:\n",
    "            logger.warning(\"No company IDs provided for get_latest_earning\")\n",
    "            return []\n",
    "        \n",
    "        logger.info(f\"Getting latest earnings for {len(company_ids)} companies\")\n",
    "        \n",
    "        # Get metadata for all companies\n",
    "        metadata_list = await self._get_earnings_metadata(company_ids)\n",
    "        \n",
    "        # Get latest metadata for each company\n",
    "        latest_metadata = []\n",
    "        for company_id in company_ids:\n",
    "            company_metadata = [m for m in metadata_list if m.companyId_s == company_id]\n",
    "            latest = self._get_latest_metadata(company_metadata)\n",
    "            if latest:\n",
    "                latest_metadata.append(latest)\n",
    "                logger.info(f\"Found latest earnings for {latest.company_s} from {latest.document_date_s}\")\n",
    "            else:\n",
    "                logger.warning(f\"No latest earnings found for company ID {company_id}\")\n",
    "        \n",
    "        if not latest_metadata:\n",
    "            logger.warning(f\"No latest metadata found for any companies: {company_ids}\")\n",
    "            return []\n",
    "        \n",
    "        # Fetch transcripts for latest metadata\n",
    "        transcripts = await self._fetch_transcripts(latest_metadata)\n",
    "        logger.info(f\"Retrieved {len(transcripts)} latest transcripts\")\n",
    "        \n",
    "        return transcripts\n",
    "    \n",
    "    async def get_filtered_earnings(self,\n",
    "                                   refinitiv_id: Union[str, List[str]],\n",
    "                                   year: Optional[int] = None,\n",
    "                                   quarter: Optional[str] = None,\n",
    "                                   max_per_company: Optional[int] = None) -> List[EarningTranscript]:\n",
    "        \"\"\"Get filtered earnings transcripts by year and quarter\"\"\"\n",
    "        # Normalize input to list\n",
    "        company_ids = [refinitiv_id] if isinstance(refinitiv_id, str) else refinitiv_id\n",
    "        \n",
    "        if not company_ids:\n",
    "            logger.warning(\"No company IDs provided for get_filtered_earnings\")\n",
    "            return []\n",
    "        \n",
    "        filter_desc = []\n",
    "        if year:\n",
    "            filter_desc.append(f\"year={year}\")\n",
    "        if quarter:\n",
    "            filter_desc.append(f\"quarter={quarter}\")\n",
    "        if max_per_company:\n",
    "            filter_desc.append(f\"max={max_per_company}\")\n",
    "        \n",
    "        filter_str = \", \".join(filter_desc) if filter_desc else \"no filters\"\n",
    "        logger.info(f\"Getting filtered earnings for {len(company_ids)} companies with {filter_str}\")\n",
    "        \n",
    "        # Get metadata for all companies\n",
    "        metadata_list = await self._get_earnings_metadata(company_ids)\n",
    "        \n",
    "        # Apply filters for each company\n",
    "        filtered_metadata = []\n",
    "        for company_id in company_ids:\n",
    "            company_metadata = [m for m in metadata_list if m.companyId_s == company_id]\n",
    "            filtered = self._filter_metadata(company_metadata, year, quarter)\n",
    "            \n",
    "            # Sort by date (newest first)\n",
    "            filtered.sort(\n",
    "                key=lambda m: parse_date(m.document_date_s) or datetime.min,\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Apply max limit\n",
    "            if max_per_company and len(filtered) > max_per_company:\n",
    "                filtered = filtered[:max_per_company]\n",
    "            \n",
    "            logger.info(f\"Found {len(filtered)} filtered earnings for company ID {company_id}\")\n",
    "            filtered_metadata.extend(filtered)\n",
    "        \n",
    "        if not filtered_metadata:\n",
    "            logger.warning(f\"No matching metadata found for {filter_str}\")\n",
    "            return []\n",
    "        \n",
    "        # Fetch transcripts for filtered metadata\n",
    "        transcripts = await self._fetch_transcripts(filtered_metadata)\n",
    "        logger.info(f\"Retrieved {len(transcripts)} filtered transcripts\")\n",
    "        \n",
    "        return transcripts\n",
    "    \n",
    "    async def clear_cache(self):\n",
    "        \"\"\"Clear metadata cache\"\"\"\n",
    "        self._metadata_cache.clear()\n",
    "        logger.info(\"Metadata cache cleared\")\n",
    "    \n",
    "    # ----- Internal Methods -----\n",
    "    \n",
    "    async def _get_earnings_metadata(self, \n",
    "                                    company_ids: List[str],\n",
    "                                    force_refresh: bool = False) -> List[EarningsMetadata]:\n",
    "        \"\"\"Fetch earnings metadata for company IDs\"\"\"\n",
    "        if not company_ids:\n",
    "            return []\n",
    "        \n",
    "        # Check cache first\n",
    "        if not force_refresh and self.cache_enabled:\n",
    "            cache_key = ','.join(sorted(company_ids))\n",
    "            if cache_key in self._metadata_cache:\n",
    "                logger.debug(f\"Using cached metadata for {len(company_ids)} companies\")\n",
    "                return self._metadata_cache[cache_key]\n",
    "        \n",
    "        logger.info(f\"Fetching earnings metadata for {len(company_ids)} companies\")\n",
    "        \n",
    "        # Construct query\n",
    "        query_parts = [f\"metadata_txt:{quote(company_id)}\" for company_id in company_ids]\n",
    "        query = \" OR \".join(query_parts)\n",
    "        endpoint = f\"/search/bulk/query/%20AND%20({query})\"\n",
    "        \n",
    "        try:\n",
    "            # Make API request\n",
    "            content = await self.request_handler.get(endpoint)\n",
    "            metadata_list = self._parse_metadata_response(content)\n",
    "            \n",
    "            # Update cache\n",
    "            if self.cache_enabled:\n",
    "                cache_key = ','.join(sorted(company_ids))\n",
    "                self._metadata_cache[cache_key] = metadata_list\n",
    "            \n",
    "            return metadata_list\n",
    "            \n",
    "        except RequestError as e:\n",
    "            logger.error(f\"Error fetching metadata: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Unexpected error fetching metadata: {e}\")\n",
    "            raise ContentFetchError(f\"Failed to fetch metadata: {str(e)}\")\n",
    "    \n",
    "    def _parse_metadata_response(self, content: str) -> List[EarningsMetadata]:\n",
    "        \"\"\"Parse metadata from API response\"\"\"\n",
    "        try:\n",
    "            result = []\n",
    "            soup = BeautifulSoup(content, \"lxml\")\n",
    "            \n",
    "            for doc in soup.find_all(\"lexDocument\"):\n",
    "                metadata_dict = {}\n",
    "                url = None\n",
    "                \n",
    "                for field in doc.find_all('field'):\n",
    "                    name = field.find('name')\n",
    "                    values = field.find('values')\n",
    "                    \n",
    "                    if name is None or values is None:\n",
    "                        continue\n",
    "                    \n",
    "                    field_name = name.text.strip()\n",
    "                    field_value = values.text.strip()\n",
    "                    \n",
    "                    if field_name == \"metadata_txt\":\n",
    "                        try:\n",
    "                            metadata_dict = json.loads(field_value)\n",
    "                        except json.JSONDecodeError:\n",
    "                            logger.error(f\"Invalid JSON in metadata_txt: {field_value[:100]}...\")\n",
    "                    elif field_name == \"objecturl\":\n",
    "                        url = field_value\n",
    "                \n",
    "                if metadata_dict:\n",
    "                    # Add URL to metadata\n",
    "                    if url:\n",
    "                        metadata_dict[\"objecturl\"] = url\n",
    "                    \n",
    "                    try:\n",
    "                        earnings_metadata = EarningsMetadata(**metadata_dict)\n",
    "                        result.append(earnings_metadata)\n",
    "                    except ValidationError as e:\n",
    "                        logger.error(f\"Invalid metadata format: {e}\")\n",
    "            \n",
    "            logger.debug(f\"Parsed {len(result)} metadata entries\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error parsing metadata response: {e}\")\n",
    "            raise ParsingError(f\"Failed to parse metadata response: {str(e)}\")\n",
    "    \n",
    "    def _get_latest_metadata(self, metadata_list: List[EarningsMetadata]) -> Optional[EarningsMetadata]:\n",
    "        \"\"\"Get latest metadata from list\"\"\"\n",
    "        if not metadata_list:\n",
    "            return None\n",
    "        \n",
    "        # Filter entries with valid dates\n",
    "        valid_metadata = []\n",
    "        for metadata in metadata_list:\n",
    "            if metadata.document_date_s:\n",
    "                date = parse_date(metadata.document_date_s)\n",
    "                if date:\n",
    "                    valid_metadata.append((metadata, date))\n",
    "        \n",
    "        if not valid_metadata:\n",
    "            return None\n",
    "        \n",
    "        # Return metadata with latest date\n",
    "        return max(valid_metadata, key=lambda x: x[1])[0]\n",
    "    \n",
    "    def _filter_metadata(self, \n",
    "                        metadata_list: List[EarningsMetadata],\n",
    "                        year: Optional[int] = None,\n",
    "                        quarter: Optional[str] = None) -> List[EarningsMetadata]:\n",
    "        \"\"\"Filter metadata by year and quarter\"\"\"\n",
    "        if not metadata_list:\n",
    "            return []\n",
    "        \n",
    "        if year is None and quarter is None:\n",
    "            return metadata_list\n",
    "        \n",
    "        filtered = []\n",
    "        \n",
    "        for metadata in metadata_list:\n",
    "            if not metadata.document_date_s:\n",
    "                continue\n",
    "                \n",
    "            date = parse_date(metadata.document_date_s)\n",
    "            if not date:\n",
    "                continue\n",
    "                \n",
    "            # Apply year filter\n",
    "            if year is not None and date.year != year:\n",
    "                continue\n",
    "                \n",
    "            # Apply quarter filter\n",
    "            if quarter is not None:\n",
    "                doc_quarter = f\"Q{(date.month - 1) // 3 + 1}\"\n",
    "                if doc_quarter != quarter:\n",
    "                    continue\n",
    "                    \n",
    "            filtered.append(metadata)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    async def _fetch_transcripts(self, metadata_list: List[EarningsMetadata]) -> List[EarningTranscript]:\n",
    "        \"\"\"Fetch transcripts for metadata\"\"\"\n",
    "        if not metadata_list:\n",
    "            return []\n",
    "        \n",
    "        logger.info(f\"Fetching {len(metadata_list)} transcripts\")\n",
    "        \n",
    "        async def fetch_with_semaphore(metadata: EarningsMetadata) -> Optional[EarningTranscript]:\n",
    "            async with self._semaphore:\n",
    "                return await self._fetch_single_transcript(metadata)\n",
    "        \n",
    "        # Create fetch tasks\n",
    "        fetch_tasks = [fetch_with_semaphore(metadata) for metadata in metadata_list]\n",
    "        \n",
    "        # Execute tasks concurrently\n",
    "        results = await asyncio.gather(*fetch_tasks, return_exceptions=True)\n",
    "        \n",
    "        # Process results\n",
    "        transcripts = []\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, Exception):\n",
    "                logger.error(f\"Error fetching transcript: {result}\")\n",
    "            elif result is not None:\n",
    "                transcripts.append(result)\n",
    "        \n",
    "        return transcripts\n",
    "    \n",
    "    async def _fetch_single_transcript(self, metadata: EarningsMetadata) -> Optional[EarningTranscript]:\n",
    "        \"\"\"Fetch transcript for single metadata\"\"\"\n",
    "        if not metadata.objecturl:\n",
    "            logger.warning(f\"No object URL for metadata: {metadata}\")\n",
    "            return None\n",
    "        \n",
    "        logger.debug(f\"Fetching transcript for {metadata.company_s} ({metadata.document_date_s})\")\n",
    "        \n",
    "        try:\n",
    "            # Make direct request to objecturl\n",
    "            response_text = await self.request_handler.get(metadata.objecturl, {}, {})\n",
    "            \n",
    "            return EarningTranscript(\n",
    "                metadata=metadata,\n",
    "                transcript=response_text\n",
    "            )\n",
    "            \n",
    "        except RequestError as e:\n",
    "            logger.error(f\"Error fetching transcript for {metadata.companyId_s}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Unexpected error fetching transcript: {e}\")\n",
    "            return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
