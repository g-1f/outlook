{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIMetricsProcessor:\n",
    "    \"\"\"\n",
    "    Processes earnings call data to extract AI metrics and uploads the results to S3.\n",
    "    \n",
    "    Assumes an external LLM instance is provided (which works with invoke_json_former\n",
    "    that already handles retries).\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _setup_logging() -> None:\n",
    "        \"\"\"\n",
    "        Sets up logging with Loguru.\n",
    "        This method removes default handlers and adds a new one configured to log INFO level.\n",
    "        \"\"\"\n",
    "        logger.remove()  # Remove default logger configuration.\n",
    "        logger.add(sink=lambda msg: print(msg, end=\"\"), level=\"INFO\")\n",
    "\n",
    "    def __init__(self, llm: Any, batch_size: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize the processor with an LLM instance and a default batch size.\n",
    "        \"\"\"\n",
    "        self._setup_logging()\n",
    "        self.llm = llm\n",
    "        self.batch_size = batch_size\n",
    "        self.s3_clients: Dict[str, Any] = {}\n",
    "        self.mappings: Dict[str, Dict] = {}\n",
    "\n",
    "    def _get_s3_client(self, bucket: str) -> Any:\n",
    "        \"\"\"\n",
    "        Return an S3 client instance for the given bucket.\n",
    "        \"\"\"\n",
    "        credentials = get_s3_credentials()  # Replace with your actual credentials function.\n",
    "        s3_client = S3(credentials=credentials, bucket=bucket)\n",
    "        return s3_client\n",
    "\n",
    "    def _load_mappings(self, bucket: str, mapping_file: str = \"mapping.json\") -> Dict:\n",
    "        \"\"\"\n",
    "        Load and reverse the mapping from the specified S3 bucket.\n",
    "        \"\"\"\n",
    "        s3_client = self._get_s3_client(bucket)\n",
    "        mapping = json.loads(s3_client.read_file(mapping_file))\n",
    "        # Reverse mapping: key becomes value and vice versa.\n",
    "        mapping = {v: k for k, v in mapping.items()}\n",
    "        return mapping\n",
    "\n",
    "    def _load_earnings_data(self, start_date: dt.date, end_date: dt.date) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load earnings data via Spark (using load_earnings_call_data_ai_metrics) and convert to Pandas.\n",
    "        \"\"\"\n",
    "        df = load_earnings_call_data_ai_metrics(start_date, end_date)\n",
    "        return df.toPandas()\n",
    "\n",
    "    def _process_batch(self, batch: pd.DataFrame, metric: Any, mapping: Dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process a batch of earnings data and extract AI metrics.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for _, row in batch.iterrows():\n",
    "            try:\n",
    "                # invoke_json_former is assumed to handle retries.\n",
    "                metrics_data = invoke_json_former(self.llm, row[\"eventBody\"], metric)\n",
    "                row_copy = row.copy()\n",
    "                row_copy[\"Metrics\"] = metrics_data.get(\"value\")\n",
    "                row_copy[\"Detail\"] = metrics_data.get(\"detail\")\n",
    "                if row_copy.get(\"ID_BB_COMPANY\") in mapping:\n",
    "                    row_copy[\"ID_BB_COMPANY\"] = mapping[row_copy[\"ID_BB_COMPANY\"]]\n",
    "                results.append(row_copy)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing row {row.get('companyTicker', 'unknown')}: {e}\")\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def get_ai_metrics(\n",
    "        self,\n",
    "        metric: Any,\n",
    "        s3_bucket_id: str,\n",
    "        start_date: dt.date,\n",
    "        end_date: dt.date,\n",
    "        batch_size: Optional[int] = None,\n",
    "        output_prefix: Optional[str] = None\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract AI metrics from earnings data and upload as a parquet file to S3.\n",
    "        \n",
    "        The earnings data is loaded via Spark (for the given date range) and processed in batches.\n",
    "        If no output_prefix is provided, one is generated based on the date range.\n",
    "        \"\"\"\n",
    "        s3_client = self._get_s3_client(s3_bucket_id)\n",
    "        mapping = self._load_mappings(\"bbg_cid_refinitiv_id_map\")\n",
    "        df = self._load_earnings_data(start_date, end_date)\n",
    "        if df.empty:\n",
    "            logger.warning(\"No earnings data to process\")\n",
    "            return None\n",
    "\n",
    "        # Rename columns as needed for downstream processing.\n",
    "        df.rename(columns={\n",
    "            \"FILE_DATE\": \"Document Date\",\n",
    "            \"SOURCE_TIMESTAMP\": \"Event Time\",\n",
    "            \"companyId\": \"ID_BB_COMPANY\",\n",
    "            \"year\": \"Year\",\n",
    "            \"quarter\": \"Quarter\",\n",
    "        }, inplace=True)\n",
    "\n",
    "        total_rows = len(df)\n",
    "        processed_df = pd.DataFrame()\n",
    "        batch_size = batch_size or self.batch_size\n",
    "\n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            logger.info(f\"Processing batch {(i // batch_size) + 1} ({len(batch)} rows)\")\n",
    "            processed_batch = self._process_batch(batch, metric, mapping)\n",
    "            if not processed_batch.empty:\n",
    "                processed_df = pd.concat([processed_df, processed_batch], ignore_index=True)\n",
    "\n",
    "        if not processed_df.empty:\n",
    "            # Generate output prefix if not provided.\n",
    "            if output_prefix is None:\n",
    "                output_prefix = f\"fluentai_{start_date}_{end_date}\"\n",
    "            output_path = f\"{output_prefix}.parquet\"\n",
    "            parquet_buffer = BytesIO()\n",
    "            processed_df.to_parquet(parquet_buffer, index=False)\n",
    "            parquet_buffer.seek(0)\n",
    "            s3_client.write_file(output_path, parquet_buffer.getvalue())\n",
    "            logger.info(f\"Results saved to S3 at: {output_path}\")\n",
    "            return output_path\n",
    "        else:\n",
    "            logger.warning(\"No data processed\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
