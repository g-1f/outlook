{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class WeeklyTradingEnvNoGym:\n",
    "    \"\"\"\n",
    "    - Single-step environment with 5-day episodes.\n",
    "    - The agent rebalances once per 5-day window.\n",
    "    - Reward is the difference in portfolio value over those 5 days.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        initial_amount: float = 1e6,\n",
    "        episode_length: int = 5,\n",
    "        news_lookback_days: int = 5,\n",
    "        tech_indicator_list=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: Must include 'close' (float) daily price, optional 'news' (str).\n",
    "            initial_amount: Starting capital (float).\n",
    "            episode_length: # of daily rows per episode (5 for 'weekly').\n",
    "            news_lookback_days: # of prior days of news to aggregate for the text prompt.\n",
    "            tech_indicator_list: list of columns for numeric indicators (optional).\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.num_days = len(self.df)\n",
    "        self.initial_amount = initial_amount\n",
    "        self.episode_length = episode_length\n",
    "        self.news_lookback_days = news_lookback_days\n",
    "        self.tech_indicator_list = tech_indicator_list or []\n",
    "\n",
    "        # Pointers\n",
    "        self.current_idx = 0  # Start day for the current episode\n",
    "        self.done = False\n",
    "\n",
    "        # Portfolio state\n",
    "        self.cash = initial_amount\n",
    "        self.position = 0\n",
    "        self.start_portfolio_value = initial_amount\n",
    "\n",
    "    def _get_aggregated_news(self, day_idx):\n",
    "        \"\"\"Collect the last `news_lookback_days` of news and join them into one string.\"\"\"\n",
    "        start_idx = max(0, day_idx - self.news_lookback_days + 1)\n",
    "        snippet_list = self.df.iloc[start_idx:day_idx+1].get(\"news\", \"\").tolist()\n",
    "        agg_news = \" \".join(str(s) for s in snippet_list if s is not None)\n",
    "        return agg_news\n",
    "\n",
    "    def _portfolio_value(self, price):\n",
    "        return self.cash + self.position * price\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset environment to a new 5-day window start.\n",
    "        Returns (state, news).\n",
    "        State is a simple numeric array, e.g. [cash, day0_price, indicators...]\n",
    "        \"\"\"\n",
    "        if self.current_idx + self.episode_length >= self.num_days:\n",
    "            self.done = True\n",
    "            raise StopIteration(\"No more data to continue the environment.\")\n",
    "\n",
    "        self.done = False\n",
    "        self.cash = self.initial_amount\n",
    "        self.position = 0\n",
    "\n",
    "        day0_data = self.df.iloc[self.current_idx]\n",
    "        day0_price = day0_data[\"close\"]\n",
    "        self.start_portfolio_value = self.initial_amount\n",
    "\n",
    "        # Numeric part of the observation\n",
    "        obs_numeric = [self.cash, day0_price]\n",
    "        for col in self.tech_indicator_list:\n",
    "            obs_numeric.append(day0_data.get(col, 0.0))\n",
    "\n",
    "        # Get aggregated news\n",
    "        news_text = self._get_aggregated_news(self.current_idx)\n",
    "\n",
    "        return np.array(obs_numeric, dtype=np.float32), news_text\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        One step = pick # of shares. We jump forward 5 days for the final portfolio.\n",
    "        Returns: (next_state, next_news, reward, done, info)\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Episode is done. Call reset() for a new episode.\")\n",
    "\n",
    "        # Day 0 price\n",
    "        day0_price = self.df.iloc[self.current_idx][\"close\"]\n",
    "        current_val = self._portfolio_value(day0_price)\n",
    "\n",
    "        # Rebalance\n",
    "        self.position = action\n",
    "        self.cash = current_val - self.position * day0_price\n",
    "\n",
    "        # Final day in this 5-day window\n",
    "        final_idx = self.current_idx + self.episode_length - 1\n",
    "        if final_idx >= self.num_days:\n",
    "            final_idx = self.num_days - 1\n",
    "            self.done = True\n",
    "        else:\n",
    "            self.done = True  # Single-step episode\n",
    "\n",
    "        final_price = self.df.iloc[final_idx][\"close\"]\n",
    "        final_val = self._portfolio_value(final_price)\n",
    "        reward = final_val - self.start_portfolio_value\n",
    "\n",
    "        # Advance pointer for next call to reset()\n",
    "        self.current_idx += self.episode_length\n",
    "\n",
    "        next_state = None  # or you could return some dummy\n",
    "        next_news = None\n",
    "        info = {}\n",
    "\n",
    "        return next_state, next_news, reward, self.done, info\n",
    "\n",
    "\n",
    "# Install dependencies (if not installed):\n",
    "# !pip install trl transformers\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# Example: Qwen 2.5 from HuggingFace\n",
    "# (Replace with actual model repo name if different)\n",
    "MODEL_NAME = \"Qwen/Qwen-2.5\"  \n",
    "\n",
    "# 1. Load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2. Configure PPO\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=1,      # In this example, we do 1 sample/episode at a time\n",
    "    mini_batch_size=1, # ...\n",
    "    optimize_cuda_usage=False,\n",
    "    log_with=None,     # or \"wandb\"\n",
    "    # ... other config\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # If you want a separate reference model for KL:\n",
    "    # ref_model_name=MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Example environment usage\n",
    "df = pd.DataFrame({\n",
    "    'close': np.random.uniform(90, 110, size=100),\n",
    "    'news': [\"Macro event \" + str(i) for i in range(100)]\n",
    "})\n",
    "env = WeeklyTradingEnvNoGym(df, initial_amount=1e6, episode_length=5)\n",
    "\n",
    "def obs_and_news_to_prompt(obs: np.ndarray, news: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert numeric obs + text snippet into a prompt for Qwen.\n",
    "    \"\"\"\n",
    "    obs_str = \", \".join([f\"{v:.2f}\" for v in obs])\n",
    "    prompt = (\n",
    "        f\"Observation: {obs_str}\\n\"\n",
    "        f\"News: {news}\\n\\n\"\n",
    "        \"As a financial expert, output three numbers (comma-separated):\\n\"\n",
    "        \"(1) number of shares to hold,\\n\"\n",
    "        \"(2) recommendation score 1-5,\\n\"\n",
    "        \"(3) risk score 1-5.\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def parse_response(response_text: str):\n",
    "    \"\"\"\n",
    "    Attempt to parse \"action, rec_score, risk_score\" from the model's text output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = response_text.strip().split(\",\")\n",
    "        action = int(float(parts[0].strip()))\n",
    "        rec = float(parts[1].strip())\n",
    "        risk = float(parts[2].strip())\n",
    "        return action, rec, risk\n",
    "    except Exception:\n",
    "        return 0, 3.0, 3.0\n",
    "\n",
    "def map_rec_risk_to_action_mod(action, rec_score, risk_score):\n",
    "    \"\"\"\n",
    "    Example heuristic to adjust the raw action from the model\n",
    "    with the recommendation & risk scores.\n",
    "    \"\"\"\n",
    "    # e.g. scale up if rec >=4, scale down if risk >=4, etc.\n",
    "    factor = 1.0\n",
    "    if rec_score >= 4.5: factor *= 1.2\n",
    "    elif rec_score >= 4.0: factor *= 1.1\n",
    "    if risk_score >= 4.5: factor *= 0.8\n",
    "    elif risk_score >= 4.0: factor *= 0.9\n",
    "\n",
    "    mod_action = int(round(action * factor))\n",
    "    return mod_action\n",
    "\n",
    "\n",
    "# We'll store (query, response, reward) each time, then train with PPO\n",
    "NUM_EPISODES = 10\n",
    "all_prompts = []\n",
    "all_responses = []\n",
    "all_rewards = []\n",
    "\n",
    "episode_count = 0\n",
    "\n",
    "while episode_count < NUM_EPISODES:\n",
    "    try:\n",
    "        obs, news = env.reset()\n",
    "    except StopIteration:\n",
    "        print(\"No more data, training done.\")\n",
    "        break\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = obs_and_news_to_prompt(obs, news)\n",
    "\n",
    "    # Tokenize for the model\n",
    "    query_tensors = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate a response\n",
    "    response_tensors = ppo_trainer.model.generate(\n",
    "        input_ids=query_tensors,\n",
    "        max_length=80,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    response_text = tokenizer.decode(response_tensors[0], skip_special_tokens=True)\n",
    "\n",
    "    # Parse the raw action from response\n",
    "    raw_action, rec_score, risk_score = parse_response(response_text)\n",
    "    final_action = map_rec_risk_to_action_mod(raw_action, rec_score, risk_score)\n",
    "\n",
    "    # Step environment\n",
    "    next_obs, next_news, reward, done, info = env.step(final_action)\n",
    "\n",
    "    # Store for PPO\n",
    "    all_prompts.append(query_tensors[0])\n",
    "    all_responses.append(response_tensors[0])\n",
    "    all_rewards.append(torch.tensor([reward], dtype=torch.float, device=model.device))\n",
    "\n",
    "    # One-step environment => done should be True after the step\n",
    "    if done:\n",
    "        # Update policy with PPO step on (prompt, response, reward)\n",
    "        # Note: PPOTrainer can do multiple steps at once if you gather more data.\n",
    "        # But here we do 1 episode -> 1 step to keep it simple.\n",
    "        ppo_trainer.step(\n",
    "            queries=all_prompts,\n",
    "            responses=all_responses,\n",
    "            rewards=all_rewards\n",
    "        )\n",
    "        # Clear buffers\n",
    "        all_prompts = []\n",
    "        all_responses = []\n",
    "        all_rewards = []\n",
    "        episode_count += 1\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def cvar_transform_rewards(rewards: torch.Tensor, alpha: float = 0.2):\n",
    "    \"\"\"\n",
    "    Simple transform: \n",
    "    - Find the alpha-quantile among all rewards.\n",
    "    - Keep only those below the quantile (the \"worst\" episodes),\n",
    "      and set others to 0 or a reduced value.\n",
    "    - Or compute the mean of the worst returns and set all to that average to \n",
    "      push the policy to improve tail risk.\n",
    "    \"\"\"\n",
    "    sorted_rewards, _ = torch.sort(rewards)\n",
    "    cutoff_index = int(len(sorted_rewards) * alpha)\n",
    "    cutoff_index = max(0, min(cutoff_index, len(sorted_rewards)-1))\n",
    "    cutoff_value = sorted_rewards[cutoff_index].item()\n",
    "\n",
    "    # For example, let's define the new reward as:\n",
    "    # new_reward = (reward if reward <= cutoff_value else 0)\n",
    "    # or we can do: new_reward = min(reward, cutoff_value)\n",
    "    # or an average of the below-threshold returns\n",
    "\n",
    "    # Here we do something simple: \n",
    "    # if reward <= cutoff_value, keep reward\n",
    "    # else set reward to the cutoff_value\n",
    "    new_rewards = torch.minimum(rewards, torch.tensor(cutoff_value, device=rewards.device))\n",
    "    return new_rewards\n",
    "\n",
    "# Let's adapt the loop to batch up multiple episodes, apply CVaR transform,\n",
    "# then do a single PPO step on the entire batch.\n",
    "NUM_EPISODES = 20\n",
    "BATCH_SIZE = 5\n",
    "episode_count = 0\n",
    "\n",
    "prompts_batch = []\n",
    "responses_batch = []\n",
    "rewards_batch = []\n",
    "\n",
    "while episode_count < NUM_EPISODES:\n",
    "    try:\n",
    "        obs, news = env.reset()\n",
    "    except StopIteration:\n",
    "        print(\"No more data.\")\n",
    "        break\n",
    "\n",
    "    prompt = obs_and_news_to_prompt(obs, news)\n",
    "    query_tensors = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    response_tensors = ppo_trainer.model.generate(\n",
    "        input_ids=query_tensors,\n",
    "        max_length=80,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    response_text = tokenizer.decode(response_tensors[0], skip_special_tokens=True)\n",
    "\n",
    "    # parse action\n",
    "    raw_action, rec_score, risk_score = parse_response(response_text)\n",
    "    final_action = map_rec_risk_to_action_mod(raw_action, rec_score, risk_score)\n",
    "\n",
    "    # step env\n",
    "    _, _, reward, done, _ = env.step(final_action)\n",
    "\n",
    "    # store\n",
    "    prompts_batch.append(query_tensors[0])\n",
    "    responses_batch.append(response_tensors[0])\n",
    "    rewards_batch.append(reward)  # keep as float for now\n",
    "\n",
    "    # single-step => done = True\n",
    "    episode_count += 1\n",
    "\n",
    "    # once we have a full batch, do cvar adjustment\n",
    "    if episode_count % BATCH_SIZE == 0 or episode_count == NUM_EPISODES:\n",
    "        # Convert to tensor\n",
    "        rewards_tensor = torch.tensor(rewards_batch, dtype=torch.float, device=model.device)\n",
    "\n",
    "        # CVaR transform\n",
    "        cvar_rewards = cvar_transform_rewards(rewards_tensor, alpha=0.2)\n",
    "\n",
    "        # Now do PPO step with transformed rewards\n",
    "        ppo_trainer.step(\n",
    "            queries=prompts_batch,\n",
    "            responses=responses_batch,\n",
    "            rewards=cvar_rewards\n",
    "        )\n",
    "\n",
    "        # clear for next batch\n",
    "        prompts_batch = []\n",
    "        responses_batch = []\n",
    "        rewards_batch = []\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
