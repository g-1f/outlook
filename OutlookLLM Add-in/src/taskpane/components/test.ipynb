{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from typing import Dict, List, Deque, Tuple\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Stateful Trading Environment (No Changes)\n",
    "# =============================================================================\n",
    "\n",
    "class DailyTradingEnv:\n",
    "    # Keep the original implementation unchanged\n",
    "    ...\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PPO-Compatible Trader Class\n",
    "# =============================================================================\n",
    "\n",
    "class PPOTrader:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen1.5-1.8B\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # PPO configuration\n",
    "        self.config = PPOConfig(\n",
    "            learning_rate=1.2e-5,\n",
    "            batch_size=32,\n",
    "            mini_batch_size=8,\n",
    "            ppo_epochs=3,\n",
    "            max_grad_norm=0.5,\n",
    "            kl_penalty=\"adaptive\",\n",
    "            target_kl=0.01,\n",
    "            seed=42,\n",
    "            adap_kl_ctrl=True,\n",
    "            init_kl_coef=0.2,\n",
    "            cliprange=0.2,\n",
    "            cliprange_value=0.2,\n",
    "            vf_coef=0.5,\n",
    "            gamma=0.99,\n",
    "            lam=0.95,\n",
    "            log_with=\"wandb\"  # or \"tensorboard\"\n",
    "        )\n",
    "        \n",
    "        self.trainer = PPOTrainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            config=self.config\n",
    "        )\n",
    "\n",
    "    def format_state(self, env: DailyTradingEnv) -> str:\n",
    "        # Keep the original prompt formatting\n",
    "        ...\n",
    "\n",
    "    def parse_response(self, response: str) -> Dict:\n",
    "        # Keep the original parsing logic\n",
    "        ...\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Modified Training Loop for PPO\n",
    "# =============================================================================\n",
    "\n",
    "class PPOTrainingOrchestrator:\n",
    "    def __init__(self, env: DailyTradingEnv, trader: PPOTrader):\n",
    "        self.env = env\n",
    "        self.trader = trader\n",
    "        self.buffer = deque(maxlen=252*5)\n",
    "        self.episode_length = 21\n",
    "        self.gamma = 0.99\n",
    "        self.lam = 0.95\n",
    "\n",
    "    def calculate_advantages(self, rewards: List[float], values: List[float]) -> List[float]:\n",
    "        \"\"\"Calculate GAE advantages\"\"\"\n",
    "        advantages = []\n",
    "        last_advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t+1] - values[t]\n",
    "            advantages.append(last_advantage * self.gamma * self.lam + delta)\n",
    "        return list(reversed(advantages))\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"Collect experience for one episode\"\"\"\n",
    "        self.env._reset_state()\n",
    "        states, actions, rewards, values = [], [], [], []\n",
    "        \n",
    "        for _ in range(self.episode_length):\n",
    "            # Generate response\n",
    "            prompt = self.trader.format_state(self.env)\n",
    "            inputs = self.trader.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                response = self.trader.model.generate(**inputs, max_new_tokens=256)\n",
    "                value = self.trader.model(**inputs).value.item()\n",
    "            \n",
    "            action = self.trader.parse_response(self.trader.tokenizer.decode(response[0]))\n",
    "            \n",
    "            # Execute action\n",
    "            reward, _ = self.env.step(action['position'])\n",
    "            \n",
    "            # Store experience\n",
    "            states.append(inputs.input_ids)\n",
    "            actions.append(response)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            \n",
    "        # Calculate advantages and returns\n",
    "        advantages = self.calculate_advantages(rewards, values)\n",
    "        returns = [r + self.gamma * v for r, v in zip(rewards, values[1:])]\n",
    "        \n",
    "        return {\n",
    "            'states': states,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'returns': returns,\n",
    "            'advantages': advantages\n",
    "        }\n",
    "\n",
    "    def train(self, num_episodes: int = 1000):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            # Collect experience\n",
    "            batch = self.run_episode()\n",
    "            \n",
    "            # Prepare PPO inputs\n",
    "            ppo_batch = {\n",
    "                'query_tensors': batch['states'],\n",
    "                'response_tensors': batch['actions'],\n",
    "                'advantages': torch.tensor(batch['advantages']),\n",
    "                'returns': torch.tensor(batch['returns']),\n",
    "                'rewards': torch.tensor(batch['rewards'])\n",
    "            }\n",
    "            \n",
    "            # Perform PPO update\n",
    "            stats = self.trader.trainer.step(\n",
    "                [ppo_batch['query_tensors']],\n",
    "                [ppo_batch['response_tensors']],\n",
    "                [ppo_batch['advantages']],\n",
    "                [ppo_batch['returns']]\n",
    "            )\n",
    "            \n",
    "            # Logging\n",
    "            if (episode+1) % 10 == 0:\n",
    "                print(f\"Episode {episode+1}\")\n",
    "                print(f\"Mean Reward: {np.mean(batch['rewards']):.2f}\")\n",
    "                print(f\"Max Advantage: {np.max(batch['advantages']):.2f}\")\n",
    "                print(\"=\"*50)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Execution Workflow\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize components\n",
    "    data = pd.read_csv('sp500_daily.csv', parse_dates=['date'])\n",
    "    env = DailyTradingEnv(data)\n",
    "    trader = PPOTrader()\n",
    "    orchestrator = PPOTrainingOrchestrator(env, trader)\n",
    "    \n",
    "    # Start training\n",
    "    orchestrator.train(num_episodes=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
