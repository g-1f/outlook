{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "Example:\n",
    "Market Context: PMI:52.6, inflation:5.1%, unemployment:3.8%\n",
    "\n",
    "<macro state>\n",
    "Economy is showing strength with PMI in expansion territory, but inflation remains elevated.\n",
    "</macro state>\n",
    "<reasoning>\n",
    "The PMI at 52.6 indicates expansion, which is positive for equities. However, inflation at 5.1% is above the Fed's target, suggesting possible rate hikes. Employment is strong at 3.8%, supporting consumer spending.\n",
    "</reasoning>\n",
    "<positioning>\n",
    "0.3\n",
    "</positioning>\n",
    "\n",
    "You must respond in the above XML format.\n",
    "\"\"\"\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, log_dir):\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        self.log_file = os.path.join(log_dir, f\"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "        self.reward_file = os.path.join(log_dir, f\"rewards_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\")\n",
    "        self.completion_file = os.path.join(log_dir, f\"completions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\")\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(\"GRPO-Trader\")\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.completions = []\n",
    "        \n",
    "    def log_info(self, message):\n",
    "        self.logger.info(message)\n",
    "        \n",
    "    def log_warning(self, message):\n",
    "        self.logger.warning(message)\n",
    "        \n",
    "    def log_error(self, message):\n",
    "        self.logger.error(message)\n",
    "        \n",
    "    def log_reward(self, step, reward_data):\n",
    "        entry = {\"step\": step, \"time\": time.time(), **reward_data}\n",
    "        self.rewards.append(entry)\n",
    "        with open(self.reward_file, 'a') as f:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "            \n",
    "    def log_completion(self, step, prompt, completion, reward, format_valid, position=None):\n",
    "        entry = {\n",
    "            \"step\": step,\n",
    "            \"time\": time.time(),\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion,\n",
    "            \"reward\": reward,\n",
    "            \"format_valid\": format_valid,\n",
    "            \"position\": position\n",
    "        }\n",
    "        self.completions.append(entry)\n",
    "        with open(self.completion_file, 'a') as f:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "            \n",
    "    def save_rewards(self):\n",
    "        with open(self.reward_file, 'w') as f:\n",
    "            for entry in self.rewards:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "                \n",
    "    def save_completions(self):\n",
    "        with open(self.completion_file, 'w') as f:\n",
    "            for entry in self.completions:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "def prepare_data(df, logger):\n",
    "    logger.log_info(\"Preparing training data...\")\n",
    "    \n",
    "    def prepare_prompt(df):\n",
    "        df['prompt'] = df.apply(lambda row: [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT.strip()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Market Context:{', '.join(f'{k}:{v}' for k, v in row.drop('date', errors='ignore').items())}\"\n",
    "            }\n",
    "        ], axis=1)\n",
    "        return df\n",
    "\n",
    "    df = prepare_prompt(df)\n",
    "    df['returns'] = df['close'].pct_change().shift(-1)\n",
    "    train_dataset = df[['prompt', 'returns']]\n",
    "    data = Dataset.from_pandas(train_dataset, preserve_index=False)\n",
    "    \n",
    "    logger.log_info(f\"Prepared {len(data)} training examples\")\n",
    "    logger.log_info(f\"Sample prompt: {data[0]['prompt']}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_positioning(text):\n",
    "    try:\n",
    "        match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            value = match.group(1).strip()\n",
    "            return float(value)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "def format_reward_func(prompts, completions, **kwargs):\n",
    "    logger = kwargs.get(\"logger\", None)\n",
    "    step = kwargs.get(\"step\", 0)\n",
    "    \n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    if logger and step % 50 == 0:\n",
    "        for i, content in enumerate(completion_contents[:2]):\n",
    "            logger.log_info(f\"Step {step} - Completion {i}: {content[:100]}...\")\n",
    "    \n",
    "    matches = [re.search(pattern, content, re.DOTALL) is not None for content in completion_contents]\n",
    "    format_rewards = [3.0 if match else -2.0 for match in matches]\n",
    "    \n",
    "    if logger:\n",
    "        format_match_rate = sum(matches) / len(matches) if matches else 0\n",
    "        logger.log_reward(step, {\n",
    "            \"format_match_rate\": format_match_rate,\n",
    "            \"avg_format_reward\": sum(format_rewards) / len(format_rewards) if format_rewards else 0\n",
    "        })\n",
    "        \n",
    "    return format_rewards\n",
    "\n",
    "def return_reward(prompts, completions, returns, **kwargs):\n",
    "    logger = kwargs.get(\"logger\", None)\n",
    "    step = kwargs.get(\"step\", 0)\n",
    "    \n",
    "    rewards = []\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    position_rewards = []\n",
    "    format_valid_count = 0\n",
    "    \n",
    "    for i, completion in enumerate(completion_contents):\n",
    "        try:\n",
    "            position = extract_positioning(completion)\n",
    "            format_valid = re.search(pattern, completion, re.DOTALL) is not None\n",
    "            \n",
    "            if format_valid:\n",
    "                format_valid_count += 1\n",
    "                position_reward = position * returns[i % len(returns)]\n",
    "                position_rewards.append(position_reward)\n",
    "                rewards.append(position_reward)\n",
    "            else:\n",
    "                rewards.append(-1.0)\n",
    "                \n",
    "            if logger and i < 2 and step % 50 == 0:\n",
    "                prompt_text = prompts[i][1][\"content\"] if isinstance(prompts[i], list) else prompts[i]\n",
    "                logger.log_completion(\n",
    "                    step=step,\n",
    "                    prompt=prompt_text,\n",
    "                    completion=completion[:500],\n",
    "                    reward=rewards[-1],\n",
    "                    format_valid=format_valid,\n",
    "                    position=position if format_valid else None\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            rewards.append(-1.0)\n",
    "    \n",
    "    if logger:\n",
    "        format_valid_rate = format_valid_count / len(completion_contents) if completion_contents else 0\n",
    "        avg_position_reward = sum(position_rewards) / len(position_rewards) if position_rewards else 0\n",
    "        \n",
    "        logger.log_reward(step, {\n",
    "            \"format_valid_rate\": format_valid_rate,\n",
    "            \"avg_position_reward\": avg_position_reward,\n",
    "            \"avg_total_reward\": sum(rewards) / len(rewards) if rewards else 0\n",
    "        })\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self, logger, tokenizer, data):\n",
    "        self.logger = logger\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.metrics_history = {\n",
    "            \"loss\": [],\n",
    "            \"learning_rate\": [],\n",
    "            \"format_match_rate\": [],\n",
    "            \"reward\": []\n",
    "        }\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            for key, value in logs.items():\n",
    "                if key in self.metrics_history:\n",
    "                    self.metrics_history[key].append((state.global_step, value))\n",
    "            \n",
    "            self.logger.log_info(f\"Step {state.global_step} - Metrics: {logs}\")\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        if state.global_step % 100 == 0 and model is not None:\n",
    "            self.sample_generation(state.global_step, model)\n",
    "    \n",
    "    def sample_generation(self, step, model):\n",
    "        self.logger.log_info(f\"\\n=== Step {step} Sample Generation ===\")\n",
    "        sample = self.data[0]\n",
    "        messages = sample['prompt']\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        output = self.tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "        has_format = re.search(pattern, output, re.DOTALL) is not None\n",
    "        \n",
    "        try:\n",
    "            position = extract_positioning(output) if has_format else None\n",
    "        except:\n",
    "            position = None\n",
    "            \n",
    "        self.logger.log_completion(\n",
    "            step=step,\n",
    "            prompt=text[:200],\n",
    "            completion=output,\n",
    "            reward=None,\n",
    "            format_valid=has_format,\n",
    "            position=position\n",
    "        )\n",
    "        \n",
    "        self.logger.log_info(f\"Output: {output[:200]}...\")\n",
    "        self.logger.log_info(f\"Has correct format: {has_format}\")\n",
    "        if position is not None:\n",
    "            self.logger.log_info(f\"Extracted positioning: {position}\")\n",
    "\n",
    "def main():\n",
    "    log_dir = \"./logs\"\n",
    "    output_dir = \"./outputs/Qwen-2.5-0.5B-GRPO-trader\"\n",
    "    run_name = \"Qwen-2.5-0.5B-GRPO-trader\"\n",
    "    \n",
    "    logger = Logger(log_dir)\n",
    "    logger.log_info(\"Starting GRPO Trader training\")\n",
    "    \n",
    "    model_path = './huggingface_mirror/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775'\n",
    "    \n",
    "    logger.log_info(f\"Loading dataset\")\n",
    "    train_df = pd.read_csv('../train.csv')\n",
    "    data = prepare_data(train_df, logger)\n",
    "    \n",
    "    logger.log_info(f\"Loading model and tokenizer from {model_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        padding_side=\"left\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    logger.log_info(\"Configuring training parameters\")\n",
    "    grpo_config = GRPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        run_name=run_name,\n",
    "        learning_rate=5e-6,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        logging_steps=10,\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_generations=4,\n",
    "        max_prompt_length=1024,\n",
    "        max_completion_length=512,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=100,\n",
    "        max_grad_norm=0.1,\n",
    "        log_on_each_node=False,\n",
    "        temperature=0.7,\n",
    "        num_iterations=2,\n",
    "        reward_weights=[1.0, 1.0],\n",
    "        log_completions=True\n",
    "    )\n",
    "    \n",
    "    # Create a factory for reward functions that capture the logger and step\n",
    "    def create_reward_funcs(logger, model):\n",
    "        def format_reward_with_logging(prompts, completions, **kwargs):\n",
    "            step = model.state.global_step if hasattr(model, 'state') else 0\n",
    "            return format_reward_func(prompts, completions, logger=logger, step=step)\n",
    "            \n",
    "        def return_reward_with_logging(prompts, completions, returns, **kwargs):\n",
    "            step = model.state.global_step if hasattr(model, 'state') else 0\n",
    "            return return_reward(prompts, completions, returns, logger=logger, step=step)\n",
    "            \n",
    "        return [format_reward_with_logging, return_reward_with_logging]\n",
    "    \n",
    "    logger.log_info(\"Initializing trainer\")\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=create_reward_funcs(logger, trainer),\n",
    "        args=grpo_config,\n",
    "        train_dataset=data,\n",
    "    )\n",
    "    \n",
    "    metrics_callback = MetricsCallback(logger, tokenizer, data)\n",
    "    trainer.add_callback(metrics_callback)\n",
    "    \n",
    "    logger.log_info(\"Starting training\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        logger.log_info(\"Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.log_error(f\"Training failed with error: {str(e)}\")\n",
    "    finally:\n",
    "        logger.save_rewards()\n",
    "        logger.save_completions()\n",
    "        logger.log_info(\"Saved all logs and metrics\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
