{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_step(self, query_tensors, response_tensors, rewards):\n",
    "    \"\"\"Run PPO optimization step\n",
    "    \n",
    "    Parameters:\n",
    "    - query_tensors: List of tensors for queries\n",
    "    - response_tensors: List of tensors for responses\n",
    "    - rewards: List of reward values\n",
    "    \n",
    "    Returns:\n",
    "    - Statistics from the PPO update\n",
    "    \"\"\"\n",
    "    if self.trainer is None:\n",
    "        self.initialize_trainer()\n",
    "    \n",
    "    # Format inputs for PPO trainer\n",
    "    texts = []\n",
    "    for i in range(len(query_tensors)):\n",
    "        query = query_tensors[i]\n",
    "        response = response_tensors[i]\n",
    "        \n",
    "        # Make sure we're working with individual tensors, not batches\n",
    "        if len(query.shape) > 1 and query.shape[0] > 1:\n",
    "            # Handle batched tensors - we'll process each item separately\n",
    "            for j in range(query.shape[0]):\n",
    "                prompt_text = self.tokenizer.decode(query[j], skip_special_tokens=True)\n",
    "                # Calculate where the response starts in the full sequence\n",
    "                query_length = query[j].shape[0]\n",
    "                response_text = self.tokenizer.decode(\n",
    "                    response[j][query_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                texts.append({\n",
    "                    \"prompt\": prompt_text,\n",
    "                    \"response\": response_text,\n",
    "                })\n",
    "        else:\n",
    "            # Handle single tensors\n",
    "            prompt_text = self.tokenizer.decode(query, skip_special_tokens=True)\n",
    "            query_length = query.shape[0]\n",
    "            response_text = self.tokenizer.decode(\n",
    "                response[query_length:], \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            texts.append({\n",
    "                \"prompt\": prompt_text,\n",
    "                \"response\": response_text,\n",
    "            })\n",
    "    \n",
    "    # Ensure rewards match the number of texts\n",
    "    if isinstance(rewards, list) and len(rewards) != len(texts):\n",
    "        # If lengths don't match, we need to expand the rewards\n",
    "        if len(rewards) == 1:\n",
    "            # If we have a single reward, duplicate it\n",
    "            rewards = [rewards[0]] * len(texts)\n",
    "        else:\n",
    "            # Otherwise, we need to handle this mismatch more carefully\n",
    "            print(f\"Warning: Number of rewards ({len(rewards)}) doesn't match number of texts ({len(texts)})\")\n",
    "            # Simple approach: truncate or pad with the last value\n",
    "            if len(rewards) < len(texts):\n",
    "                last_reward = rewards[-1]\n",
    "                rewards = rewards + [last_reward] * (len(texts) - len(rewards))\n",
    "            else:\n",
    "                rewards = rewards[:len(texts)]\n",
    "    \n",
    "    # Log what we're passing to the PPO trainer\n",
    "    print(f\"Running PPO step with {len(texts)} text pairs and {len(rewards) if isinstance(rewards, list) else 'tensor'} rewards\")\n",
    "    \n",
    "    # Run PPO step with formatted texts and rewards\n",
    "    try:\n",
    "        stats = self.trainer.step(texts, rewards)\n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error in PPO step: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingOrchestrator:\n",
    "    \"\"\"Manages the PPO training process for the portfolio manager\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MacroTradingEnv, agent: PPOPortfolioManager, output_dir=\"output/ppo_portfolio_manager\", \n",
    "                 use_sequential_training=False):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 1000\n",
    "        self.episode_length = 21  # Trading days per episode\n",
    "        self.output_dir = output_dir\n",
    "        self.use_sequential_training = use_sequential_training\n",
    "\n",
    "    def compute_format_reward(self, response_text: str) -> float:\n",
    "        \"\"\"Calculate reward for formatting according to required XML structure\"\"\"\n",
    "        # Check overall structure\n",
    "        has_correct_format = self.agent.check_format(response_text)\n",
    "        \n",
    "        # Check individual tags\n",
    "        has_macro_state = \"<macro state>\" in response_text and \"</macro state>\" in response_text\n",
    "        has_reasoning = \"<reasoning>\" in response_text and \"</reasoning>\" in response_text\n",
    "        has_positioning = \"<positioning>\" in response_text and \"</positioning>\" in response_text\n",
    "        \n",
    "        # Calculate format reward component\n",
    "        if has_correct_format:\n",
    "            return 0.5  # Full format reward\n",
    "        elif has_macro_state and has_reasoning and has_positioning:\n",
    "            return 0.3  # Tags exist but not in correct order/format\n",
    "        elif (has_macro_state and has_reasoning) or (has_macro_state and has_positioning) or (has_reasoning and has_positioning):\n",
    "            return 0.1  # Some tags exist\n",
    "        else:\n",
    "            return -0.2  # Format completely wrong\n",
    "    \n",
    "    def collect_experience(self, num_episodes=10):\n",
    "        \"\"\"Collect trading experience by running multiple episodes\"\"\"\n",
    "        all_episode_rewards = []\n",
    "        \n",
    "        for episode in tqdm(range(num_episodes), desc=\"Collecting experience\"):\n",
    "            state = self.env.reset(random_start=True)\n",
    "            episode_queries = []\n",
    "            episode_responses = []\n",
    "            episode_rewards = []\n",
    "            episode_response_texts = []\n",
    "            episode_format_rewards = []\n",
    "            \n",
    "            # Run one episode\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < self.episode_length:\n",
    "                # Get action from agent\n",
    "                position, response_text, query, response = self.agent.predict(state)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, info = self.env.step(position)\n",
    "                \n",
    "                # Calculate format reward\n",
    "                format_reward = self.compute_format_reward(response_text)\n",
    "                \n",
    "                # Combine rewards\n",
    "                total_reward = reward + format_reward\n",
    "                \n",
    "                # Store experience\n",
    "                episode_queries.append(query)\n",
    "                episode_responses.append(response)\n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_response_texts.append(response_text)\n",
    "                episode_format_rewards.append(format_reward)\n",
    "                \n",
    "                # Move to next state\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            \n",
    "            # Calculate returns with discount\n",
    "            returns = self._calculate_returns(episode_rewards)\n",
    "            \n",
    "            # Store episodes in buffer\n",
    "            for i in range(len(episode_rewards)):\n",
    "                self.buffer.append({\n",
    "                    'query': episode_queries[i],\n",
    "                    'response': episode_responses[i],\n",
    "                    'reward': returns[i],\n",
    "                    'raw_reward': episode_rewards[i],\n",
    "                    'response_text': episode_response_texts[i],\n",
    "                    'format_reward': episode_format_rewards[i]\n",
    "                })\n",
    "            \n",
    "            # Keep buffer size in check\n",
    "            if len(self.buffer) > self.buffer_size:\n",
    "                self.buffer = self.buffer[-self.buffer_size:]\n",
    "                \n",
    "            all_episode_rewards.extend(episode_rewards)\n",
    "        \n",
    "        # Return average reward per step\n",
    "        return np.mean(all_episode_rewards) if all_episode_rewards else 0.0\n",
    "\n",
    "    def _calculate_returns(self, rewards):\n",
    "        \"\"\"Calculate discounted returns\"\"\"\n",
    "        gamma = self.agent.ppo_config.gamma\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def train(self, num_iterations=100, num_episodes_per_iter=5, batch_size=8):\n",
    "        \"\"\"Main training loop with full PPO training\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            # Collect new experience\n",
    "            avg_reward = self.collect_experience(num_episodes=num_episodes_per_iter)\n",
    "            \n",
    "            # Skip training if buffer is too small\n",
    "            if len(self.buffer) < batch_size:\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}: Buffer too small ({len(self.buffer)}), skipping training\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Select experiences for this training iteration\n",
    "                if self.use_sequential_training:\n",
    "                    # Sequential approach: Use most recent experiences\n",
    "                    start_idx = max(0, len(self.buffer) - batch_size)\n",
    "                    batch = self.buffer[start_idx:]\n",
    "                    print(f\"Using sequential batch of size {len(batch)} (from idx {start_idx})\")\n",
    "                else:\n",
    "                    # Random sampling approach (original)\n",
    "                    batch_indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), replace=False)\n",
    "                    batch = [self.buffer[i] for i in batch_indices]\n",
    "                    print(f\"Using random batch of size {len(batch)}\")\n",
    "                \n",
    "                # Extract the components\n",
    "                queries = [item['query'] for item in batch]\n",
    "                responses = [item['response'] for item in batch]\n",
    "                rewards = [item['reward'] for item in batch]\n",
    "                \n",
    "                # Create a dataset for training\n",
    "                train_dataset = self.agent.create_training_dataset(queries, responses, rewards)\n",
    "                \n",
    "                # Initialize a fresh PPOTrainer with this dataset\n",
    "                self.agent.initialize_trainer(train_dataset=train_dataset)\n",
    "                \n",
    "                # Train for a few steps\n",
    "                print(\"Starting PPO training...\")\n",
    "                \n",
    "                # We need to temporarily modify the PPOConfig for shorter training\n",
    "                original_num_train_epochs = self.agent.ppo_config.num_train_epochs\n",
    "                self.agent.ppo_config.num_train_epochs = 1  # Just do one epoch for this batch\n",
    "                \n",
    "                # Run the full PPO training process\n",
    "                self.agent.trainer.train()\n",
    "                \n",
    "                # Restore original settings\n",
    "                self.agent.ppo_config.num_train_epochs = original_num_train_epochs\n",
    "                \n",
    "                # Log sample response\n",
    "                sample_idx = 0  # Just use the first sample for logging\n",
    "                sample = batch[sample_idx]\n",
    "                print(f\"\\nSample response: {sample['response_text'][:200]}...\")\n",
    "                print(f\"Trading reward: {sample['raw_reward'] - sample['format_reward']:.4f}\")\n",
    "                print(f\"Format reward: {sample['format_reward']:.4f}\")\n",
    "                print(f\"Total return: {sample['reward']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during PPO training: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                save_path = f\"{self.output_dir}/checkpoint-{iteration+1}\"\n",
    "                self.agent.policy_model.save_pretrained(save_path)\n",
    "                self.agent.tokenizer.save_pretrained(save_path)\n",
    "            \n",
    "            # Logging\n",
    "            print(f\"Iteration {iteration+1}/{num_iterations}\")\n",
    "            print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "            print(f\"Buffer Size: {len(self.buffer)}\")\n",
    "            print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
