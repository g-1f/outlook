{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import Dataset\n",
    "import copy\n",
    "\n",
    "# Set up logging\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "log_file = f\"logs/training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"GRPO-Trader\")\n",
    "\n",
    "# Define system prompt with example\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "Example:\n",
    "Market Context: PMI:52.6, inflation:5.1%, unemployment:3.8%\n",
    "\n",
    "<macro state>\n",
    "Economy is showing strength with PMI in expansion territory, but inflation remains elevated.\n",
    "</macro state>\n",
    "<reasoning>\n",
    "The PMI at 52.6 indicates expansion, which is positive for equities. However, inflation at 5.1% is above the Fed's target, suggesting possible rate hikes. Employment is strong at 3.8%, supporting consumer spending.\n",
    "</reasoning>\n",
    "<positioning>\n",
    "0.3\n",
    "</positioning>\n",
    "\n",
    "You must respond in the above XML format.\n",
    "\"\"\"\n",
    "\n",
    "# File path constants\n",
    "RESULTS_DIR = \"./results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "REWARDS_FILE = os.path.join(RESULTS_DIR, f\"rewards_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\")\n",
    "COMPLETIONS_FILE = os.path.join(RESULTS_DIR, f\"completions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\")\n",
    "MODEL_DIR = \"./outputs/Qwen-2.5-0.5B-GRPO-trader\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "def log_to_jsonl(file_path, data):\n",
    "    \"\"\"Append a JSON entry to a JSONL file\"\"\"\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "def extract_positioning(text):\n",
    "    \"\"\"Extract the positioning value from the XML format\"\"\"\n",
    "    try:\n",
    "        match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            value = match.group(1).strip()\n",
    "            return float(value)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting positioning: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def check_format(text):\n",
    "    \"\"\"Check if text follows the expected XML format\"\"\"\n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    return re.search(pattern, text, re.DOTALL) is not None\n",
    "\n",
    "def format_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if completions follow the expected format\"\"\"\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    # Log sample completions periodically\n",
    "    global_step = kwargs.get(\"global_step\", 0)\n",
    "    if global_step % 20 == 0 and len(completion_contents) > 0:\n",
    "        sample_idx = min(2, len(completion_contents) - 1)\n",
    "        sample = completion_contents[sample_idx]\n",
    "        format_valid = check_format(sample)\n",
    "        logger.info(f\"Step {global_step} - Sample completion (format valid: {format_valid}):\\n{sample[:200]}...\")\n",
    "    \n",
    "    # Calculate rewards\n",
    "    is_valid_format = [check_format(content) for content in completion_contents]\n",
    "    format_rewards = [3.0 if valid else -2.0 for valid in is_valid_format]\n",
    "    \n",
    "    # Log statistics\n",
    "    if len(is_valid_format) > 0:\n",
    "        format_rate = sum(is_valid_format) / len(is_valid_format)\n",
    "        log_to_jsonl(REWARDS_FILE, {\n",
    "            \"step\": global_step,\n",
    "            \"time\": time.time(),\n",
    "            \"type\": \"format\",\n",
    "            \"format_valid_rate\": format_rate,\n",
    "            \"avg_reward\": sum(format_rewards) / len(format_rewards)\n",
    "        })\n",
    "        \n",
    "    return format_rewards\n",
    "\n",
    "def return_reward(prompts, completions, returns, **kwargs):\n",
    "    \"\"\"Reward function based on the positioning value and returns\"\"\"\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    global_step = kwargs.get(\"global_step\", 0)\n",
    "    rewards = []\n",
    "    positions = []\n",
    "    valid_formats = []\n",
    "    \n",
    "    for i, completion in enumerate(completion_contents):\n",
    "        try:\n",
    "            format_valid = check_format(completion)\n",
    "            valid_formats.append(format_valid)\n",
    "            \n",
    "            if format_valid:\n",
    "                position = extract_positioning(completion)\n",
    "                positions.append(position)\n",
    "                reward = position * returns[i % len(returns)]\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Log sample completions with their rewards\n",
    "                if i < 2 and global_step % 20 == 0:\n",
    "                    prompt_text = prompts[i][1][\"content\"] if isinstance(prompts[i], list) else prompts[i]\n",
    "                    log_to_jsonl(COMPLETIONS_FILE, {\n",
    "                        \"step\": global_step,\n",
    "                        \"prompt\": prompt_text,\n",
    "                        \"completion\": completion[:500],\n",
    "                        \"position\": position,\n",
    "                        \"reward\": reward,\n",
    "                        \"return\": returns[i % len(returns)]\n",
    "                    })\n",
    "            else:\n",
    "                rewards.append(-1.0)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating reward: {e}\")\n",
    "            rewards.append(-1.0)\n",
    "    \n",
    "    # Log statistics\n",
    "    if len(rewards) > 0:\n",
    "        log_to_jsonl(REWARDS_FILE, {\n",
    "            \"step\": global_step,\n",
    "            \"time\": time.time(),\n",
    "            \"type\": \"returns\",\n",
    "            \"avg_reward\": sum(rewards) / len(rewards),\n",
    "            \"format_valid_rate\": sum(valid_formats) / len(valid_formats) if valid_formats else 0,\n",
    "            \"avg_position\": sum(positions) / len(positions) if positions else 0\n",
    "        })\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "class SampleGenerationCallback(TrainerCallback):\n",
    "    \"\"\"Callback to generate sample completions during training\"\"\"\n",
    "    def __init__(self, tokenizer, data):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0:\n",
    "            # Access model through the trainer\n",
    "            trainer = kwargs.get(\"trainer\", None)\n",
    "            if trainer is not None and hasattr(trainer, \"model\"):\n",
    "                self.generate_sample(trainer.model, state.global_step)\n",
    "    \n",
    "    def generate_sample(self, model, step):\n",
    "        logger.info(f\"\\n===== Sample Generation at Step {step} =====\")\n",
    "        \n",
    "        # Get a sample from the dataset\n",
    "        sample = self.data[0]\n",
    "        messages = copy.deepcopy(sample['prompt'])  # Deep copy to prevent modification\n",
    "        \n",
    "        # Generate completion\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        output = self.tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Analyze output\n",
    "        format_valid = check_format(output)\n",
    "        position = extract_positioning(output) if format_valid else None\n",
    "        \n",
    "        # Log results\n",
    "        logger.info(f\"Format valid: {format_valid}\")\n",
    "        if position is not None:\n",
    "            logger.info(f\"Position: {position}\")\n",
    "        logger.info(f\"Output: {output[:300]}...\")\n",
    "        \n",
    "        # Save to completions file\n",
    "        log_to_jsonl(COMPLETIONS_FILE, {\n",
    "            \"step\": step,\n",
    "            \"type\": \"sample_generation\",\n",
    "            \"prompt\": text[:200],\n",
    "            \"completion\": output,\n",
    "            \"format_valid\": format_valid,\n",
    "            \"position\": position\n",
    "        })\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare training data from DataFrame\"\"\"\n",
    "    logger.info(\"Preparing training data...\")\n",
    "    \n",
    "    def prepare_prompt(df):\n",
    "        df['prompt'] = df.apply(lambda row: [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT.strip()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Market Context:{', '.join(f'{k}:{v}' for k, v in row.drop('date', errors='ignore').items())}\"\n",
    "            }\n",
    "        ], axis=1)\n",
    "        return df\n",
    "\n",
    "    df = prepare_prompt(df)\n",
    "    df['returns'] = df['close'].pct_change().shift(-1)\n",
    "    train_dataset = df[['prompt', 'returns']]\n",
    "    data = Dataset.from_pandas(train_dataset, preserve_index=False)\n",
    "    \n",
    "    logger.info(f\"Prepared {len(data)} training examples\")\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Starting GRPO Trader training\")\n",
    "    \n",
    "    # Model configuration\n",
    "    model_path = './huggingface_mirror/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775'\n",
    "    \n",
    "    # Load dataset\n",
    "    train_df = pd.read_csv('../train.csv')\n",
    "    data = prepare_data(train_df)\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    logger.info(f\"Loading model from {model_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        padding_side=\"left\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # GRPO Configuration\n",
    "    grpo_config = GRPOConfig(\n",
    "        output_dir=MODEL_DIR,\n",
    "        run_name=\"Qwen-2.5-0.5B-GRPO-trader\",\n",
    "        learning_rate=5e-6,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        logging_steps=10,\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_generations=4,\n",
    "        max_prompt_length=1024,\n",
    "        max_completion_length=512,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=100,\n",
    "        max_grad_norm=0.1,\n",
    "        temperature=0.7,\n",
    "        num_iterations=2,\n",
    "        reward_weights=[1.0, 1.0],\n",
    "        log_completions=True\n",
    "    )\n",
    "    \n",
    "    # Create a wrapper for the reward functions to include the step information\n",
    "    def format_reward_with_step(prompts, completions, **kwargs):\n",
    "        return format_reward_func(prompts, completions, global_step=trainer.state.global_step, **kwargs)\n",
    "        \n",
    "    def return_reward_with_step(prompts, completions, returns, **kwargs):\n",
    "        return return_reward(prompts, completions, returns, global_step=trainer.state.global_step, **kwargs)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    logger.info(\"Initializing trainer\")\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        args=grpo_config,\n",
    "        train_dataset=data,\n",
    "        reward_funcs=[\n",
    "            lambda *args, **kwargs: format_reward_func(*args, global_step=trainer.state.global_step if hasattr(trainer, 'state') else 0, **kwargs),\n",
    "            lambda *args, **kwargs: return_reward(*args, global_step=trainer.state.global_step if hasattr(trainer, 'state') else 0, **kwargs)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Add callbacks\n",
    "    sample_callback = SampleGenerationCallback(tokenizer, data)\n",
    "    trainer.add_callback(sample_callback)\n",
    "    \n",
    "    # Start training\n",
    "    logger.info(\"Starting training\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        logger.info(\"Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed with error: {str(e)}\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
