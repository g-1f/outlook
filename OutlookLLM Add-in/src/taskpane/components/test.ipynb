{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FlagTraderAgent:\n",
    "    \"\"\"\n",
    "    Implementation of FLAG-TRADER agent: Fusion LLM-Agent with Gradient-based\n",
    "    Reinforcement Learning for Financial Trading.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, device='cuda:0', freeze_layers=0.7):\n",
    "        \"\"\"\n",
    "        Initialize FLAG-TRADER agent with parameter-efficient fine-tuning\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the LLM to use\n",
    "            device (str): Device to run the model on\n",
    "            freeze_layers (float): Proportion of layers to freeze (0.0-1.0)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # PPO Configuration\n",
    "        self.config = PPOConfig(\n",
    "            model_name=model_name,\n",
    "            learning_rate=5e-5,\n",
    "            batch_size=32,\n",
    "            mini_batch_size=4,\n",
    "            gradient_accumulation_steps=8,\n",
    "            num_train_epochs=3,\n",
    "            vf_coef=0.1,\n",
    "            kl_coef=0.05,\n",
    "            cliprange=0.2,\n",
    "            gamma=0.99,\n",
    "            lam=0.95,\n",
    "            output_dir=\"ppo_output\",\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        # Load policy model with value head (actor-critic)\n",
    "        self.policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=self.device\n",
    "        )\n",
    "\n",
    "        # Implement parameter-efficient fine-tuning\n",
    "        self._freeze_bottom_layers(freeze_proportion=freeze_layers)\n",
    "        \n",
    "        # Load reference model (for PPO's KL penalty)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=self.device\n",
    "        )\n",
    "\n",
    "        # Value model is embedded within policy_model (provided by trl)\n",
    "        self.value_model = self.policy_model\n",
    "        \n",
    "        # Simple reward model\n",
    "        class SimpleRewardModel(torch.nn.Module):\n",
    "            def forward(self, input_ids, attention_mask=None):\n",
    "                return torch.ones((input_ids.shape[0], 1), device=input_ids.device)\n",
    "\n",
    "        self.reward_model = SimpleRewardModel().to(self.device)\n",
    "\n",
    "        # PPO Trainer\n",
    "        self.trainer = PPOTrainer(\n",
    "            config=self.config,\n",
    "            model=self.policy_model,\n",
    "            ref_model=self.ref_model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            dataset=None,  # Will be set during training\n",
    "            optimizer=None  # Will use default Adam optimizer\n",
    "        )\n",
    "    \n",
    "    def _freeze_bottom_layers(self, freeze_proportion: float):\n",
    "        \"\"\"\n",
    "        Implement parameter-efficient fine-tuning by freezing bottom layers\n",
    "        \n",
    "        Args:\n",
    "            freeze_proportion (float): Proportion of layers to freeze (0.0-1.0)\n",
    "        \"\"\"\n",
    "        # Get all transformers layers\n",
    "        if hasattr(self.policy_model, 'transformer'):\n",
    "            transformer_layers = self.policy_model.transformer.h\n",
    "        elif hasattr(self.policy_model, 'model') and hasattr(self.policy_model.model, 'layers'):\n",
    "            transformer_layers = self.policy_model.model.layers\n",
    "        else:\n",
    "            logger.warning(\"Could not identify transformer layers. No layers frozen.\")\n",
    "            return\n",
    "        \n",
    "        # Calculate how many layers to freeze\n",
    "        num_layers = len(transformer_layers)\n",
    "        num_frozen = int(num_layers * freeze_proportion)\n",
    "        \n",
    "        logger.info(f\"Freezing {num_frozen} of {num_layers} transformer layers\")\n",
    "        \n",
    "        # Freeze bottom layers\n",
    "        for i in range(num_frozen):\n",
    "            for param in transformer_layers[i].parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Log number of trainable parameters\n",
    "        total_params = sum(p.numel() for p in self.policy_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.policy_model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Total parameters: {total_params:,}\")\n",
    "        logger.info(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "\n",
    "    def generate_prompt(self, state: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate a structured prompt for the LLM based on the current state\n",
    "        \n",
    "        Args:\n",
    "            state (Dict): Current market state including price history and account info\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt for the LLM\n",
    "        \"\"\"\n",
    "        # Extract key information from state\n",
    "        price_data = state.get(\"price_data\", [])\n",
    "        account_info = state.get(\"account_info\", {})\n",
    "        \n",
    "        # Format state information into a structured prompt\n",
    "        prompt = (\n",
    "            \"Financial Stock Trading\\n\"\n",
    "            \"Task: Assist in making optimal buy, hold, or sell decisions for stock \"\n",
    "            \"portfolio. The goal is to maximize returns while managing risk.\\n\\n\"\n",
    "            \"Legible Actions: Choose from \\\"Buy\\\", \\\"Sell\\\", or \\\"Hold\\\" based on \"\n",
    "            \"market conditions and risk assessment.\\n\\n\"\n",
    "            \"Current State:\\n\"\n",
    "            f\"Price History: {price_data}\\n\"\n",
    "            f\"Cash Balance: {account_info.get('cash', 0):.2f}\\n\"\n",
    "            f\"Asset Position: {account_info.get('asset', 0)}\\n\"\n",
    "            f\"Total Value: {account_info.get('total', 0):.2f}\\n\\n\"\n",
    "            \"Output Action: \"\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def predict(self, state: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Predict the next action given the current state\n",
    "        \n",
    "        Args:\n",
    "            state (Dict): Current market state\n",
    "            \n",
    "        Returns:\n",
    "            str: Predicted action (Buy, Sell, or Hold)\n",
    "        \"\"\"\n",
    "        prompt = self.generate_prompt(state)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate response with temperature for exploration\n",
    "        outputs = self.policy_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=16, \n",
    "            temperature=0.7, \n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Extract the generated text\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse the action from the generated text\n",
    "        action = generated_text.split(\"Output Action:\")[-1].strip()\n",
    "        \n",
    "        # Normalize action text - handle various formats the LLM might produce\n",
    "        if \"buy\" in action.lower():\n",
    "            return \"Buy\"\n",
    "        elif \"sell\" in action.lower():\n",
    "            return \"Sell\"\n",
    "        else:\n",
    "            return \"Hold\"\n",
    "\n",
    "class FlagTraderEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Trading environment for FLAG-TRADER, implementing Sharpe ratio based rewards\n",
    "    \"\"\"\n",
    "    def __init__(self, data, initial_balance=10000, window_size=10, risk_free_rate=0):\n",
    "        \"\"\"\n",
    "        Initialize trading environment\n",
    "        \n",
    "        Args:\n",
    "            data (np.ndarray): Market data with time steps as rows and features as columns\n",
    "            initial_balance (float): Initial cash balance\n",
    "            window_size (int): Size of historical window to use for state\n",
    "            risk_free_rate (float): Risk-free rate for Sharpe ratio calculation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        if not isinstance(data, np.ndarray):\n",
    "            raise TypeError(\"Data must be a numpy array\")\n",
    "        \n",
    "        self.data = data\n",
    "        self.initial_balance = initial_balance\n",
    "        self.window_size = window_size\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(3)  # Buy (0), Hold (1), Sell (2)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(window_size, data.shape[1])\n",
    "        )\n",
    "        \n",
    "        # Reset environment\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to initial state\n",
    "        \n",
    "        Returns:\n",
    "            dict: Initial state\n",
    "        \"\"\"\n",
    "        self.current_step = self.window_size  # Start after window_size to have history\n",
    "        self.balance = self.initial_balance\n",
    "        self.asset = 0\n",
    "        self.pnl_history = []\n",
    "        \n",
    "        # Get initial state\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Dict, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Take a step in the environment based on the action\n",
    "        \n",
    "        Args:\n",
    "            action (int): Action to take (0: Buy, 1: Hold, 2: Sell)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (state, reward, done, info)\n",
    "        \"\"\"\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            return self._get_state(), 0, True, {}\n",
    "        \n",
    "        # Get current price\n",
    "        current_price = self.data[self.current_step, -1]\n",
    "        \n",
    "        # Store portfolio value before action\n",
    "        prev_value = self.balance + self.asset * current_price\n",
    "        \n",
    "        # Execute action\n",
    "        if action == 0 and self.balance >= current_price:  # Buy\n",
    "            shares_to_buy = self.balance // current_price\n",
    "            self.asset += shares_to_buy\n",
    "            self.balance -= shares_to_buy * current_price\n",
    "        elif action == 2 and self.asset > 0:  # Sell\n",
    "            self.balance += self.asset * current_price\n",
    "            self.asset = 0\n",
    "        \n",
    "        # Move to next time step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Calculate new portfolio value\n",
    "        new_price = self.data[self.current_step, -1]\n",
    "        new_value = self.balance + self.asset * new_price\n",
    "        \n",
    "        # Calculate daily PnL and add to history\n",
    "        daily_pnl = new_value - prev_value\n",
    "        self.pnl_history.append(daily_pnl)\n",
    "        \n",
    "        # Calculate reward based on Sharpe ratio\n",
    "        reward = self._calculate_sharpe_ratio()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        # Create info dict\n",
    "        info = {\n",
    "            'portfolio_value': new_value,\n",
    "            'daily_pnl': daily_pnl,\n",
    "            'sharpe_ratio': reward\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "    def _get_state(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get current state representation\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing state information\n",
    "        \"\"\"\n",
    "        # Get window of historical data\n",
    "        historical_window = self.data[self.current_step - self.window_size:self.current_step]\n",
    "        \n",
    "        # Create account info\n",
    "        current_price = self.data[self.current_step, -1]\n",
    "        portfolio_value = self.balance + self.asset * current_price\n",
    "        \n",
    "        # Format state as a dictionary\n",
    "        state = {\n",
    "            \"price_data\": historical_window.tolist(),\n",
    "            \"account_info\": {\n",
    "                \"cash\": float(self.balance),\n",
    "                \"asset\": int(self.asset),\n",
    "                \"total\": float(portfolio_value)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _calculate_sharpe_ratio(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Sharpe ratio using PnL history\n",
    "        \n",
    "        Returns:\n",
    "            float: Sharpe ratio value\n",
    "        \"\"\"\n",
    "        if len(self.pnl_history) < 2:\n",
    "            return 0.0\n",
    "            \n",
    "        pnl_array = np.array(self.pnl_history)\n",
    "        avg_pnl = np.mean(pnl_array)\n",
    "        std_pnl = np.std(pnl_array)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if std_pnl == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        sharpe = (avg_pnl - self.risk_free_rate) / std_pnl\n",
    "        \n",
    "        # Annualize the Sharpe ratio (assuming daily data, âˆš252 factor)\n",
    "        annualized_sharpe = sharpe * np.sqrt(252)\n",
    "        \n",
    "        return annualized_sharpe\n",
    "\n",
    "def action_str_to_num(action_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert action string to numeric action\n",
    "    \n",
    "    Args:\n",
    "        action_str (str): Action string (Buy, Hold, or Sell)\n",
    "        \n",
    "    Returns:\n",
    "        int: Numeric action (0: Buy, 1: Hold, 2: Sell)\n",
    "    \"\"\"\n",
    "    action_map = {'Buy': 0, 'Hold': 1, 'Sell': 2}\n",
    "    return action_map.get(action_str, 1)  # Default to Hold if action not recognized\n",
    "\n",
    "def orchestrate_training(agent: FlagTraderAgent, env: FlagTraderEnv, epochs: int = 3, steps_per_epoch: int = 200) -> None:\n",
    "    \"\"\"\n",
    "    Train the FLAG-TRADER agent using PPO\n",
    "    \n",
    "    Args:\n",
    "        agent (FlagTraderAgent): The FLAG-TRADER agent\n",
    "        env (FlagTraderEnv): Trading environment\n",
    "        epochs (int): Number of training epochs\n",
    "        steps_per_epoch (int): Number of steps per epoch\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        logger.info(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Reset collections for this epoch\n",
    "        experiences = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        action_strs = []\n",
    "        rewards = []\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Collect experiences\n",
    "        for step in tqdm(range(steps_per_epoch), desc=f\"Collecting data - Epoch {epoch+1}/{epochs}\"):\n",
    "            # Get action from agent\n",
    "            action_str = agent.predict(state)\n",
    "            action_num = action_str_to_num(action_str)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action_num)\n",
    "            \n",
    "            # Store experience\n",
    "            query = agent.generate_prompt(state)\n",
    "            experiences.append({\n",
    "                \"query\": query,\n",
    "                \"response\": action_str,\n",
    "                \"reward\": float(reward)\n",
    "            })\n",
    "            \n",
    "            # Store state, action, and reward\n",
    "            states.append(state)\n",
    "            actions.append(action_num)\n",
    "            action_strs.append(action_str)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            # Update total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            # Reset if episode is done\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "        \n",
    "        # Convert experiences to Dataset\n",
    "        train_dataset = Dataset.from_list(experiences)\n",
    "        \n",
    "        # Train with PPO\n",
    "        logger.info(f\"Training on {len(experiences)} experiences\")\n",
    "        agent.trainer.train(train_dataset)\n",
    "        \n",
    "        # Calculate and log metrics\n",
    "        avg_reward = total_reward / steps_per_epoch\n",
    "        logger.info(f\"Epoch {epoch+1}: Avg Reward = {avg_reward:.4f}\")\n",
    "        \n",
    "        # Additional logging for action distribution\n",
    "        action_counts = {\n",
    "            'Buy': action_strs.count('Buy'),\n",
    "            'Hold': action_strs.count('Hold'),\n",
    "            'Sell': action_strs.count('Sell')\n",
    "        }\n",
    "        logger.info(f\"Action distribution: {action_counts}\")\n",
    "    \n",
    "    logger.info(\"Training complete\")\n",
    "\n",
    "def create_synthetic_market_data(time_steps: int = 1000, features: int = 5, seed: Optional[int] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create synthetic market data for testing\n",
    "    \n",
    "    Args:\n",
    "        time_steps (int): Number of time steps\n",
    "        features (int): Number of features per time step\n",
    "        seed (int, optional): Random seed\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Synthetic market data\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Create base price series with random walk\n",
    "    price = 100 + np.cumsum(np.random.normal(0.001, 0.02, time_steps))\n",
    "    \n",
    "    # Ensure price is positive\n",
    "    price = np.maximum(price, 0.1)\n",
    "    \n",
    "    # Create additional features (could be technical indicators, etc.)\n",
    "    data = np.random.randn(time_steps, features - 1)\n",
    "    \n",
    "    # Add price as the last column\n",
    "    return np.column_stack((data, price))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run FLAG-TRADER training\"\"\"\n",
    "    # Create synthetic market data\n",
    "    market_data = create_synthetic_market_data(time_steps=1000, features=10, seed=42)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = FlagTraderEnv(market_data, initial_balance=10000, window_size=10)\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = FlagTraderAgent(\"Qwen/Qwen2.5-0.5B-Instruct\", device='cuda:0', freeze_layers=0.7)\n",
    "    \n",
    "    # Run PPO training\n",
    "    orchestrate_training(agent, env, epochs=3, steps_per_epoch=200)\n",
    "    \n",
    "    # Save the trained model\n",
    "    agent.policy_model.save_pretrained(\"flag_trader_model\")\n",
    "    agent.tokenizer.save_pretrained(\"flag_trader_model\")\n",
    "    \n",
    "    logger.info(\"Model saved to flag_trader_model/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
