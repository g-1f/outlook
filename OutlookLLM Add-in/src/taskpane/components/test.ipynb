{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import deque\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Stateful Trading Environment with Daily Rollover\n",
    "# =============================================================================\n",
    "\n",
    "class DailyTradingEnv:\n",
    "    \"\"\"\n",
    "    Daily trading environment that:\n",
    "    - Maintains rolling window of market data\n",
    "    - Tracks open positions and news history\n",
    "    - Calculates incremental rewards\n",
    "    - Manages position sizing automatically\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, initial_cash: float = 1e6):\n",
    "        self.data = data.sort_index().reset_index(drop=True)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.position = 0.0  # -1 (full short) to 1 (full long)\n",
    "        self.cash = initial_cash\n",
    "        self.current_step = 0\n",
    "        self.news_window = deque(maxlen=7)  # 7-day news memory\n",
    "        self.action_history = deque(maxlen=5)  # Last 5 actions\n",
    "        \n",
    "        # State normalization\n",
    "        self._init_normalization()\n",
    "\n",
    "    def _init_normalization(self):\n",
    "        \"\"\"Initialize rolling normalization parameters\"\"\"\n",
    "        windows = [5, 20, 60]\n",
    "        for w in windows:\n",
    "            self.data[f'vol_{w}d'] = self.data['close'].pct_change().rolling(w).std().fillna(0)\n",
    "            self.data[f'sma_{w}d'] = self.data['close'].rolling(w).mean().fillna(0)\n",
    "        \n",
    "        # Calculate RSI with proper handling of NaN values\n",
    "        price_change = self.data['close'].pct_change()\n",
    "        gain = price_change.where(price_change > 0, 0).fillna(0)\n",
    "        loss = -price_change.where(price_change < 0, 0).fillna(0)\n",
    "        avg_gain = gain.rolling(14).mean().fillna(0)\n",
    "        avg_loss = loss.rolling(14).mean().fillna(0)\n",
    "        rs = avg_gain / avg_loss.replace(0, np.nan).fillna(1)\n",
    "        self.data['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "        self.data['rsi_14'] = self.data['rsi_14'].fillna(50)  # Default to neutral RSI if not enough data\n",
    "\n",
    "    def reset(self, random_start: bool = True):\n",
    "        \"\"\"Reset environment to starting state, optionally at a random point\"\"\"\n",
    "        if random_start:\n",
    "            # Ensure we have enough data ahead for a full episode\n",
    "            max_start = len(self.data) - 30  # Minimum 30 days of future data\n",
    "            self.current_step = random.randint(7, max_start) if max_start > 7 else 7\n",
    "        else:\n",
    "            self.current_step = 7  # Start with some history\n",
    "        \n",
    "        # Reset state\n",
    "        self.position = 0.0\n",
    "        self.cash = self.initial_cash\n",
    "        self.news_window.clear()\n",
    "        self.action_history.clear()\n",
    "        \n",
    "        # Initialize news window with past days\n",
    "        for i in range(7):\n",
    "            if self.current_step - i - 1 >= 0:\n",
    "                self.news_window.appendleft(self.data.iloc[self.current_step - i - 1].get('news', 'No news'))\n",
    "        \n",
    "        # Initialize action history with zeros\n",
    "        for _ in range(5):\n",
    "            self.action_history.append(0.0)\n",
    "            \n",
    "        return self.get_state()\n",
    "        \n",
    "    def get_portfolio_value(self) -> float:\n",
    "        \"\"\"Current total portfolio value\"\"\"\n",
    "        return self.cash + self.position * self.data.iloc[self.current_step]['close']\n",
    "\n",
    "    def step(self, new_position: float) -> Tuple[Dict, float, bool, Dict]:\n",
    "        \"\"\"Execute daily position adjustment and return (state, reward, done, info)\"\"\"\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            return self.get_state(), 0.0, True, {'status': 'completed'}\n",
    "            \n",
    "        prev_value = self.get_portfolio_value()\n",
    "        current_price = self.data.iloc[self.current_step]['close']\n",
    "        \n",
    "        # Calculate position delta\n",
    "        position_change = new_position - self.position\n",
    "        transaction_cost = abs(position_change * current_price) * 0.001  # 0.1% friction\n",
    "        self.cash -= transaction_cost\n",
    "        \n",
    "        # Update position\n",
    "        self.position = new_position\n",
    "        \n",
    "        # Move to next day\n",
    "        self.current_step += 1\n",
    "        new_price = self.data.iloc[self.current_step]['close']\n",
    "        \n",
    "        # Calculate daily return\n",
    "        daily_return = (new_price / current_price - 1) * self.position\n",
    "        new_value = self.get_portfolio_value()\n",
    "        \n",
    "        # Add today's news to the window\n",
    "        current_news = self.data.iloc[self.current_step].get('news', 'No news')\n",
    "        self.news_window.append(current_news)\n",
    "        \n",
    "        # Add today's action to history\n",
    "        self.action_history.append(new_position)\n",
    "        \n",
    "        # Calculate reward (daily P&L considering transaction costs)\n",
    "        reward = daily_return - (transaction_cost / prev_value)\n",
    "        \n",
    "        # Check if done\n",
    "        done = (self.current_step >= len(self.data) - 1) or (new_value <= 0)\n",
    "        \n",
    "        info = {\n",
    "            'value': new_value,\n",
    "            'return': daily_return,\n",
    "            'volatility': self.data.iloc[self.current_step]['vol_5d'],\n",
    "            'max_drawdown': self._calculate_drawdown(),\n",
    "            'position_change': position_change,\n",
    "            'transaction_cost': transaction_cost\n",
    "        }\n",
    "        \n",
    "        return self.get_state(), reward, done, info\n",
    "\n",
    "    def get_state(self) -> Dict:\n",
    "        \"\"\"Return current environment state as a dictionary\"\"\"\n",
    "        current_data = self.data.iloc[self.current_step]\n",
    "        return {\n",
    "            'date': current_data.get('date', f'Day {self.current_step}'),\n",
    "            'close': current_data['close'],\n",
    "            'vol_5d': current_data['vol_5d'],\n",
    "            'vol_20d': current_data['vol_20d'],\n",
    "            'sma_5d': current_data['sma_5d'],\n",
    "            'sma_20d': current_data['sma_20d'],\n",
    "            'rsi_14': current_data['rsi_14'],\n",
    "            'position': self.position,\n",
    "            'portfolio_value': self.get_portfolio_value(),\n",
    "            'news_window': list(self.news_window),\n",
    "            'action_history': list(self.action_history)\n",
    "        }\n",
    "\n",
    "    def _calculate_drawdown(self, window=21) -> float:\n",
    "        \"\"\"Rolling maximum drawdown\"\"\"\n",
    "        values = [self.get_portfolio_value()]\n",
    "        if len(values) < window:\n",
    "            return 0.0\n",
    "        peak = max(values[-window:])\n",
    "        return (min(values[-window:]) - peak) / peak if peak > 0 else 0.0\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Context-Aware LLM Trader with Memory (using PPO)\n",
    "# =============================================================================\n",
    "\n",
    "class DailyTrader:\n",
    "    \"\"\"\n",
    "    LLM trader with:\n",
    "    - Rolling news context\n",
    "    - Action history memory\n",
    "    - Position-aware decision making\n",
    "    - PPO-based reinforcement learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # PPO configuration\n",
    "        self.ppo_config = PPOConfig(\n",
    "            learning_rate=1.2e-5,\n",
    "            batch_size=32,\n",
    "            mini_batch_size=8,\n",
    "            gradient_accumulation_steps=1,\n",
    "            optimize_cuda_cache=True,\n",
    "            ppo_epochs=3,\n",
    "            gamma=0.98,\n",
    "            cliprange=0.15,\n",
    "            cliprange_value=0.1,\n",
    "            vf_coef=0.1,\n",
    "            adap_kl_ctrl=True,\n",
    "            init_kl_coef=0.02,\n",
    "            target_kl=0.01,\n",
    "            seed=42,\n",
    "            log_with=\"tensorboard\",\n",
    "            use_score_scaling=True,\n",
    "            use_score_norm=True,\n",
    "            score_clip=None,\n",
    "        )\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        # Initialize model with value head for PPO\n",
    "        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Apply LoRA for parameter-efficient fine-tuning\n",
    "        peft_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Adjust based on model architecture\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        \n",
    "        # Initialize trainer (we'll update this later with the dataset)\n",
    "        self.trainer = None\n",
    "        \n",
    "        # Define text generation parameters\n",
    "        self.generation_kwargs = {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"do_sample\": True,\n",
    "        }\n",
    "\n",
    "    def format_state(self, state: Dict) -> str:\n",
    "        \"\"\"Create daily trading prompt with context\"\"\"\n",
    "        return f\"\"\"Daily S&P 500 Trading Decision - {state['date']}\n",
    "Market Context:\n",
    "- Price: {state['close']:.2f}\n",
    "- 5D Volatility: {state['vol_5d']:.4f}\n",
    "- 20D Volatility: {state['vol_20d']:.4f}\n",
    "- 5D SMA: {state['sma_5d']:.2f}\n",
    "- 20D SMA: {state['sma_20d']:.2f}\n",
    "- RSI(14): {state['rsi_14']:.1f}\n",
    "- Current Position: {state['position']:.2f}\n",
    "- Portfolio Value: ${state['portfolio_value']:,.2f}\n",
    "\n",
    "Recent News:\n",
    "{'\\n'.join(state['news_window'])}\n",
    "\n",
    "Previous Actions (Last 5 Days):\n",
    "{'\\n'.join([f\"Day-{i+1}: {pos:.2f}\" for i, pos in enumerate(reversed(state['action_history']))])}\n",
    "\n",
    "Output Format (JSON):\n",
    "{{\n",
    "  \"analysis\": \"<market_analysis>\",\n",
    "  \"decision\": {{\n",
    "    \"position_target\": [-1.0 to 1.0],\n",
    "    \"rationale\": \"<risk-adjusted reasoning>\",\n",
    "    \"confidence\": [0.0-1.0]\n",
    "  }},\n",
    "  \"risk_management\": {{\n",
    "    \"stop_loss\": <optional_price>,\n",
    "    \"profit_target\": <optional_price>\n",
    "  }}\n",
    "}}\"\"\"\n",
    "\n",
    "    def parse_response(self, response: str) -> Dict:\n",
    "        \"\"\"Robust JSON parsing with error correction\"\"\"\n",
    "        try:\n",
    "            # First, try to extract the JSON block if it's in markdown format\n",
    "            if \"```json\" in response:\n",
    "                json_str = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            elif \"```\" in response:\n",
    "                json_str = response.split(\"```\")[1].strip()\n",
    "            else:\n",
    "                # Try to find a JSON object using regex\n",
    "                match = re.search(r'({[\\s\\S]*})', response)\n",
    "                if match:\n",
    "                    json_str = match.group(1)\n",
    "                else:\n",
    "                    # Fallback to using the entire response\n",
    "                    json_str = response\n",
    "            \n",
    "            # Parse the JSON\n",
    "            decision = json.loads(json_str)\n",
    "            \n",
    "            # Extract and validate the position target\n",
    "            position = float(decision['decision']['position_target'])\n",
    "            position = np.clip(position, -1.0, 1.0)\n",
    "            \n",
    "            # Extract confidence if available, default to 0.5\n",
    "            confidence = decision['decision'].get('confidence', 0.5)\n",
    "            confidence = np.clip(float(confidence), 0.0, 1.0)\n",
    "            \n",
    "            return {\n",
    "                'position': position,\n",
    "                'confidence': confidence,\n",
    "                'stop_loss': decision['risk_management'].get('stop_loss'),\n",
    "                'take_profit': decision['risk_management'].get('profit_target')\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\")\n",
    "            print(f\"Response: {response}\")\n",
    "            # Return default values if parsing fails\n",
    "            return {'position': 0.0, 'confidence': 0.5}\n",
    "\n",
    "    def predict(self, state: Dict) -> Dict:\n",
    "        \"\"\"Generate a trading decision based on current state\"\"\"\n",
    "        prompt = self.format_state(state)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            **self.generation_kwargs\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        decision = self.parse_response(response)\n",
    "        return decision, response, inputs.input_ids, outputs\n",
    "        \n",
    "    def initialize_trainer(self, buffer_size=1000):\n",
    "        \"\"\"Initialize the PPO trainer with an empty dataset\"\"\"\n",
    "        # Create a small dummy dataset to initialize the trainer\n",
    "        dummy_data = {\"prompt\": [\"\"] * 10, \"response\": [\"\"] * 10, \"reward\": [0.0] * 10}\n",
    "        from datasets import Dataset\n",
    "        dummy_dataset = Dataset.from_dict(dummy_data)\n",
    "        \n",
    "        # Initialize the PPO trainer\n",
    "        from trl import PPOTrainer\n",
    "        self.trainer = PPOTrainer(\n",
    "            config=self.ppo_config,\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            dataset=dummy_dataset\n",
    "        )\n",
    "        \n",
    "    def ppo_step(self, query_tensors, response_tensors, rewards):\n",
    "        \"\"\"Run PPO optimization step\"\"\"\n",
    "        if self.trainer is None:\n",
    "            self.initialize_trainer()\n",
    "        \n",
    "        # Ensure all inputs are properly formatted for the trainer\n",
    "        query_tensors = [t.to(self.model.device) if isinstance(t, torch.Tensor) else t for t in query_tensors]\n",
    "        response_tensors = [t.to(self.model.device) if isinstance(t, torch.Tensor) else t for t in response_tensors]\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float).to(self.model.device)\n",
    "        \n",
    "        # Run PPO step\n",
    "        stats = self.trainer.step(query_tensors, response_tensors, rewards)\n",
    "        return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Training Loop with Continuous Learning\n",
    "# =============================================================================\n",
    "\n",
    "class DailyTrainingOrchestrator:\n",
    "    \"\"\"\n",
    "    Manages daily training process with:\n",
    "    - Experience replay buffer\n",
    "    - Dynamic reward calculation\n",
    "    - Risk-aware policy updates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: DailyTradingEnv, trader: DailyTrader, output_dir=\"output/ppo_trading\"):\n",
    "        self.env = env\n",
    "        self.trader = trader\n",
    "        self.buffer = []  # Experience replay buffer\n",
    "        self.buffer_size = 1000\n",
    "        self.episode_length = 21  # Rolling 1-month episodes\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Ensure the trader's PPO trainer is initialized\n",
    "        if trader.trainer is None:\n",
    "            trader.initialize_trainer()\n",
    "\n",
    "    def collect_experience(self, num_episodes=10):\n",
    "        \"\"\"Collect trading experience by running multiple episodes\"\"\"\n",
    "        all_episode_rewards = []\n",
    "        \n",
    "        for episode in tqdm(range(num_episodes), desc=\"Collecting experience\"):\n",
    "            state = self.env.reset(random_start=True)\n",
    "            episode_states = []\n",
    "            episode_queries = []\n",
    "            episode_responses = []\n",
    "            episode_actions = []\n",
    "            episode_rewards = []\n",
    "            episode_response_texts = []\n",
    "            \n",
    "            # Run one episode\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < self.episode_length:\n",
    "                # Get action from trader\n",
    "                action_dict, response_text, query, response = self.trader.predict(state)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, info = self.env.step(action_dict['position'])\n",
    "                \n",
    "                # Store experience\n",
    "                episode_states.append(state)\n",
    "                episode_queries.append(query)\n",
    "                episode_responses.append(response)\n",
    "                episode_actions.append(action_dict)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_response_texts.append(response_text)\n",
    "                \n",
    "                # Move to next state\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            \n",
    "            # Calculate returns with discount\n",
    "            returns = self._calculate_returns(episode_rewards)\n",
    "            \n",
    "            # Store episodes in buffer\n",
    "            for i in range(len(episode_states)):\n",
    "                self.buffer.append({\n",
    "                    'query': episode_queries[i],\n",
    "                    'response': episode_responses[i],\n",
    "                    'reward': returns[i],\n",
    "                    'raw_reward': episode_rewards[i],\n",
    "                    'response_text': episode_response_texts[i],\n",
    "                    'confidence': episode_actions[i]['confidence']\n",
    "                })\n",
    "            \n",
    "            # Keep buffer size in check\n",
    "            if len(self.buffer) > self.buffer_size:\n",
    "                self.buffer = self.buffer[-self.buffer_size:]\n",
    "                \n",
    "            all_episode_rewards.extend(episode_rewards)\n",
    "        \n",
    "        # Return average reward per step\n",
    "        return np.mean(all_episode_rewards) if all_episode_rewards else 0.0\n",
    "\n",
    "    def _calculate_returns(self, rewards):\n",
    "        \"\"\"Calculate discounted returns\"\"\"\n",
    "        gamma = self.trader.ppo_config.gamma\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def train(self, num_iterations=100, num_episodes_per_iter=5, batch_size=8):\n",
    "        \"\"\"Main training loop with experience replay\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            # Collect new experience\n",
    "            avg_reward = self.collect_experience(num_episodes=num_episodes_per_iter)\n",
    "            \n",
    "            # Skip training if buffer is too small\n",
    "            if len(self.buffer) < batch_size:\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}: Buffer too small ({len(self.buffer)}), skipping training\")\n",
    "                continue\n",
    "            \n",
    "            # Run multiple PPO updates using samples from buffer\n",
    "            for _ in range(4):  # Number of PPO updates per iteration\n",
    "                # Sample from buffer\n",
    "                batch_indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), replace=False)\n",
    "                batch = [self.buffer[i] for i in batch_indices]\n",
    "                \n",
    "                # Prepare training data\n",
    "                queries = [item['query'] for item in batch]\n",
    "                responses = [item['response'] for item in batch]\n",
    "                rewards = [item['reward'] * item['confidence'] for item in batch]\n",
    "                \n",
    "                # Run PPO update\n",
    "                stats = self.trader.ppo_step(queries, responses, rewards)\n",
    "                \n",
    "                # Log some sample responses and their rewards\n",
    "                if _ == 0:\n",
    "                    sample_idx = np.random.choice(len(batch))\n",
    "                    print(f\"\\nSample response: {batch[sample_idx]['response_text'][:100]}...\")\n",
    "                    print(f\"Reward: {batch[sample_idx]['raw_reward']:.4f}, Return: {batch[sample_idx]['reward']:.4f}\")\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                save_path = f\"{self.output_dir}/checkpoint-{iteration+1}\"\n",
    "                self.trader.model.save_pretrained(save_path)\n",
    "                self.trader.tokenizer.save_pretrained(save_path)\n",
    "            \n",
    "            # Logging\n",
    "            print(f\"Iteration {iteration+1}/{num_iterations}\")\n",
    "            print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "            print(f\"Buffer Size: {len(self.buffer)}\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Execution Workflow\n",
    "# =============================================================================\n",
    "\n",
    "def run_trading_training(data_path, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\", num_iterations=50):\n",
    "    \"\"\"Main execution function for training the PPO trader\"\"\"\n",
    "    # Load and prepare data\n",
    "    data = pd.read_csv(data_path, parse_dates=['date'])\n",
    "    data['news'] = data['news'].fillna(\"No significant news\")\n",
    "    \n",
    "    # Initialize components\n",
    "    env = DailyTradingEnv(data)\n",
    "    trader = DailyTrader(model_name=model_name)\n",
    "    orchestrator = DailyTrainingOrchestrator(env, trader)\n",
    "    \n",
    "    # Start training\n",
    "    orchestrator.train(num_iterations=num_iterations, num_episodes_per_iter=5)\n",
    "    \n",
    "    return trader, env\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Example usage\n",
    "    data_path = 'sp500_daily.csv'\n",
    "    model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Use appropriate model name\n",
    "    \n",
    "    # Run training\n",
    "    trader, env = run_trading_training(data_path, model_name, num_iterations=50)\n",
    "    \n",
    "    # Save final model\n",
    "    trader.model.save_pretrained(\"output/ppo_trading/final_model\")\n",
    "    trader.tokenizer.save_pretrained(\"output/ppo_trading/final_model\")\n",
    "    \n",
    "    print(\"Training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
