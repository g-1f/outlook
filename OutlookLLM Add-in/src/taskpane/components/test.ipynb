{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from gym import spaces\n",
    "import gym\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOTrainer, PPOConfig\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TraderEnv(gym.Env):\n",
    "    def __init__(self, data, initial_balance=10000, max_steps=200):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.initial_balance = initial_balance\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.asset = 0\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(data.shape[1],), dtype=np.float32)\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.asset = 0\n",
    "        return self.data[self.current_step].astype(np.float32)\n",
    "    def step(self, action):\n",
    "        price = self.data[self.current_step, -1]\n",
    "        prev_value = self.balance + self.asset * price\n",
    "        if action == 0 and self.balance >= price:\n",
    "            shares = int(self.balance // price)\n",
    "            self.asset += shares\n",
    "            self.balance -= shares * price\n",
    "        elif action == 2 and self.asset > 0:\n",
    "            self.balance += self.asset * price\n",
    "            self.asset = 0\n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= len(self.data) - 1) or (self.current_step >= self.max_steps)\n",
    "        new_price = self.data[self.current_step, -1]\n",
    "        new_value = self.balance + self.asset * new_price\n",
    "        reward = new_value - prev_value\n",
    "        return self.data[self.current_step].astype(np.float32), reward, done, {}\n",
    "\n",
    "class PPOTrader:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\", device=\"cuda:0\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.policy_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=device)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=device)\n",
    "        self.ref_model.eval()\n",
    "        self.value_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name, torch_dtype=torch.float16, device_map=device)\n",
    "        class SimpleRewardModel(torch.nn.Module):\n",
    "            def forward(self, input_ids, attention_mask=None):\n",
    "                return torch.ones((input_ids.shape[0], 1), device=input_ids.device)\n",
    "        self.reward_model = SimpleRewardModel().to(device)\n",
    "    def generate_prompt(self, obs):\n",
    "        return f\"Financial Trading Decision:\\nMarket Data: {obs}\\nPossible Actions: [Buy, Hold, Sell]\\nDecision:\"\n",
    "    @torch.no_grad()\n",
    "    def predict(self, obs):\n",
    "        prompt = self.generate_prompt(obs)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.policy_model.generate(**inputs, max_new_tokens=16, temperature=0.7, do_sample=True)\n",
    "        t = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return t.split(\"Decision:\")[-1].strip()\n",
    "\n",
    "def map_action(a_str):\n",
    "    return {\"Buy\":0,\"Hold\":1,\"Sell\":2}.get(a_str,1)\n",
    "\n",
    "def train_with_ppotrainer(agent, env, total_epochs=3, steps_per_epoch=200):\n",
    "    config = PPOConfig(model_name=\"Qwen/Qwen2.5-0.5B-Instruct\", learning_rate=5e-5, batch_size=32, mini_batch_size=4, gradient_accumulation_steps=4, optimize_cuda_cache=True, num_train_epochs=1, kl_coef=0.05, seed=42, output_dir=\"./ppo_output\")\n",
    "    dummy_dataset = Dataset.from_list([{\"query\":\"\",\"response\":\"\",\"reward\":0.0}])\n",
    "    trainer = PPOTrainer(args=config, processing_class=agent.tokenizer, model=agent.policy_model, ref_model=agent.ref_model, reward_model=agent.reward_model, value_model=agent.value_model, train_dataset=dummy_dataset)\n",
    "    for epoch in range(total_epochs):\n",
    "        experiences = []\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in tqdm(range(steps_per_epoch), desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n",
    "            a_str = agent.predict(obs)\n",
    "            a_id = map_action(a_str)\n",
    "            nxt, rew, dn, _ = env.step(a_id)\n",
    "            total_reward += rew\n",
    "            experiences.append({\"query\":agent.generate_prompt(obs),\"response\":a_str,\"reward\":rew})\n",
    "            obs = nxt\n",
    "            if dn:\n",
    "                obs = env.reset()\n",
    "        real_ds = Dataset.from_list(experiences)\n",
    "        trainer.train_dataset = real_ds\n",
    "        trainer.train()\n",
    "        print(f\"[Epoch {epoch+1}/{total_epochs}] total reward = {total_reward:.2f}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    data = np.random.rand(1000,5).astype(np.float32)\n",
    "    data[:,-1] *= 100\n",
    "    env = TraderEnv(data)\n",
    "    agent = PPOTrader(\"Qwen/Qwen2.5-0.5B-Instruct\",\"cuda:0\")\n",
    "    train_with_ppotrainer(agent, env, 3, 200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
