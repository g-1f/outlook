{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from typing import Dict, List, Deque\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Stateful Trading Environment with Daily Rollover\n",
    "# =============================================================================\n",
    "\n",
    "class DailyTradingEnv:\n",
    "    \"\"\"\n",
    "    Daily trading environment that:\n",
    "    - Maintains rolling window of market data\n",
    "    - Tracks open positions and news history\n",
    "    - Calculates incremental rewards\n",
    "    - Manages position sizing automatically\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, initial_cash: float = 1e6):\n",
    "        self.data = data.sort_index().reset_index(drop=True)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.position = 0.0  # -1 (full short) to 1 (full long)\n",
    "        self.cash = initial_cash\n",
    "        self.current_step = 0\n",
    "        self.news_window = deque(maxlen=7)  # 7-day news memory\n",
    "        self.action_history = deque(maxlen=5)  # Last 5 actions\n",
    "        \n",
    "        # State normalization\n",
    "        self._init_normalization()\n",
    "\n",
    "    def _init_normalization(self):\n",
    "        \"\"\"Initialize rolling normalization parameters\"\"\"\n",
    "        windows = [5, 20, 60]\n",
    "        for w in windows:\n",
    "            self.data[f'vol_{w}d'] = self.data['close'].pct_change().rolling(w).std()\n",
    "            self.data[f'sma_{w}d'] = self.data['close'].rolling(w).mean()\n",
    "        \n",
    "        self.data['rsi_14'] = 100 - (100 / (1 + self.data['close'].pct_change().rolling(14).mean()))\n",
    "\n",
    "    def get_portfolio_value(self) -> float:\n",
    "        \"\"\"Current total portfolio value\"\"\"\n",
    "        return self.cash + self.position * self.data.iloc[self.current_step]['close']\n",
    "\n",
    "    def step(self, new_position: float) -> Tuple[float, Dict]:\n",
    "        \"\"\"Execute daily position adjustment\"\"\"\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            return 0.0, {'status': 'completed'}\n",
    "            \n",
    "        prev_value = self.get_portfolio_value()\n",
    "        current_price = self.data.iloc[self.current_step]['close']\n",
    "        \n",
    "        # Calculate position delta\n",
    "        position_change = new_position - self.position\n",
    "        transaction_cost = abs(position_change * current_price) * 0.001  # 0.1% friction\n",
    "        self.cash -= transaction_cost\n",
    "        \n",
    "        # Update position\n",
    "        self.position = new_position\n",
    "        \n",
    "        # Move to next day\n",
    "        self.current_step += 1\n",
    "        new_price = self.data.iloc[self.current_step]['close']\n",
    "        \n",
    "        # Calculate daily return\n",
    "        daily_return = (new_price / current_price - 1) * self.position\n",
    "        new_value = self.get_portfolio_value()\n",
    "        \n",
    "        # Update news window\n",
    "        self.news_window.append(self.data.iloc[self.current_step]['news'])\n",
    "        self.action_history.append(new_position)\n",
    "        \n",
    "        return daily_return, {\n",
    "            'value': new_value,\n",
    "            'return': daily_return,\n",
    "            'volatility': self.data.iloc[self.current_step]['vol_5d'],\n",
    "            'max_drawdown': self._calculate_drawdown(),\n",
    "            'position_change': position_change\n",
    "        }\n",
    "\n",
    "    def _calculate_drawdown(self, window=21) -> float:\n",
    "        \"\"\"Rolling maximum drawdown\"\"\"\n",
    "        values = [self.get_portfolio_value()]\n",
    "        if len(values) < window:\n",
    "            return 0.0\n",
    "        peak = max(values[-window:])\n",
    "        return (min(values[-window:]) - peak) / peak\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Context-Aware LLM Trader with Memory\n",
    "# =============================================================================\n",
    "\n",
    "class DailyTrader:\n",
    "    \"\"\"\n",
    "    LLM trader with:\n",
    "    - Rolling news context\n",
    "    - Action history memory\n",
    "    - Position-aware decision making\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen1.5-1.8B\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # GRPO configuration for daily trading\n",
    "        self.config = GRPOConfig(\n",
    "            learning_rate=1.2e-5,\n",
    "            batch_size=32,\n",
    "            mini_batch_size=8,\n",
    "            ppo_epochs=3,\n",
    "            max_grad_norm=0.5,\n",
    "            kl_coeff=0.02,\n",
    "            gamma=0.98,\n",
    "            cliprange=0.15,\n",
    "            target_kl=0.01,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        self.trainer = GRPOTrainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            config=self.config\n",
    "        )\n",
    "\n",
    "    def format_state(self, env: DailyTradingEnv) -> str:\n",
    "        \"\"\"Create daily trading prompt with context\"\"\"\n",
    "        current_data = env.data.iloc[env.current_step]\n",
    "        return f\"\"\"Daily S&P 500 Trading Decision - {current_data['date']}\n",
    "Market Context:\n",
    "- Price: {current_data['close']:.2f}\n",
    "- 5D Volatility: {current_data['vol_5d']:.2%}\n",
    "- RSI(14): {current_data['rsi_14']:.1f}\n",
    "- Position: {env.position:.2%}\n",
    "- Portfolio Value: ${env.get_portfolio_value():,.2f}\n",
    "\n",
    "Recent News:\n",
    "{'\\n'.join(env.news_window)}\n",
    "\n",
    "Previous Actions (Last 5 Days):\n",
    "{'\\n'.join([f\"Day-{i}: {pos:.2%}\" for i, pos in enumerate(reversed(env.action_history))])}\n",
    "\n",
    "Output Format (JSON):\n",
    "{{\n",
    "  \"analysis\": \"<market_analysis>\",\n",
    "  \"decision\": {{\n",
    "    \"position_target\": [-1.0 to 1.0],\n",
    "    \"rationale\": \"<risk-adjusted reasoning>\",\n",
    "    \"confidence\": [0.0-1.0]\n",
    "  }},\n",
    "  \"risk_management\": {{\n",
    "    \"stop_loss\": <optional_price>,\n",
    "    \"profit_target\": <optional_price>\n",
    "  }}\n",
    "}}\"\"\"\n",
    "\n",
    "    def parse_response(self, response: str) -> Dict:\n",
    "        \"\"\"Robust JSON parsing with error correction\"\"\"\n",
    "        try:\n",
    "            json_str = response.split(\"```json\")[1].split(\"```\")[0]\n",
    "            decision = json.loads(json_str)\n",
    "            return {\n",
    "                'position': np.clip(float(decision['decision']['position_target']), -1, 1),\n",
    "                'confidence': np.clip(float(decision['decision'].get('confidence', 0.5)), 0, 1),\n",
    "                'stop_loss': decision['risk_management'].get('stop_loss'),\n",
    "                'take_profit': decision['risk_management'].get('profit_target')\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\")\n",
    "            return {'position': 0.0, 'confidence': 0.5}\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Training Loop with Continuous Learning\n",
    "# =============================================================================\n",
    "\n",
    "class DailyTrainingOrchestrator:\n",
    "    \"\"\"\n",
    "    Manages daily training process with:\n",
    "    - Experience replay buffer\n",
    "    - Dynamic reward calculation\n",
    "    - Risk-aware policy updates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: DailyTradingEnv, trader: DailyTrader):\n",
    "        self.env = env\n",
    "        self.trader = trader\n",
    "        self.buffer = deque(maxlen=252*5)  # 5 years of daily data\n",
    "        self.episode_length = 21  # Rolling 1-month episodes\n",
    "        \n",
    "        # Reward calculation parameters\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.risk_aversion = 0.5\n",
    "\n",
    "    def _calculate_risk_adjusted_reward(self, rewards: List[float]) -> float:\n",
    "        \"\"\"Calculate discounted cumulative reward with risk penalty\"\"\"\n",
    "        cumulative = 0\n",
    "        for i, r in enumerate(reversed(rewards)):\n",
    "            cumulative = r + self.gamma * cumulative\n",
    "        return cumulative / (1 + self.risk_aversion * np.std(rewards))\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"Execute one trading month (21 days)\"\"\"\n",
    "        self.env.current_step = np.random.randint(0, len(self.env.data)-self.episode_length)\n",
    "        self.env.cash = self.env.initial_cash\n",
    "        self.env.position = 0.0\n",
    "        \n",
    "        episode_rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        \n",
    "        for _ in range(self.episode_length):\n",
    "            # Generate daily decision\n",
    "            prompt = self.trader.format_state(self.env)\n",
    "            inputs = self.trader.tokenizer(prompt, return_tensors=\"pt\").to(self.trader.device)\n",
    "            response = self.trader.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            action = self.trader.parse_response(self.trader.tokenizer.decode(response[0]))\n",
    "            \n",
    "            # Execute trade\n",
    "            reward, _ = self.env.step(action['position'])\n",
    "            \n",
    "            # Store experience\n",
    "            self.buffer.append({\n",
    "                'state': inputs.input_ids,\n",
    "                'response': response,\n",
    "                'reward': reward,\n",
    "                'confidence': action['confidence']\n",
    "            })\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            states.append(inputs)\n",
    "            actions.append(action)\n",
    "            \n",
    "            # Early termination\n",
    "            if self.env.current_step >= len(self.env.data) - 1:\n",
    "                break\n",
    "        \n",
    "        # Calculate risk-adjusted returns\n",
    "        total_reward = self._calculate_risk_adjusted_reward(episode_rewards)\n",
    "        return total_reward\n",
    "\n",
    "    def train(self, num_episodes: int = 1000):\n",
    "        \"\"\"Main training loop with experience replay\"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            # Run new episode\n",
    "            episode_reward = self.run_episode()\n",
    "            \n",
    "            # Sample from buffer\n",
    "            batch_size = min(len(self.buffer), self.trader.config.batch_size)\n",
    "            batch = np.random.choice(self.buffer, batch_size, replace=False)\n",
    "            \n",
    "            # Prepare training data\n",
    "            queries = [item['state'] for item in batch]\n",
    "            responses = [item['response'] for item in batch]\n",
    "            rewards = torch.tensor(\n",
    "                [item['reward'] * item['confidence'] for item in batch],\n",
    "                device=self.trader.device\n",
    "            )\n",
    "            \n",
    "            # Policy update\n",
    "            self.trader.trainer.step(\n",
    "                queries=queries,\n",
    "                responses=responses,\n",
    "                rewards=rewards\n",
    "            )\n",
    "            \n",
    "            # Logging\n",
    "            if (episode+1) % 50 == 0:\n",
    "                print(f\"Episode {episode+1}\")\n",
    "                print(f\"Avg Reward: {episode_reward:.2%}\")\n",
    "                print(f\"Buffer Size: {len(self.buffer)}\")\n",
    "                print(\"=\"*50)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Execution Workflow\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data\n",
    "    data = pd.read_csv('sp500_daily.csv', parse_dates=['date'])\n",
    "    data['news'] = data['news'].fillna(\"No significant news\")\n",
    "    \n",
    "    # Initialize components\n",
    "    env = DailyTradingEnv(data)\n",
    "    trader = DailyTrader()\n",
    "    orchestrator = DailyTrainingOrchestrator(env, trader)\n",
    "    \n",
    "    # Start training\n",
    "    orchestrator.train(num_episodes=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
