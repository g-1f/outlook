{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOPortfolioManager:\n",
    "    \"\"\"\n",
    "    Portfolio manager using PPO to make macro-driven investment decisions\n",
    "    Optimized for multi-GPU training with simpler, more reliable approach\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\", output_dir=\"output/ppo_portfolio_manager\"):\n",
    "        # Check available GPUs\n",
    "        self.num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Found {self.num_gpus} GPU(s)\")\n",
    "        \n",
    "        # Set device strategy\n",
    "        self.policy_device = 0  # First GPU for policy model\n",
    "        self.ref_device = 1 if self.num_gpus > 1 else 0  # Second GPU for reference model\n",
    "        \n",
    "        # Multi-GPU optimized PPO configuration without DeepSpeed\n",
    "        self.ppo_config = PPOConfig(\n",
    "            # Batch size depends on available GPUs\n",
    "            per_device_train_batch_size=4 if self.num_gpus > 1 else 2,\n",
    "            \n",
    "            # Gradient accumulation for effective larger batch sizes\n",
    "            gradient_accumulation_steps=4,\n",
    "            \n",
    "            # Enable mixed precision training\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            \n",
    "            # Memory optimizations\n",
    "            optimize_cuda_cache=True,\n",
    "            gradient_checkpointing=True,\n",
    "            \n",
    "            # Standard parameters\n",
    "            learning_rate=5e-5,\n",
    "            max_grad_norm=1.0,\n",
    "            num_train_epochs=3,\n",
    "            seed=42,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            output_dir=output_dir,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=500,\n",
    "            run_name=\"ppo_portfolio_manager\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=500,\n",
    "            \n",
    "            # PPO specific parameters\n",
    "            num_ppo_epochs=4,\n",
    "            gamma=0.99,\n",
    "            lam=0.95,\n",
    "            cliprange=0.2,\n",
    "            cliprange_value=0.2,\n",
    "            vf_coef=0.1,\n",
    "            kl_coef=0.05,\n",
    "            whiten_rewards=False,\n",
    "            temperature=0.7,\n",
    "            response_length=256,  # Reduced for memory efficiency\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load models with proper device placement\n",
    "        from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "        \n",
    "        # Select precision\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        \n",
    "        # 1. Load policy model on first GPU - simple device mapping\n",
    "        print(f\"Loading policy model on GPU {self.policy_device}\")\n",
    "        self.policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=f\"cuda:{self.policy_device}\",  # Simple device placement\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # Set generation config\n",
    "        self.policy_model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "        self.policy_model.config.use_cache = not self.ppo_config.gradient_checkpointing\n",
    "        \n",
    "        # 2. Load reference model on second GPU if available\n",
    "        if self.num_gpus > 1:\n",
    "            print(f\"Loading reference model on GPU {self.ref_device}\")\n",
    "            self.ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=f\"cuda:{self.ref_device}\",  # Use second GPU\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        else:\n",
    "            # For single GPU, create a lightweight reference model\n",
    "            print(\"Creating lightweight reference model on GPU 0\")\n",
    "            from ..models import create_reference_model\n",
    "            self.ref_model = create_reference_model(self.policy_model)\n",
    "        \n",
    "        # 3. Create a value model linked to the policy model\n",
    "        from transformers import AutoModelForCausalLMWithValueHead\n",
    "        \n",
    "        print(f\"Creating value model on GPU {self.policy_device}\")\n",
    "        self.value_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            self.policy_model,  # Use policy model backbone\n",
    "            torch_dtype=dtype,\n",
    "        )\n",
    "        \n",
    "        # 4. Create a simple reward model\n",
    "        print(f\"Creating simple reward model on GPU {self.policy_device}\")\n",
    "        \n",
    "        class SimpleRewardModel(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.reward_head = torch.nn.Linear(1, 1)\n",
    "                \n",
    "            def forward(self, input_ids, attention_mask=None):\n",
    "                # Return a simple reward\n",
    "                return torch.ones((input_ids.shape[0], 1), device=input_ids.device)\n",
    "                \n",
    "            def to(self, device):\n",
    "                self.reward_head = self.reward_head.to(device)\n",
    "                return self\n",
    "        \n",
    "        self.reward_model = SimpleRewardModel().to(f\"cuda:{self.policy_device}\")\n",
    "        \n",
    "        # Initialize trainer to None\n",
    "        self.trainer = None\n",
    "        \n",
    "        # Generation parameters\n",
    "        self.generation_kwargs = {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": True,\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "        \n",
    "        # Print memory usage information\n",
    "        print(f\"Initialized PPO Portfolio Manager with {self.num_gpus} GPUs\")\n",
    "        print(f\"GPU 0 Memory: {torch.cuda.memory_allocated(0)/1024**2:.1f}MB / {torch.cuda.get_device_properties(0).total_memory/1024**2:.1f}MB\")\n",
    "        if self.num_gpus > 1:\n",
    "            print(f\"GPU 1 Memory: {torch.cuda.memory_allocated(1)/1024**2:.1f}MB / {torch.cuda.get_device_properties(1).total_memory/1024**2:.1f}MB\")\n",
    "\n",
    "    def initialize_trainer(self, train_dataset=None, data_collator=None):\n",
    "        \"\"\"Initialize the PPO trainer with proper multi-GPU support\"\"\"\n",
    "        try:\n",
    "            # Create dummy dataset if none provided\n",
    "            if train_dataset is None:\n",
    "                # Create minimal dataset\n",
    "                dummy_text = \"Example.\"\n",
    "                dummy_encoding = self.tokenizer(dummy_text, return_tensors=\"pt\")\n",
    "                dummy_ids = dummy_encoding.input_ids[0].cpu().numpy()\n",
    "                dummy_mask = dummy_encoding.attention_mask[0].cpu().numpy()\n",
    "                \n",
    "                dummy_data = {\n",
    "                    \"input_ids\": [dummy_ids] * 2,\n",
    "                    \"attention_mask\": [dummy_mask] * 2,\n",
    "                    \"rewards\": [0.0] * 2\n",
    "                }\n",
    "                train_dataset = Dataset.from_dict(dummy_data)\n",
    "            \n",
    "            # Create data collator if needed\n",
    "            if data_collator is None:\n",
    "                from transformers import DataCollatorWithPadding\n",
    "                data_collator = DataCollatorWithPadding(\n",
    "                    self.tokenizer, \n",
    "                    pad_to_multiple_of=8\n",
    "                )\n",
    "            \n",
    "            # Free up memory\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Initialize PPOTrainer\n",
    "            self.trainer = PPOTrainer(\n",
    "                args=self.ppo_config,\n",
    "                processing_class=self.tokenizer,\n",
    "                model=self.policy_model,\n",
    "                ref_model=self.ref_model,\n",
    "                reward_model=self.reward_model,\n",
    "                train_dataset=train_dataset,\n",
    "                value_model=self.value_model,\n",
    "                data_collator=data_collator\n",
    "            )\n",
    "            \n",
    "            print(\"PPOTrainer initialized successfully!\")\n",
    "            return self.trainer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing PPOTrainer: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "            \n",
    "    def predict(self, state: Dict) -> Tuple[float, str, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generate trading decision based on current state - with multi-GPU awareness\"\"\"\n",
    "        # Format the state into a prompt\n",
    "        prompt = self.format_state(state)\n",
    "        \n",
    "        # Tokenize the input \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(f\"cuda:{self.policy_device}\")\n",
    "        \n",
    "        # Generate response on the policy model GPU\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.policy_model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    **self.generation_kwargs\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response_text = full_response[len(prompt):]\n",
    "            \n",
    "            # Extract positioning value\n",
    "            position = self.extract_positioning(full_response)\n",
    "            position = np.clip(position, -1.0, 1.0)\n",
    "            \n",
    "            # Return properly shaped tensors\n",
    "            query_tensor = inputs.input_ids\n",
    "            response_tensor = outputs\n",
    "            \n",
    "            return position, response_text, query_tensor, response_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Return safe defaults\n",
    "            return 0.0, \"\", inputs.input_ids, inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingOrchestrator:\n",
    "    \"\"\"Manages the PPO training process with simplified multi-GPU support\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MacroTradingEnv, agent: PPOPortfolioManager, output_dir=\"output/ppo_portfolio_manager\", \n",
    "                 use_sequential_training=False):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 500\n",
    "        self.episode_length = 10\n",
    "        self.output_dir = output_dir\n",
    "        self.use_sequential_training = use_sequential_training\n",
    "        \n",
    "        # Maximum sequence lengths\n",
    "        self.max_query_length = 512\n",
    "        self.max_response_length = 256\n",
    "        \n",
    "        # Determine optimal GPU allocation\n",
    "        self.num_gpus = torch.cuda.device_count()\n",
    "        # For buffer storage, use second GPU if available\n",
    "        self.buffer_device = 1 if self.num_gpus > 1 else 0\n",
    "        \n",
    "        print(f\"Training orchestrator initialized with {self.num_gpus} GPUs\")\n",
    "        if self.num_gpus > 1:\n",
    "            print(f\"Using GPU 0 for policy model and GPU 1 for reference model and buffer\")\n",
    "        else:\n",
    "            print(f\"Using single GPU for all operations\")\n",
    "    \n",
    "    def compute_format_reward(self, response_text: str) -> float:\n",
    "        \"\"\"Calculate reward for formatting according to required XML structure\"\"\"\n",
    "        has_correct_format = self.agent.check_format(response_text)\n",
    "        \n",
    "        has_macro_state = \"<macro state>\" in response_text and \"</macro state>\" in response_text\n",
    "        has_reasoning = \"<reasoning>\" in response_text and \"</reasoning>\" in response_text\n",
    "        has_positioning = \"<positioning>\" in response_text and \"</positioning>\" in response_text\n",
    "        \n",
    "        if has_correct_format:\n",
    "            return 0.5\n",
    "        elif has_macro_state and has_reasoning and has_positioning:\n",
    "            return 0.3\n",
    "        elif (has_macro_state and has_reasoning) or (has_macro_state and has_positioning) or (has_reasoning and has_positioning):\n",
    "            return 0.1\n",
    "        else:\n",
    "            return -0.2\n",
    "    \n",
    "    def print_gpu_memory_stats(self):\n",
    "        \"\"\"Print memory stats for all available GPUs\"\"\"\n",
    "        for i in range(self.num_gpus):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**2\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**2\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**2\n",
    "            print(f\"GPU {i}: {allocated:.1f}MB allocated, {reserved:.1f}MB reserved, {total:.1f}MB total ({allocated/total*100:.1f}%)\")\n",
    "    \n",
    "    def collect_experience(self, num_episodes=3):\n",
    "        \"\"\"Collect trading experience with balanced GPU usage\"\"\"\n",
    "        all_episode_rewards = []\n",
    "        \n",
    "        # Clear memory before starting collection\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        for episode in tqdm(range(num_episodes), desc=\"Collecting experience\"):\n",
    "            state = self.env.reset(random_start=True)\n",
    "            episode_queries = []\n",
    "            episode_responses = []\n",
    "            episode_rewards = []\n",
    "            episode_response_texts = []\n",
    "            episode_format_rewards = []\n",
    "            \n",
    "            # Run one episode\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < self.episode_length:\n",
    "                # Get action from agent (automatically uses policy_device)\n",
    "                position, response_text, query, response = self.agent.predict(state)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, info = self.env.step(position)\n",
    "                \n",
    "                # Calculate format reward\n",
    "                format_reward = self.compute_format_reward(response_text)\n",
    "                total_reward = reward + format_reward\n",
    "                \n",
    "                # Process tensors for efficient storage\n",
    "                # If we have multiple GPUs, store buffer data on secondary GPU\n",
    "                if self.num_gpus > 1:\n",
    "                    # Move to CPU first to avoid direct GPU-to-GPU transfer issues\n",
    "                    query = query.cpu()\n",
    "                    response = response.cpu()\n",
    "                \n",
    "                # Truncate tensors for memory efficiency\n",
    "                query = self._truncate_tensor(query, self.max_query_length)\n",
    "                response = self._truncate_tensor(response, self.max_query_length + self.max_response_length)\n",
    "                \n",
    "                # Store experience\n",
    "                episode_queries.append(query)\n",
    "                episode_responses.append(response)\n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_response_texts.append(response_text)\n",
    "                episode_format_rewards.append(format_reward)\n",
    "                \n",
    "                # Move to next state\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                \n",
    "                # Periodically clear cache\n",
    "                if step % 5 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Calculate returns with discount\n",
    "            returns = self._calculate_returns(episode_rewards)\n",
    "            \n",
    "            # Manage buffer size\n",
    "            if len(self.buffer) + len(episode_rewards) > self.buffer_size:\n",
    "                self.buffer = self.buffer[-(self.buffer_size - len(episode_rewards)):]\n",
    "            \n",
    "            # Store episodes in buffer\n",
    "            for i in range(len(episode_rewards)):\n",
    "                self.buffer.append({\n",
    "                    'query': episode_queries[i],\n",
    "                    'response': episode_responses[i],\n",
    "                    'reward': returns[i],\n",
    "                    'raw_reward': episode_rewards[i],\n",
    "                    'response_text': episode_response_texts[i],\n",
    "                    'format_reward': episode_format_rewards[i]\n",
    "                })\n",
    "                \n",
    "            all_episode_rewards.extend(episode_rewards)\n",
    "            \n",
    "            # Print GPU memory stats after each episode\n",
    "            if self.num_gpus > 1:\n",
    "                self.print_gpu_memory_stats()\n",
    "        \n",
    "        return np.mean(all_episode_rewards) if all_episode_rewards else 0.0\n",
    "    \n",
    "    def _truncate_tensor(self, tensor, max_length):\n",
    "        \"\"\"Truncate tensor to maximum length to save memory\"\"\"\n",
    "        if tensor is None:\n",
    "            return tensor\n",
    "            \n",
    "        # Handle different tensor shapes\n",
    "        if len(tensor.shape) == 1:\n",
    "            return tensor[:min(tensor.shape[0], max_length)]\n",
    "        elif len(tensor.shape) == 2:\n",
    "            return tensor[:, :min(tensor.shape[1], max_length)]\n",
    "        elif len(tensor.shape) == 3:\n",
    "            return tensor[:, :, :min(tensor.shape[2], max_length)]\n",
    "        else:\n",
    "            return tensor\n",
    "\n",
    "    def _calculate_returns(self, rewards):\n",
    "        \"\"\"Calculate discounted returns\"\"\"\n",
    "        gamma = self.agent.ppo_config.gamma\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def create_training_dataset(self, queries, responses, rewards):\n",
    "        \"\"\"Create dataset for PPO training with proper memory management\"\"\"\n",
    "        formatted_data = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "            \"rewards\": []\n",
    "        }\n",
    "        \n",
    "        # Limit examples if needed\n",
    "        max_examples = 50  # Cap for memory efficiency\n",
    "        if len(queries) > max_examples:\n",
    "            print(f\"Limiting to {max_examples} examples (from {len(queries)} available)\")\n",
    "            indices = np.random.choice(len(queries), max_examples, replace=False)\n",
    "            queries = [queries[i] for i in indices]\n",
    "            responses = [responses[i] for i in indices]\n",
    "            rewards = [rewards[i] for i in indices]\n",
    "        \n",
    "        # Process each example\n",
    "        for i in range(len(queries)):\n",
    "            try:\n",
    "                # Get individual tensors and ensure they're on CPU\n",
    "                query = queries[i].detach().cpu() if isinstance(queries[i], torch.Tensor) else queries[i]\n",
    "                response = responses[i].detach().cpu() if isinstance(responses[i], torch.Tensor) else responses[i]\n",
    "                \n",
    "                # Handle tensor shapes\n",
    "                if isinstance(query, torch.Tensor) and len(query.shape) > 1:\n",
    "                    query = query.flatten()\n",
    "                if isinstance(response, torch.Tensor) and len(response.shape) > 1:\n",
    "                    response = response.flatten()\n",
    "                \n",
    "                # Create attention mask\n",
    "                attention_mask = torch.ones_like(query, dtype=torch.long) if isinstance(query, torch.Tensor) else torch.ones(len(query), dtype=torch.long)\n",
    "                \n",
    "                # Convert to numpy for dataset creation\n",
    "                query_numpy = query.numpy() if isinstance(query, torch.Tensor) else query\n",
    "                mask_numpy = attention_mask.numpy() if isinstance(attention_mask, torch.Tensor) else attention_mask\n",
    "                \n",
    "                # Add to formatted data\n",
    "                formatted_data[\"input_ids\"].append(query_numpy)\n",
    "                formatted_data[\"attention_mask\"].append(mask_numpy)\n",
    "                \n",
    "                # Add reward\n",
    "                reward_value = float(rewards[i]) if i < len(rewards) else 0.0\n",
    "                formatted_data[\"rewards\"].append(reward_value)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {i}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Create dataset\n",
    "        if len(formatted_data[\"input_ids\"]) == 0:\n",
    "            print(\"Warning: No valid examples processed. Creating dummy data.\")\n",
    "            # Create minimal dummy data\n",
    "            dummy_data = {\"input_ids\": [[0, 1, 2]] * 2, \n",
    "                         \"attention_mask\": [[1, 1, 1]] * 2,\n",
    "                         \"rewards\": [0.0] * 2}\n",
    "            dataset = Dataset.from_dict(dummy_data)\n",
    "        else:\n",
    "            dataset = Dataset.from_dict(formatted_data)\n",
    "            \n",
    "        print(f\"Created dataset with {len(dataset)} examples\")\n",
    "        return dataset\n",
    "\n",
    "    def train(self, num_iterations=20, num_episodes_per_iter=3, batch_size=4):\n",
    "        \"\"\"Training loop with proper multi-GPU utilization\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            # Clear memory before starting a new iteration\n",
    "            torch.cuda.empty_cache()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            print(f\"\\n{'='*20} Iteration {iteration+1}/{num_iterations} {'='*20}\")\n",
    "            print(\"GPU memory status before collection:\")\n",
    "            self.print_gpu_memory_stats()\n",
    "            \n",
    "            # Collect new experience\n",
    "            avg_reward = self.collect_experience(num_episodes=num_episodes_per_iter)\n",
    "            \n",
    "            # Skip training if buffer is too small\n",
    "            if len(self.buffer) < batch_size:\n",
    "                print(f\"Buffer too small ({len(self.buffer)}), skipping training\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Clear memory before training\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # Select experiences based on strategy\n",
    "                if self.use_sequential_training:\n",
    "                    start_idx = max(0, len(self.buffer) - batch_size)\n",
    "                    batch = self.buffer[start_idx:]\n",
    "                else:\n",
    "                    batch_indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), replace=False)\n",
    "                    batch = [self.buffer[i] for i in batch_indices]\n",
    "                \n",
    "                print(f\"Using batch of size {len(batch)}\")\n",
    "                \n",
    "                # Extract components\n",
    "                queries = [item['query'] for item in batch]\n",
    "                responses = [item['response'] for item in batch]\n",
    "                rewards = [item['reward'] for item in batch]\n",
    "                \n",
    "                # Create dataset with memory management\n",
    "                train_dataset = self.create_training_dataset(queries, responses, rewards)\n",
    "                \n",
    "                # Create data collator\n",
    "                from transformers import DataCollatorWithPadding\n",
    "                data_collator = DataCollatorWithPadding(\n",
    "                    self.agent.tokenizer,\n",
    "                    pad_to_multiple_of=8\n",
    "                )\n",
    "                \n",
    "                # Free memory\n",
    "                del queries, responses\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # Print memory status before training\n",
    "                print(\"GPU memory status before training:\")\n",
    "                self.print_gpu_memory_stats()\n",
    "                \n",
    "                # Initialize trainer for this batch\n",
    "                self.agent.initialize_trainer(\n",
    "                    train_dataset=train_dataset,\n",
    "                    data_collator=data_collator\n",
    "                )\n",
    "                \n",
    "                print(\"Starting PPO training...\")\n",
    "                \n",
    "                # Shorter training run\n",
    "                original_num_train_epochs = self.agent.ppo_config.num_train_epochs\n",
    "                self.agent.ppo_config.num_train_epochs = 1\n",
    "                \n",
    "                # Run training\n",
    "                self.agent.trainer.train()\n",
    "                \n",
    "                # Restore settings\n",
    "                self.agent.ppo_config.num_train_epochs = original_num_train_epochs\n",
    "                \n",
    "                # Log sample\n",
    "                if len(batch) > 0:\n",
    "                    sample = batch[0]\n",
    "                    print(f\"\\nSample response: {sample['response_text'][:100]}...\")\n",
    "                    print(f\"Total reward: {sample['reward']:.4f}\")\n",
    "                \n",
    "                # Clean up trainer\n",
    "                del self.agent.trainer\n",
    "                self.agent.trainer = None\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "                # Clean up on error\n",
    "                if hasattr(self.agent, 'trainer') and self.agent.trainer is not None:\n",
    "                    del self.agent.trainer\n",
    "                    self.agent.trainer = None\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (iteration + 1) % 5 == 0:\n",
    "                try:\n",
    "                    save_path = f\"{self.output_dir}/checkpoint-{iteration+1}\"\n",
    "                    self.agent.policy_model.save_pretrained(save_path)\n",
    "                    self.agent.tokenizer.save_pretrained(save_path)\n",
    "                    print(f\"Model saved to {save_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving model: {e}\")\n",
    "            \n",
    "            # Logging\n",
    "            print(f\"\\nIteration {iteration+1}/{num_iterations}\")\n",
    "            print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "            print(f\"Buffer Size: {len(self.buffer)}\")\n",
    "            print(\"GPU memory status after iteration:\")\n",
    "            self.print_gpu_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
