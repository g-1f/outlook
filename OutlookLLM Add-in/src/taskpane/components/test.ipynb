{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inference service module for CMS and DSML backends.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from typing import Optional, Dict, List, Any, Literal, Callable, Union\n",
    "from loguru import logger\n",
    "\n",
    "from ..utils.request_handler import RequestHandler\n",
    "from ..utils.gemini_utils import GeminiUtils\n",
    "\n",
    "\n",
    "class InferenceService:\n",
    "    \"\"\"\n",
    "    Inference service for language model interactions.\n",
    "    \n",
    "    This class provides methods to interact with different inference backends,\n",
    "    including CMS and DSML, with support for function/tool calling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        app_id: str,\n",
    "        request_handler: RequestHandler,\n",
    "        preferences: Dict[str, Any],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the inference service.\n",
    "        \n",
    "        Args:\n",
    "            app_id: Application identifier\n",
    "            request_handler: Request handler for API calls\n",
    "            preferences: User preferences for inference\n",
    "        \"\"\"\n",
    "        self.app_id = app_id\n",
    "        self.request_handler = request_handler\n",
    "        self.preferences = preferences or {}\n",
    "        \n",
    "        # Document type mapping for wider support\n",
    "        self.doc_type_mapping = {\n",
    "            \"pdf\": \"application/pdf\", \n",
    "            \"jpg\": \"image/jpg\",\n",
    "            \"jpeg\": \"image/jpeg\",\n",
    "            \"png\": \"image/png\"\n",
    "        }\n",
    "\n",
    "    def _create_cms_headers(self, sso: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Create standard headers for CMS API requests.\n",
    "        \n",
    "        Args:\n",
    "            sso: User's SSO cookie\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of headers\n",
    "        \"\"\"\n",
    "        return {\"Cookie\": \"SSO=\" + sso, \"app-id\": self.app_id}\n",
    "\n",
    "    def _set_cms_preferences(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Set user preferences in the payload.\n",
    "        \n",
    "        Args:\n",
    "            payload: Request payload to modify\n",
    "            \n",
    "        Returns:\n",
    "            Modified payload with preferences\n",
    "        \"\"\"\n",
    "        if \"conversation\" in payload:\n",
    "            payload[\"conversation\"][\"preferences\"] = self.preferences\n",
    "        return payload\n",
    "\n",
    "    def _get_cms_payload(\n",
    "        self,\n",
    "        question: str,\n",
    "        schema: Optional[Any] = None,\n",
    "        tools: Optional[List[Union[Dict[str, Any], Callable]]] = None,\n",
    "        document_id: Optional[str] = None,\n",
    "        document_type: Optional[Literal[\"pdf\", \"jpg\", \"jpeg\", \"png\"]] = None,\n",
    "        payload: Optional[Dict[str, Any]] = None,\n",
    "        function_results: Optional[List[Dict[str, Any]]] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create the payload for CMS API requests.\n",
    "        \n",
    "        Args:\n",
    "            question: User question or prompt\n",
    "            schema: Optional response schema (Pydantic model or dict)\n",
    "            tools: Optional LLM tools (functions or definitions)\n",
    "            document_id: Optional document reference ID\n",
    "            document_type: Optional document type\n",
    "            payload: Optional base payload to modify\n",
    "            function_results: Optional results from function execution\n",
    "            \n",
    "        Returns:\n",
    "            Complete payload for the API request\n",
    "        \"\"\"\n",
    "        # Start with provided payload or create new one\n",
    "        if payload:\n",
    "            # For stream endpoints that need different structure\n",
    "            payload[\"question\"] = question\n",
    "        else:\n",
    "            # Standard structure for other endpoints\n",
    "            payload = {\"conversation\": {\"title\": f\"{self.app_id}\"}}\n",
    "            payload[\"question\"] = {\"question\": f\"{question}\"}\n",
    "            payload = self._set_cms_preferences(payload)\n",
    "\n",
    "        # Convert schema if provided\n",
    "        if schema:\n",
    "            if \"responseConfig\" not in payload[\"question\"]:\n",
    "                payload[\"question\"][\"responseConfig\"] = {}\n",
    "                \n",
    "            # Convert schema if it's a Pydantic model\n",
    "            gemini_schema = GeminiUtils.pydantic_to_schema(schema)\n",
    "            \n",
    "            payload[\"question\"][\"responseConfig\"].update({\n",
    "                \"responseType\": \"application/json\",\n",
    "                \"responseSchema\": gemini_schema,\n",
    "            })\n",
    "\n",
    "        # Convert and add tools if provided\n",
    "        if tools:\n",
    "            # Convert functions to tool definitions if needed\n",
    "            tool_definitions = []\n",
    "            for tool in tools:\n",
    "                if callable(tool):\n",
    "                    # Convert function to tool definition\n",
    "                    tool_definition = GeminiUtils.python_to_function(tool)\n",
    "                    tool_definitions.append(tool_definition)\n",
    "                elif isinstance(tool, dict):\n",
    "                    # Already a tool definition\n",
    "                    tool_definitions.append(tool)\n",
    "                else:\n",
    "                    logger.warning(f\"Ignoring unsupported tool type: {type(tool)}\")\n",
    "                    \n",
    "            if tool_definitions:\n",
    "                payload[\"question\"][\"llmTools\"] = tool_definitions\n",
    "\n",
    "        # Add document context if provided\n",
    "        if document_id and document_type:\n",
    "            if document_type not in self.doc_type_mapping:\n",
    "                logger.warning(f\"Unsupported document type: {document_type}\")\n",
    "            else:\n",
    "                payload[\"questionContext\"] = {\n",
    "                    \"type\": \"LEXDOCUMENT\",\n",
    "                    \"documentReference\": document_id,\n",
    "                    \"documentType\": self.doc_type_mapping[document_type],\n",
    "                }\n",
    "                \n",
    "        # Add function results if provided\n",
    "        if function_results:\n",
    "            # Add results to context\n",
    "            if \"Content:\" not in question:\n",
    "                # Format function results for inclusion in the question context\n",
    "                results_str = json.dumps(function_results)\n",
    "                payload[\"question\"][\"question\"] = f\"{question}\\nContent: {results_str}\"\n",
    "\n",
    "        return payload\n",
    "    \n",
    "    def _process_response(self, response: Any) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Process API response with error handling.\n",
    "        \n",
    "        Args:\n",
    "            response: API response\n",
    "            \n",
    "        Returns:\n",
    "            Processed response or None\n",
    "        \"\"\"\n",
    "        if response is None:\n",
    "            logger.error(\"Failed to get response after retries\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # If response has json method, use it\n",
    "            if hasattr(response, 'json') and callable(response.json):\n",
    "                return response.json()\n",
    "            # If response is already processed, return it\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing response: {e}\")\n",
    "            # Try to return as text if json fails\n",
    "            try:\n",
    "                if hasattr(response, 'text'):\n",
    "                    if callable(response.text):\n",
    "                        return response.text()\n",
    "                    return response.text\n",
    "            except Exception:\n",
    "                pass\n",
    "            return None\n",
    "\n",
    "    async def _process_async_response(self, response: Any) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Process async API response with error handling.\n",
    "        \n",
    "        Args:\n",
    "            response: Async API response\n",
    "            \n",
    "        Returns:\n",
    "            Processed response or None\n",
    "        \"\"\"\n",
    "        if response is None:\n",
    "            logger.error(\"Failed to get async response after retries\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Try to parse as JSON first\n",
    "            return await response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing async response as JSON: {e}\")\n",
    "            # Try to return as text if json fails\n",
    "            try:\n",
    "                return await response.text()\n",
    "            except Exception:\n",
    "                pass\n",
    "            return None\n",
    "\n",
    "    def cms_inference(\n",
    "        self,\n",
    "        question: str,\n",
    "        sso: str,\n",
    "        schema: Optional[Any] = None,\n",
    "        tools: Optional[List[Union[Dict[str, Any], Callable]]] = None,\n",
    "        document_id: Optional[str] = None,\n",
    "        document_type: Optional[Literal[\"pdf\", \"jpg\", \"jpeg\", \"png\"]] = None,\n",
    "        function_results: Optional[List[Dict[str, Any]]] = None,\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Make a CMS inference request.\n",
    "        \n",
    "        Args:\n",
    "            question: User question or prompt\n",
    "            sso: User's SSO cookie\n",
    "            schema: Optional response schema (Pydantic model or dict)\n",
    "            tools: Optional LLM tools (functions or definitions)\n",
    "            document_id: Optional document reference ID\n",
    "            document_type: Optional document type\n",
    "            function_results: Optional results from function execution\n",
    "            \n",
    "        Returns:\n",
    "            Model response\n",
    "        \"\"\"\n",
    "        path = f\"/api/{self.app_id}/conversations_with_question\"\n",
    "        headers = self._create_cms_headers(sso)\n",
    "        payload = self._get_cms_payload(\n",
    "            question=question,\n",
    "            schema=schema,\n",
    "            tools=tools,\n",
    "            document_id=document_id,\n",
    "            document_type=document_type,\n",
    "            function_results=function_results,\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"CMS Inference Payload: {payload}\")\n",
    "        response = self.request_handler.post(\n",
    "            path=path, headers=headers, payload=payload\n",
    "        )\n",
    "        logger.debug(f\"CMS Inference Response: {response}\")\n",
    "        \n",
    "        processed_response = self._process_response(response)\n",
    "        \n",
    "        # Extract answer if available\n",
    "        if isinstance(processed_response, dict) and \"answer\" in processed_response:\n",
    "            # Check if answer is JSON string\n",
    "            answer = processed_response[\"answer\"]\n",
    "            try:\n",
    "                return json.loads(answer)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                return answer\n",
    "                \n",
    "        return processed_response\n",
    "\n",
    "    async def async_cms_inference(\n",
    "        self,\n",
    "        question: str,\n",
    "        sso: str,\n",
    "        schema: Optional[Any] = None,\n",
    "        tools: Optional[List[Union[Dict[str, Any], Callable]]] = None,\n",
    "        document_id: Optional[str] = None,\n",
    "        document_type: Optional[Literal[\"pdf\", \"jpg\", \"jpeg\", \"png\"]] = None,\n",
    "        function_results: Optional[List[Dict[str, Any]]] = None,\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Make an asynchronous CMS inference request.\n",
    "        \n",
    "        Args:\n",
    "            question: User question or prompt\n",
    "            sso: User's SSO cookie\n",
    "            schema: Optional response schema (Pydantic model or dict)\n",
    "            tools: Optional LLM tools (functions or definitions)\n",
    "            document_id: Optional document reference ID\n",
    "            document_type: Optional document type\n",
    "            function_results: Optional results from function execution\n",
    "            \n",
    "        Returns:\n",
    "            Model response\n",
    "        \"\"\"\n",
    "        path = f\"/api/{self.app_id}/conversations_with_question\"\n",
    "        headers = self._create_cms_headers(sso)\n",
    "        payload = self._get_cms_payload(\n",
    "            question=question,\n",
    "            schema=schema,\n",
    "            tools=tools,\n",
    "            document_id=document_id,\n",
    "            document_type=document_type,\n",
    "            function_results=function_results,\n",
    "        )\n",
    "        \n",
    "        logger.debug(f\"Async CMS Inference Payload: {payload}\")\n",
    "        response = await self.request_handler.async_post(\n",
    "            path=path, headers=headers, payload=payload\n",
    "        )\n",
    "        logger.debug(f\"Async CMS Inference Response: {response}\")\n",
    "        \n",
    "        processed_response = await self._process_async_response(response)\n",
    "        \n",
    "        # Extract answer if available\n",
    "        if isinstance(processed_response, dict) and \"answer\" in processed_response:\n",
    "            # Check if answer is JSON string\n",
    "            answer = processed_response[\"answer\"]\n",
    "            try:\n",
    "                return json.loads(answer)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                return answer\n",
    "                \n",
    "        return processed_response\n",
    "\n",
    "    def stream_cms_inference(\n",
    "        self,\n",
    "        conversation_id: str,\n",
    "        payload: Dict[str, Any],\n",
    "        question: str,\n",
    "        sso: str,\n",
    "        schema: Optional[Any] = None,\n",
    "        tools: Optional[List[Union[Dict[str, Any], Callable]]] = None,\n",
    "        function_results: Optional[List[Dict[str, Any]]] = None,\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Make a streaming CMS inference request.\n",
    "        \n",
    "        Args:\n",
    "            conversation_id: Existing conversation ID\n",
    "            payload: Base payload with conversation details\n",
    "            question: User question or prompt\n",
    "            sso: User's SSO cookie\n",
    "            schema: Optional response schema (Pydantic model or dict)\n",
    "            tools: Optional LLM tools (functions or definitions)\n",
    "            function_results: Optional results from function execution\n",
    "            \n",
    "        Returns:\n",
    "            Model response\n",
    "        \"\"\"\n",
    "        if not conversation_id:\n",
    "            logger.error(\"Cannot stream without a valid conversation ID\")\n",
    "            return None\n",
    "            \n",
    "        path = f\"/api/{self.app_id}/conversations/{conversation_id}/questions/stream\"\n",
    "        headers = self._create_cms_headers(sso)\n",
    "        \n",
    "        # Prepare payload for streaming endpoint\n",
    "        prepared_payload = self._get_cms_payload(\n",
    "            question=question, \n",
    "            schema=schema, \n",
    "            tools=tools, \n",
    "            payload=payload,\n",
    "            function_results=function_results,\n",
    "        )\n",
    "        logger.debug(f\"Stream CMS Inference Payload: {prepared_payload}\")\n",
    "\n",
    "        response = self.request_handler.post(\n",
    "            path=path, headers=headers, payload=prepared_payload\n",
    "        )\n",
    "        logger.debug(f\"Stream CMS Inference Response: {response}\")\n",
    "        \n",
    "        processed_response = self._process_response(response)\n",
    "        \n",
    "        # Extract answer if available\n",
    "        if isinstance(processed_response, dict) and \"answer\" in processed_response:\n",
    "            # Check if answer is JSON string\n",
    "            answer = processed_response[\"answer\"]\n",
    "            try:\n",
    "                return json.loads(answer)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                return answer\n",
    "                \n",
    "        return processed_response\n",
    "\n",
    "    async def async_stream_cms_inference(\n",
    "        self,\n",
    "        conversation_id: str,\n",
    "        payload: Dict[str, Any],\n",
    "        question: str,\n",
    "        sso: str,\n",
    "        schema: Optional[Any] = None,\n",
    "        tools: Optional[List[Union[Dict[str, Any], Callable]]] = None,\n",
    "        function_results: Optional[List[Dict[str, Any]]] = None,\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Make an asynchronous streaming CMS inference request.\n",
    "        \n",
    "        Args:\n",
    "            conversation_id: Existing conversation ID\n",
    "            payload: Base payload with conversation details\n",
    "            question: User question or prompt\n",
    "            sso: User's SSO cookie\n",
    "            schema: Optional response schema (Pydantic model or dict)\n",
    "            tools: Optional LLM tools (functions or definitions)\n",
    "            function_results: Optional results from function execution\n",
    "            \n",
    "        Returns:\n",
    "            Model response\n",
    "        \"\"\"\n",
    "        if not conversation_id:\n",
    "            logger.error(\"Cannot stream without a valid conversation ID\")\n",
    "            return None\n",
    "            \n",
    "        # Note: This uses /questions endpoint not /questions/stream as seen in original code\n",
    "        path = f\"/api/{self.app_id}/conversations/{conversation_id}/questions\"\n",
    "        headers = self._create_cms_headers(sso)\n",
    "        \n",
    "        # Prepare payload for streaming endpoint\n",
    "        prepared_payload = self._get_cms_payload(\n",
    "            question=question, \n",
    "            schema=schema, \n",
    "            tools=tools, \n",
    "            payload=payload,\n",
    "            function_results=function_results,\n",
    "        )\n",
    "        logger.debug(f\"Async Stream CMS Inference Payload: {prepared_payload}\")\n",
    "\n",
    "        response = await self.request_handler.async_post(\n",
    "            path=path, headers=headers, payload=prepared_payload\n",
    "        )\n",
    "        logger.debug(f\"Async Stream CMS Inference Response: {response}\")\n",
    "        \n",
    "        processed_response = await self._process_async_response(response)\n",
    "        \n",
    "        # Extract answer if available\n",
    "        if isinstance(processed_response, dict) and \"answer\" in processed_response:\n",
    "            # Check if answer is JSON string\n",
    "            answer = processed_response[\"answer\"]\n",
    "            try:\n",
    "                return json.loads(answer)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                return answer\n",
    "                \n",
    "        return processed_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
