{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset(self, query_tensors, response_tensors, rewards):\n",
    "    \"\"\"Create a dataset for PPOTrainer with the correct format\n",
    "    \n",
    "    PPOTrainer expects a dataset with input_ids, not text fields\n",
    "    \n",
    "    Parameters:\n",
    "    - query_tensors: List of query tensors\n",
    "    - response_tensors: List of response tensors \n",
    "    - rewards: List of reward values\n",
    "    \n",
    "    Returns:\n",
    "    - Dataset object ready for PPO training\n",
    "    \"\"\"\n",
    "    # PPOTrainer expects tokenized inputs, not text\n",
    "    formatted_data = {\n",
    "        \"input_ids\": [],      # Tokenized prompts (queries)\n",
    "        \"query_ids\": [],      # Store original queries for reference\n",
    "        \"response_ids\": [],   # Tokenized responses\n",
    "        \"attention_mask\": [], # Attention masks for the input_ids\n",
    "        \"rewards\": []         # Reward values\n",
    "    }\n",
    "    \n",
    "    print(f\"Creating dataset from {len(query_tensors)} experiences\")\n",
    "    \n",
    "    # Process each example\n",
    "    for i in range(len(query_tensors)):\n",
    "        try:\n",
    "            # Get individual tensors\n",
    "            query = query_tensors[i]\n",
    "            response = response_tensors[i]\n",
    "            \n",
    "            # Handle any tensor shape by flattening to 1D if needed\n",
    "            if len(query.shape) > 1:\n",
    "                # For tensors with shape [1, 1, sequence_length] or [1, sequence_length]\n",
    "                query = query.view(-1)  # Flatten to 1D\n",
    "            \n",
    "            # Same for response tensor\n",
    "            if len(response.shape) > 1:\n",
    "                response = response.view(-1)  # Flatten to 1D\n",
    "            \n",
    "            # Create attention mask (1s for all tokens)\n",
    "            attention_mask = torch.ones_like(query, dtype=torch.long)\n",
    "            \n",
    "            # Add to formatted data\n",
    "            formatted_data[\"input_ids\"].append(query.cpu().numpy())\n",
    "            formatted_data[\"query_ids\"].append(query.cpu().numpy())\n",
    "            formatted_data[\"response_ids\"].append(response.cpu().numpy())\n",
    "            formatted_data[\"attention_mask\"].append(attention_mask.cpu().numpy())\n",
    "            \n",
    "            # Add reward\n",
    "            if i < len(rewards):\n",
    "                reward_value = float(rewards[i])\n",
    "            else:\n",
    "                reward_value = 0.0\n",
    "            formatted_data[\"rewards\"].append(reward_value)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing experience {i}: {e}\")\n",
    "            # Skip this example\n",
    "    \n",
    "    # Make sure we have at least some data\n",
    "    if len(formatted_data[\"input_ids\"]) == 0:\n",
    "        print(\"Warning: No valid examples. Creating dummy data.\")\n",
    "        # Create dummy data with proper token IDs\n",
    "        dummy_text = \"This is a dummy example.\"\n",
    "        dummy_encoding = self.tokenizer(dummy_text, return_tensors=\"pt\")\n",
    "        dummy_ids = dummy_encoding.input_ids[0].cpu().numpy()\n",
    "        dummy_mask = dummy_encoding.attention_mask[0].cpu().numpy()\n",
    "        \n",
    "        formatted_data[\"input_ids\"] = [dummy_ids] * 2\n",
    "        formatted_data[\"query_ids\"] = [dummy_ids] * 2\n",
    "        formatted_data[\"response_ids\"] = [dummy_ids] * 2\n",
    "        formatted_data[\"attention_mask\"] = [dummy_mask] * 2\n",
    "        formatted_data[\"rewards\"] = [0.0] * 2\n",
    "    \n",
    "    # Create a dataset from the formatted data\n",
    "    dataset = Dataset.from_dict(formatted_data)\n",
    "    print(f\"Created dataset with {len(dataset)} examples\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataCollator:\n",
    "    \"\"\"Custom data collator for PPO training\n",
    "    \n",
    "    Converts the dataset items into the format expected by PPOTrainer\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        # Get max length for padding\n",
    "        max_length = max(len(feature[\"input_ids\"]) for feature in features)\n",
    "        \n",
    "        # Initialize batch\n",
    "        batch = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "            \"rewards\": []\n",
    "        }\n",
    "        \n",
    "        # Process each feature\n",
    "        for feature in features:\n",
    "            # Pad input_ids\n",
    "            padded_input_ids = feature[\"input_ids\"] + [self.pad_token_id] * (max_length - len(feature[\"input_ids\"]))\n",
    "            batch[\"input_ids\"].append(padded_input_ids)\n",
    "            \n",
    "            # Pad attention_mask\n",
    "            padded_attention_mask = feature[\"attention_mask\"] + [0] * (max_length - len(feature[\"attention_mask\"]))\n",
    "            batch[\"attention_mask\"].append(padded_attention_mask)\n",
    "            \n",
    "            # Add reward\n",
    "            batch[\"rewards\"].append(feature[\"rewards\"])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        batch[\"input_ids\"] = torch.tensor(batch[\"input_ids\"], dtype=torch.long)\n",
    "        batch[\"attention_mask\"] = torch.tensor(batch[\"attention_mask\"], dtype=torch.long)\n",
    "        batch[\"rewards\"] = torch.tensor(batch[\"rewards\"], dtype=torch.float)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "class TrainingOrchestrator:\n",
    "    \"\"\"Manages the PPO training process for the portfolio manager\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MacroTradingEnv, agent: PPOPortfolioManager, output_dir=\"output/ppo_portfolio_manager\", \n",
    "                 use_sequential_training=False):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 1000\n",
    "        self.episode_length = 21  # Trading days per episode\n",
    "        self.output_dir = output_dir\n",
    "        self.use_sequential_training = use_sequential_training\n",
    "\n",
    "    def compute_format_reward(self, response_text: str) -> float:\n",
    "        \"\"\"Calculate reward for formatting according to required XML structure\"\"\"\n",
    "        # Check overall structure\n",
    "        has_correct_format = self.agent.check_format(response_text)\n",
    "        \n",
    "        # Check individual tags\n",
    "        has_macro_state = \"<macro state>\" in response_text and \"</macro state>\" in response_text\n",
    "        has_reasoning = \"<reasoning>\" in response_text and \"</reasoning>\" in response_text\n",
    "        has_positioning = \"<positioning>\" in response_text and \"</positioning>\" in response_text\n",
    "        \n",
    "        # Calculate format reward component\n",
    "        if has_correct_format:\n",
    "            return 0.5  # Full format reward\n",
    "        elif has_macro_state and has_reasoning and has_positioning:\n",
    "            return 0.3  # Tags exist but not in correct order/format\n",
    "        elif (has_macro_state and has_reasoning) or (has_macro_state and has_positioning) or (has_reasoning and has_positioning):\n",
    "            return 0.1  # Some tags exist\n",
    "        else:\n",
    "            return -0.2  # Format completely wrong\n",
    "    \n",
    "    def collect_experience(self, num_episodes=10):\n",
    "        \"\"\"Collect trading experience by running multiple episodes\"\"\"\n",
    "        all_episode_rewards = []\n",
    "        \n",
    "        for episode in tqdm(range(num_episodes), desc=\"Collecting experience\"):\n",
    "            state = self.env.reset(random_start=True)\n",
    "            episode_queries = []\n",
    "            episode_responses = []\n",
    "            episode_rewards = []\n",
    "            episode_response_texts = []\n",
    "            episode_format_rewards = []\n",
    "            \n",
    "            # Run one episode\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < self.episode_length:\n",
    "                # Get action from agent\n",
    "                position, response_text, query, response = self.agent.predict(state)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, info = self.env.step(position)\n",
    "                \n",
    "                # Calculate format reward\n",
    "                format_reward = self.compute_format_reward(response_text)\n",
    "                \n",
    "                # Combine rewards\n",
    "                total_reward = reward + format_reward\n",
    "                \n",
    "                # Store experience\n",
    "                episode_queries.append(query)\n",
    "                episode_responses.append(response)\n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_response_texts.append(response_text)\n",
    "                episode_format_rewards.append(format_reward)\n",
    "                \n",
    "                # Move to next state\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            \n",
    "            # Calculate returns with discount\n",
    "            returns = self._calculate_returns(episode_rewards)\n",
    "            \n",
    "            # Store episodes in buffer\n",
    "            for i in range(len(episode_rewards)):\n",
    "                self.buffer.append({\n",
    "                    'query': episode_queries[i],\n",
    "                    'response': episode_responses[i],\n",
    "                    'reward': returns[i],\n",
    "                    'raw_reward': episode_rewards[i],\n",
    "                    'response_text': episode_response_texts[i],\n",
    "                    'format_reward': episode_format_rewards[i]\n",
    "                })\n",
    "            \n",
    "            # Keep buffer size in check\n",
    "            if len(self.buffer) > self.buffer_size:\n",
    "                self.buffer = self.buffer[-self.buffer_size:]\n",
    "                \n",
    "            all_episode_rewards.extend(episode_rewards)\n",
    "        \n",
    "        # Return average reward per step\n",
    "        return np.mean(all_episode_rewards) if all_episode_rewards else 0.0\n",
    "\n",
    "    def _calculate_returns(self, rewards):\n",
    "        \"\"\"Calculate discounted returns\"\"\"\n",
    "        gamma = self.agent.ppo_config.gamma\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def train(self, num_iterations=100, num_episodes_per_iter=5, batch_size=8):\n",
    "        \"\"\"Main training loop with properly formatted data for PPOTrainer\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            # Collect new experience\n",
    "            avg_reward = self.collect_experience(num_episodes=num_episodes_per_iter)\n",
    "            \n",
    "            # Skip training if buffer is too small\n",
    "            if len(self.buffer) < batch_size:\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}: Buffer too small ({len(self.buffer)}), skipping training\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Select experiences for this training iteration\n",
    "                if self.use_sequential_training:\n",
    "                    # Sequential approach: Use most recent experiences\n",
    "                    start_idx = max(0, len(self.buffer) - batch_size)\n",
    "                    batch = self.buffer[start_idx:]\n",
    "                    print(f\"Using sequential batch of size {len(batch)} (from idx {start_idx})\")\n",
    "                else:\n",
    "                    # Random sampling approach (original)\n",
    "                    batch_indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), replace=False)\n",
    "                    batch = [self.buffer[i] for i in batch_indices]\n",
    "                    print(f\"Using random batch of size {len(batch)}\")\n",
    "                \n",
    "                # Extract the components\n",
    "                queries = [item['query'] for item in batch]\n",
    "                responses = [item['response'] for item in batch]\n",
    "                rewards = [item['reward'] for item in batch]\n",
    "                \n",
    "                # Create a dataset with the correct format for PPOTrainer\n",
    "                train_dataset = self.agent.create_training_dataset(queries, responses, rewards)\n",
    "                \n",
    "                # Create a custom data collator\n",
    "                data_collator = CustomDataCollator(self.agent.tokenizer)\n",
    "                \n",
    "                # Initialize a fresh PPOTrainer with this dataset and collator\n",
    "                self.agent.initialize_trainer(\n",
    "                    train_dataset=train_dataset,\n",
    "                    data_collator=data_collator\n",
    "                )\n",
    "                \n",
    "                print(\"Starting PPO training...\")\n",
    "                \n",
    "                # Set shorter training for this batch\n",
    "                original_num_train_epochs = self.agent.ppo_config.num_train_epochs\n",
    "                self.agent.ppo_config.num_train_epochs = 1  # Just do one epoch\n",
    "                \n",
    "                # Run the full PPO training process\n",
    "                self.agent.trainer.train()\n",
    "                \n",
    "                # Restore original settings\n",
    "                self.agent.ppo_config.num_train_epochs = original_num_train_epochs\n",
    "                \n",
    "                # Log sample response\n",
    "                sample_idx = 0  # Just use the first sample for logging\n",
    "                sample = batch[sample_idx]\n",
    "                print(f\"\\nSample response: {sample['response_text'][:200]}...\")\n",
    "                print(f\"Trading reward: {sample['raw_reward'] - sample['format_reward']:.4f}\")\n",
    "                print(f\"Format reward: {sample['format_reward']:.4f}\")\n",
    "                print(f\"Total return: {sample['reward']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during PPO training: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                save_path = f\"{self.output_dir}/checkpoint-{iteration+1}\"\n",
    "                try:\n",
    "                    self.agent.policy_model.save_pretrained(save_path)\n",
    "                    self.agent.tokenizer.save_pretrained(save_path)\n",
    "                    print(f\"Model saved to {save_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving model: {e}\")\n",
    "            \n",
    "            # Logging\n",
    "            print(f\"Iteration {iteration+1}/{num_iterations}\")\n",
    "            print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "            print(f\"Buffer Size: {len(self.buffer)}\")\n",
    "            print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_trainer(self, train_dataset=None, data_collator=None):\n",
    "    \"\"\"Initialize the PPO trainer with the given dataset and collator\n",
    "    \n",
    "    Parameters:\n",
    "    - train_dataset: Dataset object for training (optional)\n",
    "    - data_collator: Custom data collator (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - The initialized PPOTrainer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If no train dataset is provided, create a dummy one\n",
    "        if train_dataset is None:\n",
    "            # Create a dummy dataset with proper token IDs\n",
    "            dummy_text = \"This is a dummy example.\"\n",
    "            dummy_encoding = self.tokenizer(dummy_text, return_tensors=\"pt\")\n",
    "            dummy_ids = dummy_encoding.input_ids[0].cpu().numpy()\n",
    "            dummy_mask = dummy_encoding.attention_mask[0].cpu().numpy()\n",
    "            \n",
    "            # Create dataset with proper fields\n",
    "            dummy_data = {\n",
    "                \"input_ids\": [dummy_ids] * 2,\n",
    "                \"attention_mask\": [dummy_mask] * 2,\n",
    "                \"rewards\": [0.0] * 2\n",
    "            }\n",
    "            train_dataset = Dataset.from_dict(dummy_data)\n",
    "        \n",
    "        # If no data collator is provided, create a default one\n",
    "        if data_collator is None:\n",
    "            from transformers import DataCollatorWithPadding\n",
    "            data_collator = DataCollatorWithPadding(self.tokenizer)\n",
    "        \n",
    "        # Initialize the trainer with our dataset and collator\n",
    "        self.trainer = PPOTrainer(\n",
    "            args=self.ppo_config,\n",
    "            processing_class=self.tokenizer,\n",
    "            model=self.policy_model,\n",
    "            ref_model=self.ref_model,\n",
    "            reward_model=self.reward_model,\n",
    "            train_dataset=train_dataset,\n",
    "            value_model=self.value_model,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        \n",
    "        print(\"PPOTrainer initialized successfully!\")\n",
    "        return self.trainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing PPOTrainer: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
