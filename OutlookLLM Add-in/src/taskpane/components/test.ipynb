{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests for the LLM class.\n",
    "\"\"\"\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import Mock, AsyncMock, patch, MagicMock, call\n",
    "from typing import List, Dict, Any, Optional, Type, Tuple\n",
    "import asyncio\n",
    "\n",
    "# Import your module\n",
    "from your_module.llm import LLM\n",
    "from your_module.provider.cms import ConversationManagementService\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Fixtures\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_cms():\n",
    "    \"\"\"Create a mock CMS service.\"\"\"\n",
    "    cms = Mock(spec=ConversationManagementService)\n",
    "    cms.inference.return_value = \"Sample response\"\n",
    "    cms.inference_with_tools.return_value = \"Sample response with tools\"\n",
    "    cms.async_inference = AsyncMock(return_value=\"Async sample response\")\n",
    "    cms.async_inference_with_tools = AsyncMock(return_value=\"Async sample response with tools\")\n",
    "    cms.stream_inference.return_value = \"Streamed response\"\n",
    "    cms.async_stream_inference = AsyncMock(return_value=\"Async streamed response\")\n",
    "    cms.upload_document.return_value = \"doc123\"\n",
    "    return cms\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_dsml():\n",
    "    \"\"\"Create a mock DSML service.\"\"\"\n",
    "    dsml = Mock()\n",
    "    return dsml\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_utils():\n",
    "    \"\"\"Create a mock Utils class.\"\"\"\n",
    "    utils = Mock()\n",
    "    return utils\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_gemini_utils():\n",
    "    \"\"\"Create a mock GeminiUtils class.\"\"\"\n",
    "    gemini_utils = Mock()\n",
    "    gemini_utils.process_tools.return_value = ([\"processed_tool\"], {\"tool_name\": lambda: \"tool_result\"})\n",
    "    return gemini_utils\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def llm_instance(mock_cms):\n",
    "    \"\"\"Create an LLM instance with mocked dependencies.\"\"\"\n",
    "    return LLM(app_id=\"test-app\", env=\"dev\", cms=mock_cms)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Initialization Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def test_init_with_default_values():\n",
    "    \"\"\"Test initialization with minimal parameters.\"\"\"\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\")\n",
    "    \n",
    "    assert llm.app_id == \"test-app\"\n",
    "    assert llm.env == \"dev\"\n",
    "    assert llm.cms is None\n",
    "    assert llm.dsml is None\n",
    "    assert llm.provider == \"cms\"  # Default provider\n",
    "\n",
    "\n",
    "def test_init_with_all_values(mock_cms, mock_dsml):\n",
    "    \"\"\"Test initialization with all parameters.\"\"\"\n",
    "    llm = LLM(\n",
    "        app_id=\"test-app\",\n",
    "        env=\"prod\",\n",
    "        cms=mock_cms,\n",
    "        dsml=mock_dsml,\n",
    "        provider=\"dsml\"\n",
    "    )\n",
    "    \n",
    "    assert llm.app_id == \"test-app\"\n",
    "    assert llm.env == \"prod\"\n",
    "    assert llm.cms == mock_cms\n",
    "    assert llm.dsml == mock_dsml\n",
    "    assert llm.provider == \"dsml\"\n",
    "\n",
    "\n",
    "@patch(\"your_module.llm.Utils\")\n",
    "@patch(\"your_module.llm.GeminiUtils\")\n",
    "def test_init_sets_utils(mock_gemini_utils_class, mock_utils_class):\n",
    "    \"\"\"Test that __init__ sets up utils correctly.\"\"\"\n",
    "    mock_utils_instance = Mock()\n",
    "    mock_gemini_utils_instance = Mock()\n",
    "    mock_utils_class.return_value = mock_utils_instance\n",
    "    mock_gemini_utils_class.return_value = mock_gemini_utils_instance\n",
    "    \n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\")\n",
    "    \n",
    "    mock_utils_class.assert_called_once_with(app_id=\"test-app\", env=\"dev\")\n",
    "    mock_gemini_utils_class.assert_called_once()\n",
    "    assert llm.utils == mock_utils_instance\n",
    "    assert llm.gemini_utils == mock_gemini_utils_instance\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Factory Method Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@patch(\"your_module.llm.get_preferences\")\n",
    "def test_build_cms_classmethod(mock_get_preferences, mock_gemini_utils):\n",
    "    \"\"\"Test the build_cms classmethod.\"\"\"\n",
    "    mock_get_preferences.return_value = {\"temperature\": 0.7}\n",
    "    \n",
    "    # Mock the class to test the classmethod\n",
    "    cls_mock = Mock()\n",
    "    \n",
    "    # Create a mock CMS instance to be returned by the CMS constructor\n",
    "    cms_instance = Mock()\n",
    "    cls_mock.return_value = cms_instance\n",
    "    \n",
    "    # Call the method under test\n",
    "    result = LLM.build_cms(\n",
    "        cls_mock,\n",
    "        app_id=\"test-app\",\n",
    "        env=\"dev\",\n",
    "        model_name=\"test-model\",\n",
    "        tools=[\"tool1\", \"tool2\"],\n",
    "        temperature=0.5,\n",
    "        reasoning_effort=\"medium\",\n",
    "        log_level=\"DEBUG\",\n",
    "        custom_param=\"value\"\n",
    "    )\n",
    "    \n",
    "    # Verify preferences were retrieved\n",
    "    mock_get_preferences.assert_called_once_with(\n",
    "        inference_model_name=\"test-model\",\n",
    "        temperature=0.5,\n",
    "        reasoning_effort=\"medium\",\n",
    "        custom_param=\"value\"\n",
    "    )\n",
    "    \n",
    "    # Verify CMS was constructed correctly\n",
    "    cls_mock.assert_called_once_with(\n",
    "        app_id=\"test-app\",\n",
    "        env=\"dev\",\n",
    "        preferences={\"temperature\": 0.7},\n",
    "        tools=None,  # Should be processed_tools, but we're mocking\n",
    "        log_level=\"DEBUG\",\n",
    "        custom_param=\"value\"\n",
    "    )\n",
    "    \n",
    "    # Verify result is the CMS instance\n",
    "    assert result == cms_instance\n",
    "\n",
    "\n",
    "@patch.object(LLM, \"build_cms\")\n",
    "def test_init_classmethod(mock_build_cms, mock_gemini_utils):\n",
    "    \"\"\"Test the init classmethod.\"\"\"\n",
    "    # Setup mock CMS instance\n",
    "    mock_cms_instance = Mock()\n",
    "    mock_build_cms.return_value = mock_cms_instance\n",
    "    \n",
    "    # Setup mock LLM instance\n",
    "    mock_llm_instance = Mock()\n",
    "    cls_mock = Mock(return_value=mock_llm_instance)\n",
    "    \n",
    "    # Call the method under test\n",
    "    result = LLM.init(\n",
    "        cls_mock,\n",
    "        app_id=\"test-app\",\n",
    "        env=\"dev\",\n",
    "        model_name=\"test-model\",\n",
    "        tools=[\"tool1\", \"tool2\"],\n",
    "        provider_type=\"cms\",\n",
    "        temperature=0.5,\n",
    "        reasoning_effort=\"medium\",\n",
    "        log_level=\"DEBUG\",\n",
    "        custom_param=\"value\"\n",
    "    )\n",
    "    \n",
    "    # Verify build_cms was called correctly\n",
    "    mock_build_cms.assert_called_once_with(\n",
    "        app_id=\"test-app\",\n",
    "        env=\"dev\",\n",
    "        model_name=\"test-model\",\n",
    "        tools=[\"tool1\", \"tool2\"],\n",
    "        temperature=0.5,\n",
    "        reasoning_effort=\"medium\",\n",
    "        log_level=\"DEBUG\",\n",
    "        custom_param=\"value\"\n",
    "    )\n",
    "    \n",
    "    # Verify LLM was constructed correctly\n",
    "    cls_mock.assert_called_once_with(\n",
    "        app_id=\"test-app\",\n",
    "        env=\"dev\",\n",
    "        cms=mock_cms_instance,\n",
    "        provider_type=\"cms\"\n",
    "    )\n",
    "    \n",
    "    # Verify result is the LLM instance\n",
    "    assert result == mock_llm_instance\n",
    "\n",
    "\n",
    "@patch(\"your_module.llm.GeminiUtils\")\n",
    "@patch.object(LLM, \"build_cms\")\n",
    "def test_init_with_tools_processing(mock_build_cms, mock_gemini_utils_class, mock_gemini_utils):\n",
    "    \"\"\"Test the init classmethod with tools processing.\"\"\"\n",
    "    # Setup mock CMS instance\n",
    "    mock_cms_instance = Mock()\n",
    "    mock_build_cms.return_value = mock_cms_instance\n",
    "    \n",
    "    # Setup mock for GeminiUtils\n",
    "    mock_gemini_utils_class.return_value = mock_gemini_utils\n",
    "    mock_gemini_utils.process_tools.return_value = (\n",
    "        [\"processed_tool1\", \"processed_tool2\"],\n",
    "        {\"tool1\": lambda: \"result1\", \"tool2\": lambda: \"result2\"}\n",
    "    )\n",
    "    \n",
    "    # Setup mock LLM instance\n",
    "    mock_llm_instance = Mock()\n",
    "    cls_mock = Mock(return_value=mock_llm_instance)\n",
    "    \n",
    "    # Define test tools\n",
    "    tools = [lambda: \"tool1\", (lambda: \"tool2\", \"param_model\", \"response_model\")]\n",
    "    \n",
    "    # Call the method under test\n",
    "    result = LLM.init(\n",
    "        cls_mock,\n",
    "        app_id=\"test-app\",\n",
    "        env=\"dev\",\n",
    "        tools=tools\n",
    "    )\n",
    "    \n",
    "    # Verify tools were processed\n",
    "    mock_gemini_utils.process_tools.assert_called_once_with(tools)\n",
    "    \n",
    "    # Verify build_cms was called with processed tools\n",
    "    mock_build_cms.assert_called_once()\n",
    "    _, kwargs = mock_build_cms.call_args\n",
    "    assert kwargs[\"tools\"] == [\"processed_tool1\", \"processed_tool2\"]\n",
    "    \n",
    "    # Verify tool callables were registered\n",
    "    assert mock_cms_instance.tool_callables == {\n",
    "        \"tool1\": tools[0], \n",
    "        \"tool2\": tools[1][0]\n",
    "    }\n",
    "\n",
    "\n",
    "@patch.object(LLM, \"build_cms\")\n",
    "def test_init_dsml_provider_not_implemented(mock_build_cms):\n",
    "    \"\"\"Test init raises NotImplementedError with dsml provider.\"\"\"\n",
    "    cls_mock = Mock()\n",
    "    \n",
    "    with pytest.raises(NotImplementedError, match=\"DSML provider not yet implemented\"):\n",
    "        LLM.init(\n",
    "            cls_mock,\n",
    "            app_id=\"test-app\",\n",
    "            env=\"dev\",\n",
    "            provider_type=\"dsml\"\n",
    "        )\n",
    "\n",
    "\n",
    "@patch.object(LLM, \"build_cms\")\n",
    "def test_init_invalid_provider(mock_build_cms):\n",
    "    \"\"\"Test init raises ValueError with invalid provider.\"\"\"\n",
    "    cls_mock = Mock()\n",
    "    \n",
    "    with pytest.raises(ValueError, match=\"Unknown provider type: invalid\"):\n",
    "        LLM.init(\n",
    "            cls_mock,\n",
    "            app_id=\"test-app\",\n",
    "            env=\"dev\",\n",
    "            provider_type=\"invalid\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Provider Selection Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def test_get_provider_default(llm_instance):\n",
    "    \"\"\"Test _get_provider returns default provider.\"\"\"\n",
    "    provider = llm_instance._get_provider()\n",
    "    \n",
    "    assert provider == \"cms\"\n",
    "\n",
    "\n",
    "def test_get_provider_override(llm_instance):\n",
    "    \"\"\"Test _get_provider respects override.\"\"\"\n",
    "    # Set default to cms, but override to dsml\n",
    "    llm_instance.dsml = Mock()  # Need to set this to avoid error\n",
    "    provider = llm_instance._get_provider(\"dsml\")\n",
    "    \n",
    "    assert provider == \"dsml\"\n",
    "\n",
    "\n",
    "def test_get_provider_cms_not_initialized():\n",
    "    \"\"\"Test _get_provider raises error when CMS is not initialized.\"\"\"\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\")  # No CMS set\n",
    "    \n",
    "    with pytest.raises(ValueError, match=\"CMS client not initialized\"):\n",
    "        llm._get_provider(\"cms\")\n",
    "\n",
    "\n",
    "def test_get_provider_dsml_not_initialized():\n",
    "    \"\"\"Test _get_provider raises error when DSML is not initialized.\"\"\"\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\")  # No DSML set\n",
    "    \n",
    "    with pytest.raises(ValueError, match=\"DSML client not initialized\"):\n",
    "        llm._get_provider(\"dsml\")\n",
    "\n",
    "\n",
    "def test_get_provider_invalid():\n",
    "    \"\"\"Test _get_provider raises error for invalid provider.\"\"\"\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\")\n",
    "    \n",
    "    with pytest.raises(ValueError, match=\"Unknown provider type: invalid\"):\n",
    "        llm._get_provider(\"invalid\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# GSSSO Token Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@patch(\"your_module.llm.gs_auth\")\n",
    "def test_get_gssso_success(mock_gs_auth):\n",
    "    \"\"\"Test successful GSSSO token retrieval.\"\"\"\n",
    "    mock_gs_auth.get_gssso.return_value = \"test-token\"\n",
    "    \n",
    "    token = LLM._get_gssso()\n",
    "    \n",
    "    assert token == \"test-token\"\n",
    "    mock_gs_auth.get_gssso.assert_called_once()\n",
    "\n",
    "\n",
    "@patch(\"your_module.llm.gs_auth\")\n",
    "def test_get_gssso_failure(mock_gs_auth):\n",
    "    \"\"\"Test GSSSO token retrieval failure.\"\"\"\n",
    "    mock_gs_auth.get_gssso.side_effect = Exception(\"Auth error\")\n",
    "    \n",
    "    with pytest.raises(ValueError, match=\"Failed to get GSSSO token: Auth error\"):\n",
    "        LLM._get_gssso()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Invoke Method Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def test_invoke_with_provided_token(llm_instance, mock_cms):\n",
    "    \"\"\"Test invoke with provided token.\"\"\"\n",
    "    result = llm_instance.invoke(\n",
    "        user_input=\"Hello\",\n",
    "        schema={\"type\": \"object\"},\n",
    "        gssso=\"provided-token\",\n",
    "        document_id=\"doc123\",\n",
    "        document_type=\"pdf\"\n",
    "    )\n",
    "    \n",
    "    assert result == \"Sample response\"\n",
    "    mock_cms.inference.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"provided-token\",\n",
    "        schema={\"type\": \"object\"},\n",
    "        document_id=\"doc123\",\n",
    "        document_type=\"pdf\"\n",
    "    )\n",
    "\n",
    "\n",
    "@patch.object(LLM, \"_get_gssso\")\n",
    "def test_invoke_gets_token_if_not_provided(mock_get_gssso, llm_instance, mock_cms):\n",
    "    \"\"\"Test invoke gets token if not provided.\"\"\"\n",
    "    mock_get_gssso.return_value = \"auto-token\"\n",
    "    \n",
    "    result = llm_instance.invoke(user_input=\"Hello\")\n",
    "    \n",
    "    assert result == \"Sample response\"\n",
    "    mock_get_gssso.assert_called_once()\n",
    "    mock_cms.inference.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"auto-token\",\n",
    "        schema=None,\n",
    "        document_id=None,\n",
    "        document_type=None\n",
    "    )\n",
    "\n",
    "\n",
    "def test_invoke_with_tools(llm_instance, mock_cms):\n",
    "    \"\"\"Test invoke with tools enabled.\"\"\"\n",
    "    result = llm_instance.invoke(\n",
    "        user_input=\"Hello\",\n",
    "        gssso=\"token\",\n",
    "        use_tools=True\n",
    "    )\n",
    "    \n",
    "    assert result == \"Sample response with tools\"\n",
    "    mock_cms.inference_with_tools.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"token\",\n",
    "        schema=None,\n",
    "        document_id=None,\n",
    "        document_type=None\n",
    "    )\n",
    "\n",
    "\n",
    "@patch.object(LLM, \"_get_provider\")\n",
    "def test_invoke_with_provider_override(mock_get_provider, llm_instance, mock_cms):\n",
    "    \"\"\"Test invoke with provider override.\"\"\"\n",
    "    mock_get_provider.return_value = \"cms\"\n",
    "    \n",
    "    result = llm_instance.invoke(\n",
    "        user_input=\"Hello\",\n",
    "        gssso=\"token\",\n",
    "        provider=\"dsml\"  # Should be overridden and validated by _get_provider\n",
    "    )\n",
    "    \n",
    "    assert result == \"Sample response\"\n",
    "    mock_get_provider.assert_called_once_with(\"dsml\")\n",
    "    mock_cms.inference.assert_called_once()\n",
    "\n",
    "\n",
    "def test_invoke_with_dsml_provider():\n",
    "    \"\"\"Test invoke raises NotImplementedError with dsml provider.\"\"\"\n",
    "    # Create LLM with dsml\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\", dsml=Mock(), provider=\"dsml\")\n",
    "    \n",
    "    with pytest.raises(NotImplementedError, match=\"DSML provider not yet implemented\"):\n",
    "        llm.invoke(user_input=\"Hello\", gssso=\"token\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Async Invoke Method Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_ainvoke_with_provided_token(llm_instance, mock_cms):\n",
    "    \"\"\"Test ainvoke with provided token.\"\"\"\n",
    "    result = await llm_instance.ainvoke(\n",
    "        user_input=\"Hello\",\n",
    "        schema={\"type\": \"object\"},\n",
    "        gssso=\"provided-token\",\n",
    "        document_id=\"doc123\",\n",
    "        document_type=\"pdf\"\n",
    "    )\n",
    "    \n",
    "    assert result == \"Async sample response\"\n",
    "    mock_cms.async_inference.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"provided-token\",\n",
    "        schema={\"type\": \"object\"},\n",
    "        document_id=\"doc123\",\n",
    "        document_type=\"pdf\"\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "@patch.object(LLM, \"_get_gssso\")\n",
    "async def test_ainvoke_gets_token_if_not_provided(mock_get_gssso, llm_instance, mock_cms):\n",
    "    \"\"\"Test ainvoke gets token if not provided.\"\"\"\n",
    "    mock_get_gssso.return_value = \"auto-token\"\n",
    "    \n",
    "    result = await llm_instance.ainvoke(user_input=\"Hello\")\n",
    "    \n",
    "    assert result == \"Async sample response\"\n",
    "    mock_get_gssso.assert_called_once()\n",
    "    mock_cms.async_inference.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"auto-token\",\n",
    "        schema=None,\n",
    "        document_id=None,\n",
    "        document_type=None\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_ainvoke_with_tools(llm_instance, mock_cms):\n",
    "    \"\"\"Test ainvoke with tools enabled.\"\"\"\n",
    "    result = await llm_instance.ainvoke(\n",
    "        user_input=\"Hello\",\n",
    "        gssso=\"token\",\n",
    "        use_tools=True\n",
    "    )\n",
    "    \n",
    "    assert result == \"Async sample response with tools\"\n",
    "    mock_cms.async_inference_with_tools.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"token\",\n",
    "        schema=None,\n",
    "        document_id=None,\n",
    "        document_type=None\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "@patch.object(LLM, \"_get_provider\")\n",
    "async def test_ainvoke_with_provider_override(mock_get_provider, llm_instance, mock_cms):\n",
    "    \"\"\"Test ainvoke with provider override.\"\"\"\n",
    "    mock_get_provider.return_value = \"cms\"\n",
    "    \n",
    "    result = await llm_instance.ainvoke(\n",
    "        user_input=\"Hello\",\n",
    "        gssso=\"token\",\n",
    "        provider=\"dsml\"  # Should be overridden and validated by _get_provider\n",
    "    )\n",
    "    \n",
    "    assert result == \"Async sample response\"\n",
    "    mock_get_provider.assert_called_once_with(\"dsml\")\n",
    "    mock_cms.async_inference.assert_called_once()\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_ainvoke_with_dsml_provider():\n",
    "    \"\"\"Test ainvoke raises NotImplementedError with dsml provider.\"\"\"\n",
    "    # Create LLM with dsml\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\", dsml=Mock(), provider=\"dsml\")\n",
    "    \n",
    "    with pytest.raises(NotImplementedError, match=\"DSML provider not yet implemented\"):\n",
    "        await llm.ainvoke(user_input=\"Hello\", gssso=\"token\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Stream Method Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def test_stream(llm_instance, mock_cms):\n",
    "    \"\"\"Test stream method.\"\"\"\n",
    "    result = llm_instance.stream(\n",
    "        user_input=\"Hello\",\n",
    "        schema={\"type\": \"object\"},\n",
    "        gssso=\"token\",\n",
    "        provider=\"cms\"\n",
    "    )\n",
    "    \n",
    "    assert result == \"Streamed response\"\n",
    "    mock_cms.stream_inference.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"token\",\n",
    "        schema={\"type\": \"object\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@patch.object(LLM, \"_get_gssso\")\n",
    "def test_stream_gets_token_if_not_provided(mock_get_gssso, llm_instance, mock_cms):\n",
    "    \"\"\"Test stream gets token if not provided.\"\"\"\n",
    "    mock_get_gssso.return_value = \"auto-token\"\n",
    "    \n",
    "    result = llm_instance.stream(user_input=\"Hello\")\n",
    "    \n",
    "    assert result == \"Streamed response\"\n",
    "    mock_get_gssso.assert_called_once()\n",
    "    mock_cms.stream_inference.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"auto-token\",\n",
    "        schema=None\n",
    "    )\n",
    "\n",
    "\n",
    "def test_stream_with_dsml_provider():\n",
    "    \"\"\"Test stream raises NotImplementedError with dsml provider.\"\"\"\n",
    "    # Create LLM with dsml\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\", dsml=Mock(), provider=\"dsml\")\n",
    "    \n",
    "    with pytest.raises(NotImplementedError, match=\"DSML provider not yet implemented\"):\n",
    "        llm.stream(user_input=\"Hello\", gssso=\"token\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Async Stream Method Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_astream(llm_instance, mock_cms):\n",
    "    \"\"\"Test astream method.\"\"\"\n",
    "    result = await llm_instance.astream(\n",
    "        user_input=\"Hello\",\n",
    "        schema={\"type\": \"object\"},\n",
    "        gssso=\"token\",\n",
    "        provider=\"cms\"\n",
    "    )\n",
    "    \n",
    "    assert result == \"Async streamed response\"\n",
    "    mock_cms.async_stream_inference.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"token\",\n",
    "        schema={\"type\": \"object\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "@patch.object(LLM, \"_get_gssso\")\n",
    "async def test_astream_gets_token_if_not_provided(mock_get_gssso, llm_instance, mock_cms):\n",
    "    \"\"\"Test astream gets token if not provided.\"\"\"\n",
    "    mock_get_gssso.return_value = \"auto-token\"\n",
    "    \n",
    "    result = await llm_instance.astream(user_input=\"Hello\")\n",
    "    \n",
    "    assert result == \"Async streamed response\"\n",
    "    mock_get_gssso.assert_called_once()\n",
    "    mock_cms.async_stream_inference.assert_called_once_with(\n",
    "        question=\"Hello\",\n",
    "        gssso=\"auto-token\",\n",
    "        schema=None\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_astream_with_dsml_provider():\n",
    "    \"\"\"Test astream raises NotImplementedError with dsml provider.\"\"\"\n",
    "    # Create LLM with dsml\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\", dsml=Mock(), provider=\"dsml\")\n",
    "    \n",
    "    with pytest.raises(NotImplementedError, match=\"DSML provider not yet implemented\"):\n",
    "        await llm.astream(user_input=\"Hello\", gssso=\"token\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Batch Processing Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@patch.object(LLM, \"invoke\")\n",
    "def test_batch(mock_invoke, llm_instance):\n",
    "    \"\"\"Test batch processing.\"\"\"\n",
    "    # Setup return values for each invocation\n",
    "    mock_invoke.side_effect = [\"Result 1\", \"Result 2\", \"Result 3\"]\n",
    "    \n",
    "    result = llm_instance.batch(\n",
    "        user_inputs=[\"Input 1\", \"Input 2\", \"Input 3\"],\n",
    "        gssso=\"token\"\n",
    "    )\n",
    "    \n",
    "    assert result == [\"Result 1\", \"Result 2\", \"Result 3\"]\n",
    "    assert mock_invoke.call_count == 3\n",
    "    mock_invoke.assert_has_calls([\n",
    "        call(\"Input 1\", \"token\"),\n",
    "        call(\"Input 2\", \"token\"),\n",
    "        call(\"Input 3\", \"token\")\n",
    "    ])\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "@patch.object(LLM, \"ainvoke\")\n",
    "async def test_abatch(mock_ainvoke, llm_instance):\n",
    "    \"\"\"Test asynchronous batch processing.\"\"\"\n",
    "    # Setup return values for each invocation\n",
    "    mock_ainvoke.side_effect = [\n",
    "        asyncio.Future(),\n",
    "        asyncio.Future(),\n",
    "        asyncio.Future()\n",
    "    ]\n",
    "    mock_ainvoke.side_effect[0].set_result(\"Async Result 1\")\n",
    "    mock_ainvoke.side_effect[1].set_result(\"Async Result 2\")\n",
    "    mock_ainvoke.side_effect[2].set_result(\"Async Result 3\")\n",
    "    \n",
    "    result = await llm_instance.abatch(\n",
    "        user_inputs=[\"Input 1\", \"Input 2\", \"Input 3\"],\n",
    "        gssso=\"token\"\n",
    "    )\n",
    "    \n",
    "    assert result == [\"Async Result 1\", \"Async Result 2\", \"Async Result 3\"]\n",
    "    assert mock_ainvoke.call_count == 3\n",
    "    mock_ainvoke.assert_has_calls([\n",
    "        call(\"Input 1\", \"token\"),\n",
    "        call(\"Input 2\", \"token\"),\n",
    "        call(\"Input 3\", \"token\")\n",
    "    ])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Upload Document Tests\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def test_upload_document(llm_instance, mock_cms):\n",
    "    \"\"\"Test upload_document method.\"\"\"\n",
    "    result = llm_instance.upload_document(\n",
    "        file_path=\"/path/to/file.pdf\",\n",
    "        gssso=\"token\",\n",
    "        provider=\"cms\"\n",
    "    )\n",
    "    \n",
    "    assert result == \"doc123\"\n",
    "    mock_cms.upload_document.assert_called_once_with(\n",
    "        gssso=\"token\",\n",
    "        file_path=\"/path/to/file.pdf\"\n",
    "    )\n",
    "\n",
    "\n",
    "@patch.object(LLM, \"_get_gssso\")\n",
    "def test_upload_document_gets_token_if_not_provided(mock_get_gssso, llm_instance, mock_cms):\n",
    "    \"\"\"Test upload_document gets token if not provided.\"\"\"\n",
    "    mock_get_gssso.return_value = \"auto-token\"\n",
    "    \n",
    "    result = llm_instance.upload_document(file_path=\"/path/to/file.pdf\")\n",
    "    \n",
    "    assert result == \"doc123\"\n",
    "    mock_get_gssso.assert_called_once()\n",
    "    mock_cms.upload_document.assert_called_once_with(\n",
    "        gssso=\"auto-token\",\n",
    "        file_path=\"/path/to/file.pdf\"\n",
    "    )\n",
    "\n",
    "\n",
    "def test_upload_document_with_dsml_provider():\n",
    "    \"\"\"Test upload_document raises NotImplementedError with dsml provider.\"\"\"\n",
    "    # Create LLM with dsml\n",
    "    llm = LLM(app_id=\"test-app\", env=\"dev\", dsml=Mock(), provider=\"dsml\")\n",
    "    \n",
    "    with pytest.raises(NotImplementedError, match=\"DSML provider not yet implemented\"):\n",
    "        llm.upload_document(file_path=\"/path/to/file.pdf\", gssso=\"token\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pytest.main([\"-v\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
