{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOPortfolioManager:\n",
    "    \"\"\"\n",
    "    Portfolio manager using PPO to make macro-driven investment decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\", output_dir=\"output/ppo_portfolio_manager\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # PPO configuration with corrected parameters\n",
    "        self.ppo_config = PPOConfig(\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            gradient_accumulation_steps=1,\n",
    "            max_grad_norm=1.0,\n",
    "            optimize_cuda_cache=True,\n",
    "            num_train_epochs=3,\n",
    "            seed=42,\n",
    "            report_to=[\"tensorboard\"],  # Changed from log_with to report_to\n",
    "            output_dir=output_dir,      # Using output_dir directly\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=500,\n",
    "            run_name=\"ppo_portfolio_manager\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=500,\n",
    "            # PPO specific parameters\n",
    "            num_ppo_epochs=4,\n",
    "            gamma=0.99,\n",
    "            lam=0.95,\n",
    "            cliprange=0.2,\n",
    "            cliprange_value=0.2,\n",
    "            vf_coef=0.1,\n",
    "            kl_coef=0.05,\n",
    "            whiten_rewards=False,\n",
    "            temperature=0.7,\n",
    "            response_length=512,\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # 1. Initialize policy model\n",
    "        from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "        self.policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        # Set generation config\n",
    "        self.policy_model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "        \n",
    "        # 2. Create reference model (for KL penalty)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # 3. Create a simple reward model (PyTorch module)\n",
    "        class SimpleRewardModel(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                # Just a dummy parameter so PyTorch considers this a proper module\n",
    "                self.dummy_param = torch.nn.Parameter(torch.zeros(1))\n",
    "                \n",
    "            def forward(self, input_ids, attention_mask=None):\n",
    "                # Return a dummy reward\n",
    "                return torch.ones((input_ids.shape[0], 1), device=input_ids.device)\n",
    "                \n",
    "            def to(self, device):\n",
    "                return self\n",
    "        \n",
    "        # Create the reward model\n",
    "        self.reward_model = SimpleRewardModel()\n",
    "        \n",
    "        # 4. Create a simple value model\n",
    "        class SimpleValueModel(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.dummy_param = torch.nn.Parameter(torch.zeros(1))\n",
    "                self.base_model_prefix = 'transformer'  # Required attribute\n",
    "                \n",
    "            def forward(self, input_ids, attention_mask=None):\n",
    "                # Return dummy hidden states\n",
    "                return torch.ones((input_ids.shape[0], input_ids.shape[1], 768), device=input_ids.device)\n",
    "                \n",
    "            def score(self, hidden_states):\n",
    "                # Required method that returns a scalar value\n",
    "                return torch.ones((hidden_states.shape[0], 1), device=hidden_states.device)\n",
    "        \n",
    "        # Create value model\n",
    "        self.value_model = SimpleValueModel()\n",
    "        \n",
    "        # Initialize trainer to None\n",
    "        self.trainer = None\n",
    "        \n",
    "        # Define text generation parameters\n",
    "        self.generation_kwargs = {\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": True,\n",
    "        }\n",
    "\n",
    "    def initialize_trainer(self):\n",
    "        \"\"\"Initialize the PPO trainer with dummy dataset\"\"\"\n",
    "        # Create a simple dummy dataset\n",
    "        dummy_data = {\"prompt\": [\"\"] * 2, \"response\": [\"\"] * 2, \"reward\": [0.0] * 2}\n",
    "        dummy_dataset = Dataset.from_dict(dummy_data)\n",
    "        \n",
    "        # Initialize the PPO Trainer with all required components\n",
    "        try:\n",
    "            self.trainer = PPOTrainer(\n",
    "                args=self.ppo_config,\n",
    "                processing_class=self.tokenizer,\n",
    "                model=self.policy_model,\n",
    "                ref_model=self.ref_model,\n",
    "                reward_model=self.reward_model,\n",
    "                train_dataset=dummy_dataset,\n",
    "                value_model=self.value_model,\n",
    "            )\n",
    "            print(\"PPOTrainer initialized successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing PPOTrainer: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def format_state(self, state: Dict) -> str:\n",
    "        \"\"\"Create prompt from current state\"\"\"\n",
    "        # Format market context\n",
    "        context_str = []\n",
    "        for k, v in state['market_context'].items():\n",
    "            # Format number with appropriate precision\n",
    "            if isinstance(v, (int, float)):\n",
    "                if abs(v) < 0.01:\n",
    "                    formatted_value = f\"{v:.6f}\"\n",
    "                elif abs(v) < 1:\n",
    "                    formatted_value = f\"{v:.4f}\"\n",
    "                else:\n",
    "                    formatted_value = f\"{v:.2f}\"\n",
    "            else:\n",
    "                formatted_value = str(v)\n",
    "                \n",
    "            context_str.append(f\"{k}: {formatted_value}\")\n",
    "        \n",
    "        # Format headlines\n",
    "        headlines_str = \"\\n\".join([f\"- {h}\" for h in state['headlines']])\n",
    "        \n",
    "        # Format previous positions\n",
    "        positions_str = \", \".join([f\"{pos:.2f}\" for pos in state['action_history']])\n",
    "        \n",
    "        # Combine all context\n",
    "        prompt = f\"{SYSTEM_PROMPT.strip()}\\n\\n\"\n",
    "        prompt += \"Market Context:\\n\"\n",
    "        prompt += \", \".join(context_str) + \"\\n\\n\"\n",
    "        prompt += f\"Current Position: {state['position']:.2f}\\n\\n\"\n",
    "        prompt += \"Recent Headlines:\\n\"\n",
    "        prompt += headlines_str + \"\\n\\n\"\n",
    "        prompt += f\"Previous Positions: [{positions_str}]\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def extract_positioning(self, text: str) -> float:\n",
    "        \"\"\"Extract positioning value from XML response\"\"\"\n",
    "        try:\n",
    "            match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "            if match:\n",
    "                position_str = match.group(1).strip()\n",
    "                # Try to extract a float from the text\n",
    "                try:\n",
    "                    # First look for float patterns\n",
    "                    float_pattern = r\"[-+]?\\d*\\.\\d+|\\d+\"\n",
    "                    float_match = re.search(float_pattern, position_str)\n",
    "                    if float_match:\n",
    "                        return float(float_match.group())\n",
    "                    else:\n",
    "                        return float(position_str)\n",
    "                except ValueError:\n",
    "                    print(f\"Could not convert position to float: {position_str}\")\n",
    "                    return 0.0\n",
    "            return 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting position: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def check_format(self, text: str) -> bool:\n",
    "        \"\"\"Check if response follows the required XML format\"\"\"\n",
    "        pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "        return bool(re.search(pattern, text, re.DOTALL))\n",
    "\n",
    "    def predict(self, state: Dict) -> Tuple[float, str, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generate trading decision based on current state\"\"\"\n",
    "        prompt = self.format_state(state)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.policy_model.device)\n",
    "        \n",
    "        # Generate response - use the policy model's generate method directly\n",
    "        outputs = self.policy_model.generate(\n",
    "            inputs.input_ids,\n",
    "            **self.generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract positioning\n",
    "        position = self.extract_positioning(response)\n",
    "        position = np.clip(position, -1.0, 1.0)\n",
    "        \n",
    "        return position, response, inputs.input_ids, outputs\n",
    "\n",
    "    def ppo_step(self, query_tensors, response_tensors, rewards):\n",
    "        \"\"\"Run PPO optimization step\"\"\"\n",
    "        if self.trainer is None:\n",
    "            self.initialize_trainer()\n",
    "        \n",
    "        # Format inputs for PPO trainer\n",
    "        texts = []\n",
    "        for query, response in zip(query_tensors, response_tensors):\n",
    "            prompt_text = self.tokenizer.decode(query, skip_special_tokens=True)\n",
    "            response_text = self.tokenizer.decode(response[query.shape[0]:], skip_special_tokens=True)\n",
    "            texts.append({\n",
    "                \"prompt\": prompt_text,\n",
    "                \"response\": response_text,\n",
    "            })\n",
    "        \n",
    "        # Run PPO step with formatted texts and rewards\n",
    "        stats = self.trainer.step(texts, rewards)\n",
    "        return stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
