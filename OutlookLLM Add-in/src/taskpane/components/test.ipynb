{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import deque\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "# System prompt for the portfolio manager\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "You must respond in the following XML format:\n",
    "\n",
    "<macro state>\n",
    "...\n",
    "</macro state>\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<positioning>\n",
    "...\n",
    "</positioning>\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Trading Environment\n",
    "# =============================================================================\n",
    "\n",
    "class MacroTradingEnv:\n",
    "    \"\"\"\n",
    "    Trading environment for macro-driven portfolio management\n",
    "    Tracks market state, positions, and calculates returns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, window_size: int = 7):\n",
    "        self.data = data.copy()\n",
    "        self.position = 0.0  # -1 (full short) to 1 (full long)\n",
    "        self.current_step = 0\n",
    "        self.window_size = window_size\n",
    "        self.headline_window = deque(maxlen=window_size)\n",
    "        self.action_history = deque(maxlen=5)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['headline', 'returns']\n",
    "        for col in required_cols:\n",
    "            assert col in self.data.columns, f\"DataFrame must contain '{col}' column\"\n",
    "        \n",
    "        # Initialize action history with zeros\n",
    "        for _ in range(5):\n",
    "            self.action_history.append(0.0)\n",
    "\n",
    "    def reset(self, random_start: bool = True):\n",
    "        \"\"\"Reset environment, optionally to a random starting point\"\"\"\n",
    "        if random_start:\n",
    "            # Ensure we have enough data ahead for a full episode\n",
    "            max_start = len(self.data) - 30\n",
    "            self.current_step = random.randint(self.window_size, max_start) if max_start > self.window_size else self.window_size\n",
    "        else:\n",
    "            self.current_step = self.window_size\n",
    "        \n",
    "        # Reset state\n",
    "        self.position = 0.0\n",
    "        self.headline_window.clear()\n",
    "        self.action_history.clear()\n",
    "        \n",
    "        # Initialize headline window with past headlines\n",
    "        for i in range(self.window_size):\n",
    "            idx = self.current_step - i - 1\n",
    "            if idx >= 0:\n",
    "                self.headline_window.appendleft(self.data.iloc[idx]['headline'])\n",
    "            else:\n",
    "                self.headline_window.appendleft(\"No headline available\")\n",
    "        \n",
    "        # Initialize action history with zeros\n",
    "        for _ in range(5):\n",
    "            self.action_history.append(0.0)\n",
    "            \n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, new_position: float) -> Tuple[Dict, float, bool, Dict]:\n",
    "        \"\"\"Execute position adjustment and return (state, reward, done, info)\"\"\"\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            return self.get_state(), 0.0, True, {'status': 'completed'}\n",
    "        \n",
    "        # Calculate position change\n",
    "        position_change = new_position - self.position\n",
    "        transaction_cost = abs(position_change) * 0.001  # 0.1% friction\n",
    "        \n",
    "        # Update position\n",
    "        self.position = new_position\n",
    "        \n",
    "        # Calculate return (using pre-calculated returns from dataframe)\n",
    "        next_return = self.data.iloc[self.current_step]['returns']\n",
    "        position_return = next_return * self.position\n",
    "        \n",
    "        # Move to next day\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Update headline window and action history\n",
    "        if self.current_step < len(self.data):\n",
    "            self.headline_window.append(self.data.iloc[self.current_step]['headline'])\n",
    "        self.action_history.append(new_position)\n",
    "        \n",
    "        # Calculate reward (return minus transaction cost)\n",
    "        reward = position_return - transaction_cost\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.current_step >= len(self.data) - 1)\n",
    "        \n",
    "        info = {\n",
    "            'return': position_return,\n",
    "            'transaction_cost': transaction_cost,\n",
    "            'position_change': position_change\n",
    "        }\n",
    "        \n",
    "        return self.get_state(), reward, done, info\n",
    "\n",
    "    def get_state(self) -> Dict:\n",
    "        \"\"\"Return current environment state dictionary\"\"\"\n",
    "        if self.current_step >= len(self.data):\n",
    "            self.current_step = len(self.data) - 1\n",
    "            \n",
    "        current_row = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # Create context dictionary with all technical indicators\n",
    "        context = {}\n",
    "        for col in current_row.index:\n",
    "            # Skip specific columns\n",
    "            if col not in ['headline', 'returns', 'date']:\n",
    "                context[col] = current_row[col]\n",
    "        \n",
    "        return {\n",
    "            'market_context': context,\n",
    "            'headlines': list(self.headline_window),\n",
    "            'position': self.position,\n",
    "            'action_history': list(self.action_history)\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PPO Portfolio Manager\n",
    "# =============================================================================\n",
    "\n",
    "class PPOPortfolioManager:\n",
    "    \"\"\"\n",
    "    Portfolio manager using PPO to make macro-driven investment decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # PPO configuration\n",
    "        self.ppo_config = PPOConfig(\n",
    "            model_name=model_name,\n",
    "            learning_rate=5e-6,\n",
    "            batch_size=16,\n",
    "            mini_batch_size=4,\n",
    "            gradient_accumulation_steps=1,\n",
    "            optimize_cuda_cache=True,\n",
    "            ppo_epochs=4,\n",
    "            gamma=0.99,\n",
    "            remove_unused_columns=False,\n",
    "            target_kl=0.1,\n",
    "            seed=42,\n",
    "            log_with=\"tensorboard\",\n",
    "            max_grad_norm=0.5\n",
    "        )\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize model with value head for PPO\n",
    "        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        self.trainer = None\n",
    "        \n",
    "        # Define text generation parameters\n",
    "        self.generation_kwargs = {\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": True,\n",
    "        }\n",
    "\n",
    "    def format_state(self, state: Dict) -> str:\n",
    "        \"\"\"Create prompt from current state\"\"\"\n",
    "        # Format market context\n",
    "        context_str = []\n",
    "        for k, v in state['market_context'].items():\n",
    "            # Format number with appropriate precision\n",
    "            if isinstance(v, (int, float)):\n",
    "                if abs(v) < 0.01:\n",
    "                    formatted_value = f\"{v:.6f}\"\n",
    "                elif abs(v) < 1:\n",
    "                    formatted_value = f\"{v:.4f}\"\n",
    "                else:\n",
    "                    formatted_value = f\"{v:.2f}\"\n",
    "            else:\n",
    "                formatted_value = str(v)\n",
    "                \n",
    "            context_str.append(f\"{k}: {formatted_value}\")\n",
    "        \n",
    "        # Format headlines\n",
    "        headlines_str = \"\\n\".join([f\"- {h}\" for h in state['headlines']])\n",
    "        \n",
    "        # Format previous positions\n",
    "        positions_str = \", \".join([f\"{pos:.2f}\" for pos in state['action_history']])\n",
    "        \n",
    "        # Combine all context\n",
    "        prompt = f\"{SYSTEM_PROMPT.strip()}\\n\\n\"\n",
    "        prompt += \"Market Context:\\n\"\n",
    "        prompt += \", \".join(context_str) + \"\\n\\n\"\n",
    "        prompt += f\"Current Position: {state['position']:.2f}\\n\\n\"\n",
    "        prompt += \"Recent Headlines:\\n\"\n",
    "        prompt += headlines_str + \"\\n\\n\"\n",
    "        prompt += f\"Previous Positions: [{positions_str}]\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def extract_positioning(self, text: str) -> float:\n",
    "        \"\"\"Extract positioning value from XML response\"\"\"\n",
    "        try:\n",
    "            match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "            if match:\n",
    "                position_str = match.group(1).strip()\n",
    "                # Try to extract a float from the text\n",
    "                try:\n",
    "                    # First look for float patterns\n",
    "                    float_pattern = r\"[-+]?\\d*\\.\\d+|\\d+\"\n",
    "                    float_match = re.search(float_pattern, position_str)\n",
    "                    if float_match:\n",
    "                        return float(float_match.group())\n",
    "                    else:\n",
    "                        return float(position_str)\n",
    "                except ValueError:\n",
    "                    print(f\"Could not convert position to float: {position_str}\")\n",
    "                    return 0.0\n",
    "            return 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting position: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def check_format(self, text: str) -> bool:\n",
    "        \"\"\"Check if response follows the required XML format\"\"\"\n",
    "        pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "        return bool(re.search(pattern, text, re.DOTALL))\n",
    "\n",
    "    def predict(self, state: Dict) -> Tuple[float, str, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generate trading decision based on current state\"\"\"\n",
    "        prompt = self.format_state(state)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            **self.generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract positioning\n",
    "        position = self.extract_positioning(response)\n",
    "        position = np.clip(position, -1.0, 1.0)\n",
    "        \n",
    "        return position, response, inputs.input_ids, outputs\n",
    "\n",
    "    def initialize_trainer(self):\n",
    "        \"\"\"Initialize the PPO trainer with dummy dataset\"\"\"\n",
    "        dummy_data = {\"prompt\": [\"\"] * 2, \"response\": [\"\"] * 2, \"reward\": [0.0] * 2}\n",
    "        dummy_dataset = Dataset.from_dict(dummy_data)\n",
    "        \n",
    "        # Create a reference model (copy of the base model)\n",
    "        ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # The current PPO Trainer requires a reward model - we'll use the main model\n",
    "        # This is a simplification - in a full implementation, you might want a separate reward model\n",
    "        reward_model = self.model\n",
    "        \n",
    "        self.trainer = PPOTrainer(\n",
    "            args=self.ppo_config,\n",
    "            processing_class=self.tokenizer,\n",
    "            model=self.model,\n",
    "            ref_model=ref_model,\n",
    "            reward_model=reward_model,\n",
    "            train_dataset=dummy_dataset\n",
    "        )\n",
    "        \n",
    "    def ppo_step(self, query_tensors, response_tensors, rewards):\n",
    "        \"\"\"Run PPO optimization step\"\"\"\n",
    "        if self.trainer is None:\n",
    "            self.initialize_trainer()\n",
    "            \n",
    "        # Ensure all inputs are properly formatted\n",
    "        query_tensors = [t.to(self.model.device) if isinstance(t, torch.Tensor) else t for t in query_tensors]\n",
    "        response_tensors = [t.to(self.model.device) if isinstance(t, torch.Tensor) else t for t in response_tensors]\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float).to(self.model.device)\n",
    "        \n",
    "        # Prepare inputs in the format expected by the current PPOTrainer\n",
    "        ppo_inputs = []\n",
    "        for query, response, reward in zip(query_tensors, response_tensors, rewards):\n",
    "            # Tokenize the combined prompt+response\n",
    "            prompt_tokens = query\n",
    "            response_tokens = response[query.shape[0]:]  # Remove prompt part\n",
    "            \n",
    "            ppo_inputs.append({\n",
    "                \"prompt_input_ids\": prompt_tokens,\n",
    "                \"response\": self.tokenizer.decode(response_tokens, skip_special_tokens=True),\n",
    "                \"reward\": reward.item()\n",
    "            })\n",
    "        \n",
    "        # Run PPO step\n",
    "        stats = self.trainer.step(ppo_inputs)\n",
    "        return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Training Orchestrator\n",
    "# =============================================================================\n",
    "\n",
    "class TrainingOrchestrator:\n",
    "    \"\"\"Manages the PPO training process for the portfolio manager\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MacroTradingEnv, agent: PPOPortfolioManager, output_dir=\"output/ppo_portfolio_manager\"):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 1000\n",
    "        self.episode_length = 21  # Trading days per episode\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Ensure the agent's PPO trainer is initialized\n",
    "        if agent.trainer is None:\n",
    "            agent.initialize_trainer()\n",
    "\n",
    "    def compute_format_reward(self, response_text: str) -> float:\n",
    "        \"\"\"Calculate reward for formatting according to required XML structure\"\"\"\n",
    "        # Check overall structure\n",
    "        has_correct_format = self.agent.check_format(response_text)\n",
    "        \n",
    "        # Check individual tags\n",
    "        has_macro_state = \"<macro state>\" in response_text and \"</macro state>\" in response_text\n",
    "        has_reasoning = \"<reasoning>\" in response_text and \"</reasoning>\" in response_text\n",
    "        has_positioning = \"<positioning>\" in response_text and \"</positioning>\" in response_text\n",
    "        \n",
    "        # Calculate format reward component\n",
    "        if has_correct_format:\n",
    "            return 0.5  # Full format reward\n",
    "        elif has_macro_state and has_reasoning and has_positioning:\n",
    "            return 0.3  # Tags exist but not in correct order/format\n",
    "        elif (has_macro_state and has_reasoning) or (has_macro_state and has_positioning) or (has_reasoning and has_positioning):\n",
    "            return 0.1  # Some tags exist\n",
    "        else:\n",
    "            return -0.2  # Format completely wrong\n",
    "    \n",
    "    def collect_experience(self, num_episodes=10):\n",
    "        \"\"\"Collect trading experience by running multiple episodes\"\"\"\n",
    "        all_episode_rewards = []\n",
    "        \n",
    "        for episode in tqdm(range(num_episodes), desc=\"Collecting experience\"):\n",
    "            state = self.env.reset(random_start=True)\n",
    "            episode_queries = []\n",
    "            episode_responses = []\n",
    "            episode_rewards = []\n",
    "            episode_response_texts = []\n",
    "            episode_format_rewards = []\n",
    "            \n",
    "            # Run one episode\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < self.episode_length:\n",
    "                # Get action from agent\n",
    "                position, response_text, query, response = self.agent.predict(state)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, info = self.env.step(position)\n",
    "                \n",
    "                # Calculate format reward\n",
    "                format_reward = self.compute_format_reward(response_text)\n",
    "                \n",
    "                # Combine rewards\n",
    "                total_reward = reward + format_reward\n",
    "                \n",
    "                # Store experience\n",
    "                episode_queries.append(query)\n",
    "                episode_responses.append(response)\n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_response_texts.append(response_text)\n",
    "                episode_format_rewards.append(format_reward)\n",
    "                \n",
    "                # Move to next state\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            \n",
    "            # Calculate returns with discount\n",
    "            returns = self._calculate_returns(episode_rewards)\n",
    "            \n",
    "            # Store episodes in buffer\n",
    "            for i in range(len(episode_rewards)):\n",
    "                self.buffer.append({\n",
    "                    'query': episode_queries[i],\n",
    "                    'response': episode_responses[i],\n",
    "                    'reward': returns[i],\n",
    "                    'raw_reward': episode_rewards[i],\n",
    "                    'response_text': episode_response_texts[i],\n",
    "                    'format_reward': episode_format_rewards[i]\n",
    "                })\n",
    "            \n",
    "            # Keep buffer size in check\n",
    "            if len(self.buffer) > self.buffer_size:\n",
    "                self.buffer = self.buffer[-self.buffer_size:]\n",
    "                \n",
    "            all_episode_rewards.extend(episode_rewards)\n",
    "        \n",
    "        # Return average reward per step\n",
    "        return np.mean(all_episode_rewards) if all_episode_rewards else 0.0\n",
    "\n",
    "    def _calculate_returns(self, rewards):\n",
    "        \"\"\"Calculate discounted returns\"\"\"\n",
    "        gamma = self.agent.ppo_config.gamma\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def train(self, num_iterations=100, num_episodes_per_iter=5, batch_size=8):\n",
    "        \"\"\"Main training loop with experience replay\"\"\"\n",
    "        for iteration in range(num_iterations):\n",
    "            # Collect new experience\n",
    "            avg_reward = self.collect_experience(num_episodes=num_episodes_per_iter)\n",
    "            \n",
    "            # Skip training if buffer is too small\n",
    "            if len(self.buffer) < batch_size:\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}: Buffer too small ({len(self.buffer)}), skipping training\")\n",
    "                continue\n",
    "            \n",
    "            # Run multiple PPO updates using samples from buffer\n",
    "            for update in range(4):  # Number of PPO updates per iteration\n",
    "                # Sample from buffer\n",
    "                batch_indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), replace=False)\n",
    "                batch = [self.buffer[i] for i in batch_indices]\n",
    "                \n",
    "                # Prepare training data\n",
    "                queries = [item['query'] for item in batch]\n",
    "                responses = [item['response'] for item in batch]\n",
    "                rewards = [item['reward'] for item in batch]\n",
    "                \n",
    "                # Run PPO update\n",
    "                stats = self.agent.ppo_step(queries, responses, rewards)\n",
    "                \n",
    "                # Log sample response and reward for first update\n",
    "                if update == 0:\n",
    "                    sample_idx = np.random.choice(len(batch))\n",
    "                    sample = batch[sample_idx]\n",
    "                    print(f\"\\nSample response: {sample['response_text'][:200]}...\")\n",
    "                    print(f\"Trading reward: {sample['raw_reward'] - sample['format_reward']:.4f}\")\n",
    "                    print(f\"Format reward: {sample['format_reward']:.4f}\")\n",
    "                    print(f\"Total return: {sample['reward']:.4f}\")\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                save_path = f\"{self.output_dir}/checkpoint-{iteration+1}\"\n",
    "                self.agent.model.save_pretrained(save_path)\n",
    "                self.agent.tokenizer.save_pretrained(save_path)\n",
    "            \n",
    "            # Logging\n",
    "            print(f\"Iteration {iteration+1}/{num_iterations}\")\n",
    "            print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "            print(f\"Buffer Size: {len(self.buffer)}\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Execution Function\n",
    "# =============================================================================\n",
    "\n",
    "def train_portfolio_manager(df, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\", \n",
    "                           num_iterations=50, output_dir=\"output/ppo_portfolio_manager\"):\n",
    "    \"\"\"Main execution function for training the PPO portfolio manager\"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Initialize environment and agent\n",
    "    env = MacroTradingEnv(df)\n",
    "    agent = PPOPortfolioManager(model_name=model_name)\n",
    "    orchestrator = TrainingOrchestrator(env, agent, output_dir=output_dir)\n",
    "    \n",
    "    # Start training\n",
    "    orchestrator.train(num_iterations=num_iterations, num_episodes_per_iter=5, batch_size=8)\n",
    "    \n",
    "    # Save final model\n",
    "    agent.model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "    agent.tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
    "    \n",
    "    print(\"Training completed successfully!\")\n",
    "    return agent, env\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data (example)\n",
    "    df = pd.read_csv('market_data.csv')\n",
    "    \n",
    "    # Add 'returns' column if not present\n",
    "    if 'returns' not in df.columns:\n",
    "        df['returns'] = df['close'].pct_change().shift(-1)\n",
    "    \n",
    "    # Train portfolio manager\n",
    "    agent, env = train_portfolio_manager(df, num_iterations=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
