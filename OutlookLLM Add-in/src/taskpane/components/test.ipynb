{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grpo_trainer_no_pop.py\n",
    "\n",
    "# Copyright 2025 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Except as otherwise noted (below), this is the same code as the official GRPOTrainer\n",
    "# in the HuggingFace TRL repository, with minimal modifications to remove the \"pop\"\n",
    "# logic that was discarding the last message from the conversation.\n",
    "#\n",
    "# Modifications (search for \"MODIFICATION\"):\n",
    "# 1) The class is renamed to GRPOTrainerNoPop.\n",
    "# 2) In _generate_and_score_completions(), we removed the block that calls:\n",
    "#    `prompt.pop() if prompt[-1][\"role\"] == \"assistant\" else ...`\n",
    "#    and replaced it with a simpler approach that does not pop.\n",
    "\n",
    "import contextlib\n",
    "import functools\n",
    "import os\n",
    "import textwrap\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Optional, Sized, Union\n",
    "from unittest.mock import patch\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import transformers\n",
    "from accelerate import PartialState\n",
    "from accelerate.utils import broadcast_object_list, gather, gather_object, is_peft_model, set_seed\n",
    "from accelerate.utils.other import is_compiled_module\n",
    "from datasets import Dataset, IterableDataset\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from torch.utils.data import Sampler\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    is_wandb_available,\n",
    ")\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.utils import is_peft_available\n",
    "\n",
    "# The following imports come from TRL source:\n",
    "from trl.trainer.grpo_config import GRPOConfig\n",
    "from trl.trainer.utils import (\n",
    "    generate_model_card,\n",
    "    get_comet_experiment_url,\n",
    "    pad,\n",
    "    print_prompt_completions_sample,\n",
    "    selective_log_softmax,\n",
    ")\n",
    "from trl.models import create_reference_model, prepare_deepspeed, unwrap_model_for_generation\n",
    "from trl.trainer.callbacks import SyncRefModelCallback\n",
    "from trl.data_utils import apply_chat_template, is_conversational, maybe_apply_chat_template\n",
    "from trl.extras.profiling import profiling_context, profiling_decorator\n",
    "from trl.import_utils import is_rich_available, is_vllm_available\n",
    "\n",
    "if is_peft_available():\n",
    "    from peft import PeftConfig, get_peft_model\n",
    "\n",
    "if is_vllm_available():\n",
    "    from vllm import LLM, SamplingParams\n",
    "    from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb\n",
    "\n",
    "# RewardFunc is from the official code\n",
    "RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n",
    "\n",
    "\n",
    "class RepeatRandomSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Sampler that repeats the indices of a dataset in a structured manner.\n",
    "\n",
    "    Args:\n",
    "        data_source (`Sized`):\n",
    "            Dataset to sample from.\n",
    "        mini_repeat_count (`int`):\n",
    "            Number of times to repeat each index per batch.\n",
    "        batch_size (`int`, *optional*, defaults to `1`):\n",
    "            Number of unique indices per batch.\n",
    "        repeat_count (`int`, *optional*, defaults to `1`):\n",
    "            Number of times to repeat the full sampling process.\n",
    "        seed (`int` or `None`, *optional*, defaults to `None`):\n",
    "            Random seed for reproducibility (only affects this sampler).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: Sized,\n",
    "        mini_repeat_count: int,\n",
    "        batch_size: int = 1,\n",
    "        repeat_count: int = 1,\n",
    "        seed: Optional[int] = None,\n",
    "    ):\n",
    "        self.data_source = data_source\n",
    "        self.mini_repeat_count = mini_repeat_count\n",
    "        self.batch_size = batch_size\n",
    "        self.repeat_count = repeat_count\n",
    "        self.num_samples = len(data_source)\n",
    "        self.seed = seed\n",
    "        self.generator = torch.Generator()  # Create a local random generator\n",
    "        if seed is not None:\n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "    def __iter__(self):\n",
    "        indexes = torch.randperm(self.num_samples, generator=self.generator).tolist()\n",
    "        # chunk them by batch_size\n",
    "        indexes = [indexes[i : i + self.batch_size] for i in range(0, len(indexes), self.batch_size)]\n",
    "        # keep only chunks that match the batch_size exactly\n",
    "        indexes = [chunk for chunk in indexes if len(chunk) == self.batch_size]\n",
    "\n",
    "        for chunk in indexes:\n",
    "            for _ in range(self.repeat_count):\n",
    "                for index in chunk:\n",
    "                    for _ in range(self.mini_repeat_count):\n",
    "                        yield index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples * self.mini_repeat_count * self.repeat_count\n",
    "\n",
    "\n",
    "class GRPOTrainerNoPop(Trainer):\n",
    "    \"\"\"\n",
    "    MODIFICATION: This class is identical to the official `GRPOTrainer` except we have renamed it to\n",
    "    `GRPOTrainerNoPop` and we have removed the snippet that calls `prompt.pop()`\n",
    "    in `_generate_and_score_completions()`.\n",
    "\n",
    "    Trainer for the Group Relative Policy Optimization (GRPO) method.\n",
    "    For details, see the TRL repository. The docstring below matches official docs:\n",
    "    https://github.com/lvwerra/trl/blob/main/trl/trainer/grpo_trainer.py\n",
    "    \"\"\"\n",
    "\n",
    "    _tag_names = [\"trl\", \"grpo\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[str, PreTrainedModel],\n",
    "        reward_funcs: Union[RewardFunc, list[RewardFunc]],\n",
    "        args: Optional[GRPOConfig] = None,\n",
    "        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,\n",
    "        eval_dataset: Optional[Union[Dataset, IterableDataset, dict[str, Union[Dataset, IterableDataset]]]] = None,\n",
    "        processing_class: Optional[PreTrainedTokenizerBase] = None,\n",
    "        reward_processing_classes: Optional[Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]] = None,\n",
    "        callbacks: Optional[list[TrainerCallback]] = None,\n",
    "        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),\n",
    "        peft_config: Optional[\"PeftConfig\"] = None,\n",
    "    ):\n",
    "        # 1) same as official\n",
    "        if args is None:\n",
    "            model_name = model if isinstance(model, str) else model.config._name_or_path\n",
    "            model_name = model_name.split(\"/\")[-1]\n",
    "            args = GRPOConfig(f\"{model_name}-GRPO\")\n",
    "\n",
    "        # 2) load model\n",
    "        model_init_kwargs = args.model_init_kwargs or {}\n",
    "        if isinstance(model, str):\n",
    "            model_id = model\n",
    "            torch_dtype = model_init_kwargs.get(\"torch_dtype\")\n",
    "            if isinstance(torch_dtype, torch.dtype) or torch_dtype == \"auto\" or torch_dtype is None:\n",
    "                pass\n",
    "            elif isinstance(torch_dtype, str):\n",
    "                torch_dtype = getattr(torch, torch_dtype)\n",
    "                model_init_kwargs[\"torch_dtype\"] = torch_dtype\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid `torch_dtype` passed to `GRPOConfig`. Expected 'auto' or a valid torch.dtype string.\"\n",
    "                )\n",
    "            model_init_kwargs[\"use_cache\"] = False if args.gradient_checkpointing else model_init_kwargs.get(\"use_cache\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n",
    "        else:\n",
    "            model_id = model.config._name_or_path\n",
    "            if args.model_init_kwargs is not None:\n",
    "                raise ValueError(\n",
    "                    \"You passed `model_init_kwargs` to the `GRPOConfig`, but your model is already instantiated.\"\n",
    "                )\n",
    "\n",
    "        if peft_config is not None:\n",
    "            if not is_peft_available():\n",
    "                raise ImportError(\"PEFT is required to use `peft_config`. Run `pip install peft`.\")\n",
    "            model = get_peft_model(model, peft_config)\n",
    "\n",
    "        # 3) gradient checkpoint\n",
    "        if args.gradient_checkpointing:\n",
    "            model = self._enable_gradient_checkpointing(model, args)\n",
    "\n",
    "        # 4) reference model\n",
    "        self.beta = args.beta\n",
    "        if self.beta == 0.0:\n",
    "            self.ref_model = None\n",
    "        elif is_deepspeed_zero3_enabled():\n",
    "            self.ref_model = AutoModelForCausalLM.from_pretrained(model_id, **model_init_kwargs)\n",
    "        elif is_peft_model(model):\n",
    "            self.ref_model = None\n",
    "        else:\n",
    "            self.ref_model = create_reference_model(model)\n",
    "\n",
    "        # 5) processing_class\n",
    "        if processing_class is None:\n",
    "            processing_class = AutoTokenizer.from_pretrained(model.config._name_or_path, padding_side=\"left\")\n",
    "\n",
    "        # 6) reward funcs\n",
    "        if not isinstance(reward_funcs, list):\n",
    "            reward_funcs = [reward_funcs]\n",
    "        for i, reward_func in enumerate(reward_funcs):\n",
    "            if isinstance(reward_func, str):\n",
    "                reward_funcs[i] = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    reward_func, num_labels=1, **model_init_kwargs\n",
    "                )\n",
    "        self.reward_funcs = reward_funcs\n",
    "\n",
    "        # reward weights\n",
    "        if args.reward_weights is not None:\n",
    "            if len(args.reward_weights) != len(reward_funcs):\n",
    "                raise ValueError(\n",
    "                    \"Number of reward weights must match number of reward_funcs.\"\n",
    "                )\n",
    "            self.reward_weights = torch.tensor(args.reward_weights, dtype=torch.float32)\n",
    "        else:\n",
    "            self.reward_weights = torch.ones(len(reward_funcs), dtype=torch.float32)\n",
    "\n",
    "        # reward tokenizers\n",
    "        if reward_processing_classes is None:\n",
    "            reward_processing_classes = [None] * len(reward_funcs)\n",
    "        elif not isinstance(reward_processing_classes, list):\n",
    "            reward_processing_classes = [reward_processing_classes]\n",
    "        else:\n",
    "            if len(reward_processing_classes) != len(reward_funcs):\n",
    "                raise ValueError(\"Mismatch in length of reward_processing_classes vs reward_funcs.\")\n",
    "\n",
    "        for i, (reward_processing_class, reward_func) in enumerate(zip(reward_processing_classes, reward_funcs)):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                if reward_processing_class is None:\n",
    "                    reward_processing_class = AutoTokenizer.from_pretrained(reward_func.config._name_or_path)\n",
    "                if reward_processing_class.pad_token_id is None:\n",
    "                    reward_processing_class.pad_token = reward_processing_class.eos_token\n",
    "                reward_func.config.pad_token_id = reward_processing_class.pad_token_id\n",
    "                reward_processing_classes[i] = reward_processing_class\n",
    "        self.reward_processing_classes = reward_processing_classes\n",
    "\n",
    "        # data collator\n",
    "        def data_collator(features):\n",
    "            return features\n",
    "\n",
    "        # 7) store training args\n",
    "        self.max_prompt_length = args.max_prompt_length\n",
    "        self.max_completion_length = args.max_completion_length\n",
    "        self.num_generations = args.num_generations\n",
    "        self.use_vllm = args.use_vllm\n",
    "        self.num_iterations = args.num_iterations\n",
    "        self.epsilon = args.epsilon\n",
    "        self._step = 0\n",
    "        self._buffered_inputs = [None] * args.gradient_accumulation_steps\n",
    "\n",
    "        model.warnings_issued[\"estimate_tokens\"] = True\n",
    "\n",
    "        self._metrics = {\"train\": defaultdict(list), \"eval\": defaultdict(list)}\n",
    "        self.log_completions = args.log_completions\n",
    "\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            processing_class=processing_class,\n",
    "            callbacks=callbacks,\n",
    "            optimizers=optimizers,\n",
    "        )\n",
    "\n",
    "        # 8) checks for generation batch size divisibility\n",
    "        num_processes = self.accelerator.num_processes\n",
    "        global_batch_size = args.per_device_train_batch_size * num_processes\n",
    "        possible_values = [n for n in range(2, global_batch_size + 1) if (global_batch_size) % n == 0]\n",
    "        if self.num_generations not in possible_values:\n",
    "            raise ValueError(\n",
    "                f\"Global train batch size {global_batch_size} must be divisible by num_generations={self.num_generations}.\"\n",
    "            )\n",
    "        if self.args.eval_strategy != \"no\":\n",
    "            global_eval_bs = args.per_device_eval_batch_size * num_processes\n",
    "            possible_eval = [n for n in range(2, global_eval_bs + 1) if global_eval_bs % n == 0]\n",
    "            if self.num_generations not in possible_eval:\n",
    "                raise ValueError(\n",
    "                    f\"Global eval batch size {global_eval_bs} must be divisible by num_generations={self.num_generations}.\"\n",
    "                )\n",
    "\n",
    "        set_seed(args.seed, device_specific=True)\n",
    "\n",
    "        # 9) vLLM logic\n",
    "        if self.use_vllm:\n",
    "            if not is_vllm_available():\n",
    "                raise ImportError(\"vLLM is not installed.  pip install vllm\")\n",
    "            if self.accelerator.is_main_process:\n",
    "                vllm_device = self.args.vllm_device\n",
    "                device_type = PartialState().default_device.type\n",
    "                device_module = getattr(torch, device_type)\n",
    "                if vllm_device == \"auto\":\n",
    "                    if device_module.device_count() == 1:\n",
    "                        vllm_device = f\"{device_type}:0\"\n",
    "                    else:\n",
    "                        vllm_device = f\"{device_type}:{self.accelerator.num_processes}\"\n",
    "                if (\n",
    "                    vllm_device.split(\":\")[0] == device_type\n",
    "                    and int(vllm_device.split(\":\")[1]) >= device_module.device_count()\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"Requested vLLM device {vllm_device} not available. Possibly #devices < #processes.\"\n",
    "                    )\n",
    "                if vllm_device in {f\"{device_type}:{idx}\" for idx in range(self.accelerator.num_processes)}:\n",
    "                    warnings.warn(\n",
    "                        f\"Requested device {vllm_device} is also used for training. This can cause OOM issues.\"\n",
    "                    )\n",
    "                world_size_patch = patch(\"torch.distributed.get_world_size\", return_value=1)\n",
    "                profiling_patch = patch(\n",
    "                    \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n",
    "                    return_value=None\n",
    "                )\n",
    "                @contextlib.contextmanager\n",
    "                def new_group_context():\n",
    "                    new_group = torch.distributed.new_group\n",
    "                    try:\n",
    "                        torch.distributed.new_group = functools.partial(new_group, use_local_synchronization=True)\n",
    "                        torch.npu.mem_get_info = functools.partial(torch.npu.mem_get_info, device=vllm_device)\n",
    "                        yield\n",
    "                    finally:\n",
    "                        torch.distributed.new_group = new_group\n",
    "\n",
    "                if device_type == \"npu\":\n",
    "                    new_group_patch = new_group_context()\n",
    "                else:\n",
    "                    new_group_patch = contextlib.nullcontext()\n",
    "\n",
    "                with world_size_patch, profiling_patch, new_group_patch:\n",
    "                    self.llm = LLM(\n",
    "                        model=model.name_or_path,\n",
    "                        device=vllm_device,\n",
    "                        gpu_memory_utilization=self.args.vllm_gpu_memory_utilization,\n",
    "                        dtype=self.args.vllm_dtype,\n",
    "                        enable_prefix_caching=self.args.vllm_enable_prefix_caching,\n",
    "                        max_model_len=self.args.vllm_max_model_len,\n",
    "                    )\n",
    "\n",
    "                if args.vllm_guided_decoding_regex is not None:\n",
    "                    guided_decoding = GuidedDecodingParams(backend=\"outlines\", regex=args.vllm_guided_decoding_regex)\n",
    "                else:\n",
    "                    guided_decoding = None\n",
    "                self.sampling_params = SamplingParams(\n",
    "                    temperature=args.temperature,\n",
    "                    max_tokens=self.max_completion_length,\n",
    "                    guided_decoding=guided_decoding,\n",
    "                    n=args.num_generations,\n",
    "                )\n",
    "            self._last_loaded_step = 0\n",
    "            self.accelerator.wait_for_everyone()\n",
    "        else:\n",
    "            self.generation_config = GenerationConfig(\n",
    "                max_new_tokens=self.max_completion_length,\n",
    "                do_sample=True,\n",
    "                temperature=args.temperature,\n",
    "                pad_token_id=processing_class.pad_token_id,\n",
    "            )\n",
    "\n",
    "        self.model_accepts_loss_kwargs = False\n",
    "        self.model.add_model_tags(self._tag_names)\n",
    "\n",
    "        if self.ref_model is not None:\n",
    "            if self.is_deepspeed_enabled:\n",
    "                self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)\n",
    "            else:\n",
    "                self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)\n",
    "\n",
    "        if args.sync_ref_model:\n",
    "            self.add_callback(SyncRefModelCallback(ref_model=self.ref_model, accelerator=self.accelerator))\n",
    "\n",
    "        for i, reward_func in enumerate(self.reward_funcs):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                self.reward_funcs[i] = self.accelerator.prepare_model(reward_func, evaluation_mode=True)\n",
    "\n",
    "    def _set_signature_columns_if_needed(self):\n",
    "        # override from Trainer so we only keep \"prompt\"\n",
    "        if self._signature_columns is None:\n",
    "            self._signature_columns = [\"prompt\"]\n",
    "\n",
    "    def _get_train_sampler(self) -> Sampler:\n",
    "        effective_batch_size = (\n",
    "            self.args.per_device_train_batch_size\n",
    "            * self.accelerator.num_processes\n",
    "            * self.args.gradient_accumulation_steps\n",
    "        )\n",
    "        return RepeatRandomSampler(\n",
    "            data_source=self.train_dataset,\n",
    "            mini_repeat_count=self.num_generations,\n",
    "            batch_size=effective_batch_size // self.num_generations,\n",
    "            repeat_count=self.num_iterations,\n",
    "            seed=self.args.seed,\n",
    "        )\n",
    "\n",
    "    def _get_eval_sampler(self, eval_dataset) -> Sampler:\n",
    "        return RepeatRandomSampler(\n",
    "            data_source=eval_dataset,\n",
    "            mini_repeat_count=self.num_generations,\n",
    "            seed=self.args.seed,\n",
    "        )\n",
    "\n",
    "    def _enable_gradient_checkpointing(self, model: PreTrainedModel, args: GRPOConfig) -> PreTrainedModel:\n",
    "        model.config.use_cache = False\n",
    "        if is_peft_model(model):\n",
    "            model.base_model.gradient_checkpointing_enable()\n",
    "        else:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs or {}\n",
    "        use_reentrant = (\n",
    "            \"use_reentrant\" not in gradient_checkpointing_kwargs or gradient_checkpointing_kwargs[\"use_reentrant\"]\n",
    "        )\n",
    "        if use_reentrant:\n",
    "            model.enable_input_require_grads()\n",
    "        return model\n",
    "\n",
    "    @profiling_decorator\n",
    "    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):\n",
    "        # identical to official code\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits\n",
    "        logits = logits[:, :-1, :]\n",
    "        input_ids = input_ids[:, -logits_to_keep:]\n",
    "        logits = logits[:, -logits_to_keep:]\n",
    "        return selective_log_softmax(logits, input_ids)\n",
    "\n",
    "    @profiling_decorator\n",
    "    def _move_model_to_vllm(self):\n",
    "        with unwrap_model_for_generation(\n",
    "            self.model, self.accelerator, gather_deepspeed3_params=self.args.ds3_gather_for_generation\n",
    "        ) as unwrapped_model:\n",
    "            if is_compiled_module(unwrapped_model):\n",
    "                unwrapped_model = unwrapped_model._orig_mod\n",
    "            if is_peft_model(unwrapped_model):\n",
    "                unwrapped_model.merge_adapter()\n",
    "                state_dict = unwrapped_model.state_dict()\n",
    "                # Remove some prefixes:\n",
    "                state_dict = {\n",
    "                    k.removeprefix(\"base_model.model.\").replace(\".base_layer\", \"\"): v for k, v in state_dict.items()\n",
    "                }\n",
    "                state_dict = {k: v for k, v in state_dict.items() if unwrapped_model.prefix not in k}\n",
    "                state_dict = {\n",
    "                    k.replace(\"modules_to_save.default.\", \"\"): v\n",
    "                    for k, v in state_dict.items()\n",
    "                    if \"original_module\" not in k\n",
    "                }\n",
    "            else:\n",
    "                state_dict = unwrapped_model.state_dict()\n",
    "\n",
    "            if self.accelerator.is_main_process:\n",
    "                llm_model = self.llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "                llm_model.load_weights(state_dict.items())\n",
    "\n",
    "            if is_peft_model(unwrapped_model):\n",
    "                unwrapped_model.unmerge_adapter()\n",
    "\n",
    "    @profiling_decorator\n",
    "    def _prepare_inputs(self, inputs: dict[str, Union[torch.Tensor, Any]]) -> dict[str, Union[torch.Tensor, Any]]:\n",
    "        mode = \"eval\" if self.control.should_evaluate else \"train\"\n",
    "        if mode == \"train\":\n",
    "            if self.state.global_step % self.num_iterations == 0:\n",
    "                inputs = self._generate_and_score_completions(inputs)\n",
    "                self._buffered_inputs[self._step % self.args.gradient_accumulation_steps] = inputs\n",
    "            else:\n",
    "                inputs = self._buffered_inputs[self._step % self.args.gradient_accumulation_steps]\n",
    "            self._step += 1\n",
    "        else:\n",
    "            inputs = self._generate_and_score_completions(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def _generate_and_score_completions(\n",
    "        self, inputs: dict[str, Union[torch.Tensor, Any]]\n",
    "    ) -> dict[str, Union[torch.Tensor, Any]]:\n",
    "        \"\"\"\n",
    "        MODIFICATION: The logic is identical to official GRPOTrainer, EXCEPT\n",
    "        we do NOT do 'prompt.pop()' when building completions for is_conversational() data.\n",
    "        \"\"\"\n",
    "\n",
    "        device = self.accelerator.device\n",
    "        prompts = [x[\"prompt\"] for x in inputs]\n",
    "\n",
    "        # same logic from official code:\n",
    "        prompts_text = [maybe_apply_chat_template(example, self.processing_class)[\"prompt\"] for example in inputs]\n",
    "        prompt_inputs = super(GRPOTrainerNoPop, self)._prepare_inputs(\n",
    "            self.processing_class(\n",
    "                prompts_text, return_tensors=\"pt\", padding=True, padding_side=\"left\", add_special_tokens=False\n",
    "            )\n",
    "        )\n",
    "        prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"], prompt_inputs[\"attention_mask\"]\n",
    "\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n",
    "            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n",
    "\n",
    "        # Generate completions:\n",
    "        if self.args.use_vllm:\n",
    "            # Official code that calls self._move_model_to_vllm() and so on:\n",
    "            if self.state.global_step != getattr(self, \"_last_loaded_step\", 0):\n",
    "                self._move_model_to_vllm()\n",
    "                self._last_loaded_step = self.state.global_step\n",
    "\n",
    "            all_prompts_text = gather_object(prompts_text)\n",
    "            if self.accelerator.is_main_process:\n",
    "                ordered_set_of_prompts = list(dict.fromkeys(all_prompts_text))\n",
    "                with profiling_context(self, \"vLLM.generate\"):\n",
    "                    all_outputs = self.llm.generate(\n",
    "                        ordered_set_of_prompts, sampling_params=self.sampling_params, use_tqdm=False\n",
    "                    )\n",
    "                completion_ids = []\n",
    "                for outputs in all_outputs:\n",
    "                    for output in outputs.outputs:\n",
    "                        completion_ids.append(output.token_ids)\n",
    "            else:\n",
    "                completion_ids = [None] * len(all_prompts_text)\n",
    "            completion_ids = broadcast_object_list(completion_ids, from_process=0)\n",
    "            process_slice = slice(\n",
    "                self.accelerator.process_index * len(prompts),\n",
    "                (self.accelerator.process_index + 1) * len(prompts),\n",
    "            )\n",
    "            completion_ids = completion_ids[process_slice]\n",
    "\n",
    "            completion_ids = [torch.tensor(ids, device=device) for ids in completion_ids]\n",
    "            completion_ids = pad(completion_ids, padding_value=self.processing_class.pad_token_id)\n",
    "            prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "\n",
    "        else:\n",
    "            with unwrap_model_for_generation(self.model, self.accelerator) as unwrapped_model:\n",
    "                prompt_completion_ids = unwrapped_model.generate(\n",
    "                    prompt_ids, attention_mask=prompt_mask, generation_config=self.generation_config\n",
    "                )\n",
    "\n",
    "        # separate prompt & completions\n",
    "        prompt_length = prompt_ids.size(1)\n",
    "        prompt_ids = prompt_completion_ids[:, :prompt_length]\n",
    "        completion_ids = prompt_completion_ids[:, prompt_length:]\n",
    "\n",
    "        # Build a completion_mask up to first EOS\n",
    "        is_eos = completion_ids == self.processing_class.eos_token_id\n",
    "        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)\n",
    "        eos_rows = is_eos.any(dim=1)\n",
    "        eos_idx[eos_rows] = is_eos.int().argmax(dim=1)[eos_rows]\n",
    "        seq_idx = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)\n",
    "        completion_mask = (seq_idx <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "        old_per_token_logps = None\n",
    "        if self.num_iterations > 1:\n",
    "            with torch.inference_mode():\n",
    "                old_per_token_logps = self._get_per_token_logps(\n",
    "                    self.model, prompt_completion_ids, attention_mask, logits_to_keep\n",
    "                )\n",
    "\n",
    "        ref_per_token_logps = None\n",
    "        if self.beta != 0.0:\n",
    "            if self.ref_model is not None:\n",
    "                with torch.inference_mode():\n",
    "                    ref_per_token_logps = self._get_per_token_logps(\n",
    "                        self.ref_model, prompt_completion_ids, attention_mask, logits_to_keep\n",
    "                    )\n",
    "            else:\n",
    "                # peft\n",
    "                with torch.inference_mode(), self.accelerator.unwrap_model(self.model).disable_adapter():\n",
    "                    ref_per_token_logps = self._get_per_token_logps(\n",
    "                        self.model, prompt_completion_ids, attention_mask, logits_to_keep\n",
    "                    )\n",
    "\n",
    "        # decode completions\n",
    "        completions_text = self.processing_class.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "\n",
    "        # MODIFICATION: remove the \"pop\" logic\n",
    "        # Official code used:\n",
    "        #   if is_conversational(inputs[0]):\n",
    "        #       completions = []\n",
    "        #       for prompt, completion in zip(prompts, completions_text):\n",
    "        #           bootstrap = prompt.pop()[\"content\"] if prompt[-1][\"role\"] == \"assistant\" else \"\"\n",
    "        #           completions.append([{\"role\": \"assistant\", \"content\": bootstrap + completion}])\n",
    "        #\n",
    "        # Instead, we do NOT pop anything:\n",
    "        if is_conversational(inputs[0]):\n",
    "            completions = []\n",
    "            for (_, completion_str) in zip(prompts, completions_text):\n",
    "                completions.append([{\"role\": \"assistant\", \"content\": completion_str}])\n",
    "        else:\n",
    "            completions = completions_text\n",
    "\n",
    "        # Evaluate all rewards\n",
    "        rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)\n",
    "        for i, (reward_func, reward_processing_class) in enumerate(zip(self.reward_funcs, self.reward_processing_classes)):\n",
    "            if isinstance(reward_func, nn.Module):\n",
    "                reward_func_name = f\"reward {reward_func.config._name_or_path.split('/')[-1]}\"\n",
    "            else:\n",
    "                reward_func_name = reward_func.__name__\n",
    "\n",
    "            with profiling_context(self, reward_func_name):\n",
    "                if isinstance(reward_func, nn.Module):\n",
    "                    # Possibly build a text for each prompt+completion\n",
    "                    if is_conversational(inputs[0]):\n",
    "                        messages = [{\"messages\": p + c} for p, c in zip(prompts, completions)]\n",
    "                        texts = [apply_chat_template(x, reward_processing_class)[\"text\"] for x in messages]\n",
    "                    else:\n",
    "                        texts = [p + c for p, c in zip(prompts, completions)]\n",
    "                    reward_inputs = super(GRPOTrainerNoPop, self)._prepare_inputs(\n",
    "                        reward_processing_class(\n",
    "                            texts, return_tensors=\"pt\", padding=True, padding_side=\"right\", add_special_tokens=False\n",
    "                        )\n",
    "                    )\n",
    "                    with torch.inference_mode():\n",
    "                        rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:, 0]\n",
    "                else:\n",
    "                    keys = [k for k in inputs[0] if k not in [\"prompt\", \"completion\"]]\n",
    "                    reward_kwargs = {k: [ex[k] for ex in inputs] for k in keys}\n",
    "                    output_reward_func = reward_func(prompts=prompts, completions=completions, **reward_kwargs)\n",
    "                    rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
    "\n",
    "        # gather the reward\n",
    "        rewards_per_func = gather(rewards_per_func)\n",
    "        # apply weighting\n",
    "        device_rw = self.reward_weights.to(device).unsqueeze(0)\n",
    "        rewards = (rewards_per_func * device_rw).sum(dim=1)\n",
    "\n",
    "        # group wise\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n",
    "\n",
    "        process_slice = slice(\n",
    "            self.accelerator.process_index * len(prompts),\n",
    "            (self.accelerator.process_index + 1) * len(prompts),\n",
    "        )\n",
    "        advantages = advantages[process_slice]\n",
    "\n",
    "        # logging\n",
    "        mode = \"eval\" if self.control.should_evaluate else \"train\"\n",
    "        completion_length = self.accelerator.gather_for_metrics(completion_mask.sum(1)).float().mean().item()\n",
    "        self._metrics[mode][\"completion_length\"].append(completion_length)\n",
    "\n",
    "        reward_per_func = rewards_per_func.mean(0)\n",
    "        for i, reward_func in enumerate(self.reward_funcs):\n",
    "            if isinstance(reward_func, nn.Module):\n",
    "                rname = reward_func.config._name_or_path.split(\"/\")[-1]\n",
    "            else:\n",
    "                rname = reward_func.__name__\n",
    "            self._metrics[mode][f\"rewards/{rname}\"].append(reward_per_func[i].item())\n",
    "\n",
    "        self._metrics[mode][\"reward\"].append(rewards.mean().item())\n",
    "        self._metrics[mode][\"reward_std\"].append(std_grouped_rewards.mean().item())\n",
    "\n",
    "        if self.log_completions and self.state.global_step % self.args.logging_steps == 0:\n",
    "            prompts_to_log = gather_object(prompts_text)\n",
    "            completions_to_log = gather_object(completions_text)\n",
    "            rewards_to_log = rewards.tolist()\n",
    "            if self.accelerator.is_main_process:\n",
    "                if is_rich_available():\n",
    "                    print_prompt_completions_sample(\n",
    "                        prompts_to_log,\n",
    "                        completions_to_log,\n",
    "                        rewards_to_log,\n",
    "                        self.state.global_step,\n",
    "                    )\n",
    "                if self.args.report_to and \"wandb\" in self.args.report_to and wandb.run is not None:\n",
    "                    import pandas as pd\n",
    "                    table = {\n",
    "                        \"step\": [str(self.state.global_step)] * len(rewards),\n",
    "                        \"prompt\": prompts_to_log,\n",
    "                        \"completion\": completions_to_log,\n",
    "                        \"reward\": rewards_to_log,\n",
    "                    }\n",
    "                    df = pd.DataFrame(table)\n",
    "                    wandb.log({\"completions\": wandb.Table(dataframe=df)})\n",
    "\n",
    "        return {\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"prompt_mask\": prompt_mask,\n",
    "            \"completion_ids\": completion_ids,\n",
    "            \"completion_mask\": completion_mask,\n",
    "            \"old_per_token_logps\": old_per_token_logps,\n",
    "            \"ref_per_token_logps\": ref_per_token_logps,\n",
    "            \"advantages\": advantages,\n",
    "        }\n",
    "\n",
    "    @profiling_decorator\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if return_outputs:\n",
    "            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n",
    "\n",
    "        prompt_ids, prompt_mask = inputs[\"prompt_ids\"], inputs[\"prompt_mask\"]\n",
    "        completion_ids, completion_mask = inputs[\"completion_ids\"], inputs[\"completion_mask\"]\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "\n",
    "        per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)\n",
    "\n",
    "        if self.beta != 0.0:\n",
    "            ref_per_token_logps = inputs[\"ref_per_token_logps\"]\n",
    "            per_token_kl = (\n",
    "                torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1\n",
    "            )\n",
    "\n",
    "        advantages = inputs[\"advantages\"]\n",
    "        old_per_token_logps = inputs[\"old_per_token_logps\"] if self.num_iterations > 1 else per_token_logps.detach()\n",
    "\n",
    "        coef_1 = torch.exp(per_token_logps - old_per_token_logps)\n",
    "        coef_2 = torch.clamp(coef_1, 1 - self.epsilon, 1 + self.epsilon)\n",
    "        per_token_loss1 = coef_1 * advantages.unsqueeze(1)\n",
    "        per_token_loss2 = coef_2 * advantages.unsqueeze(1)\n",
    "        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)\n",
    "        if self.beta != 0.0:\n",
    "            per_token_loss = per_token_loss + self.beta * per_token_kl\n",
    "        loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()\n",
    "\n",
    "        mode = \"eval\" if self.control.should_evaluate else \"train\"\n",
    "        if self.beta != 0.0:\n",
    "            mean_kl = ((per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "            self._metrics[mode][\"kl\"].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())\n",
    "\n",
    "        is_clipped = (per_token_loss1 < per_token_loss2).float()\n",
    "        clip_ratio = (is_clipped * completion_mask).sum() / completion_mask.sum()\n",
    "        self._metrics[mode][\"clip_ratio\"].append(self.accelerator.gather_for_metrics(clip_ratio).mean().item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys: Optional[list[str]] = None):\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            with self.compute_loss_context_manager():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "            loss = loss.mean().detach()\n",
    "        return loss, None, None\n",
    "\n",
    "    def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> None:\n",
    "        mode = \"eval\" if self.control.should_evaluate else \"train\"\n",
    "        metrics = {key: sum(val) / len(val) for key, val in self._metrics[mode].items()}\n",
    "        if mode == \"eval\":\n",
    "            metrics = {f\"eval_{key}\": val for key, val in metrics.items()}\n",
    "\n",
    "        logs = {**logs, **metrics}\n",
    "        if version.parse(transformers.__version__) >= version.parse(\"4.47.0.dev0\"):\n",
    "            super().log(logs, start_time)\n",
    "        else:\n",
    "            super().log(logs)\n",
    "        self._metrics[mode].clear()\n",
    "\n",
    "    def create_model_card(\n",
    "        self,\n",
    "        model_name: Optional[str] = None,\n",
    "        dataset_name: Optional[str] = None,\n",
    "        tags: Union[str, list[str], None] = None,\n",
    "    ):\n",
    "        if not self.is_world_process_zero():\n",
    "            return\n",
    "\n",
    "        if hasattr(self.model.config, \"_name_or_path\") and not os.path.isdir(self.model.config._name_or_path):\n",
    "            base_model = self.model.config._name_or_path\n",
    "        else:\n",
    "            base_model = None\n",
    "\n",
    "        tags = tags or []\n",
    "        if isinstance(tags, str):\n",
    "            tags = [tags]\n",
    "\n",
    "        if hasattr(self.model.config, \"unsloth_version\"):\n",
    "            tags.append(\"unsloth\")\n",
    "\n",
    "        citation = textwrap.dedent(\n",
    "            \"\"\"\\\n",
    "            @article{zhihong2024deepseekmath,\n",
    "                title        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},\n",
    "                author       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},\n",
    "                year         = 2024,\n",
    "                eprint       = {arXiv:2402.03300},\n",
    "            }\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        model_card = generate_model_card(\n",
    "            base_model=base_model,\n",
    "            model_name=model_name,\n",
    "            hub_model_id=getattr(self, \"hub_model_id\", None),\n",
    "            dataset_name=dataset_name,\n",
    "            tags=tags,\n",
    "            wandb_url=wandb.run.get_url() if is_wandb_available() and wandb.run is not None else None,\n",
    "            comet_url=get_comet_experiment_url(),\n",
    "            trainer_name=\"GRPO (NoPop)\",\n",
    "            trainer_citation=citation,\n",
    "            paper_title=\"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\",\n",
    "            paper_id=\"2402.03300\",\n",
    "        )\n",
    "\n",
    "        model_card.save(os.path.join(self.args.output_dir, \"README.md\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
