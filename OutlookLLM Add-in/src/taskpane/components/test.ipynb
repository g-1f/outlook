{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset(self, query_tensors, response_tensors, rewards):\n",
    "    \"\"\"Create a dataset for PPO training from experience\n",
    "    \n",
    "    Handles tensors of any shape and ensures proper conversion to strings\n",
    "    \n",
    "    Parameters:\n",
    "    - query_tensors: List of query tensors (any shape)\n",
    "    - response_tensors: List of response tensors (any shape)\n",
    "    - rewards: List of reward values\n",
    "    \n",
    "    Returns:\n",
    "    - Dataset object ready for PPO training\n",
    "    \"\"\"\n",
    "    # Format data for the dataset\n",
    "    formatted_data = {\n",
    "        \"prompt\": [],\n",
    "        \"response\": [],\n",
    "        \"reward\": []\n",
    "    }\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"Creating dataset from {len(query_tensors)} experiences\")\n",
    "    if len(query_tensors) > 0:\n",
    "        print(f\"Sample query tensor shape: {query_tensors[0].shape}\")\n",
    "        print(f\"Sample response tensor shape: {response_tensors[0].shape}\")\n",
    "    \n",
    "    # Process each example\n",
    "    for i in range(len(query_tensors)):\n",
    "        try:\n",
    "            # Get individual tensors\n",
    "            query = query_tensors[i]\n",
    "            response = response_tensors[i]\n",
    "            \n",
    "            # Handle any tensor shape by flattening to 1D if needed\n",
    "            # We need to ensure we're working with 1D tensors\n",
    "            if len(query.shape) > 1:\n",
    "                # For tensors with shape [1, 1, sequence_length]\n",
    "                if len(query.shape) == 3:\n",
    "                    query = query.squeeze()  # Remove extra dimensions\n",
    "                # For tensors with shape [1, sequence_length]\n",
    "                elif len(query.shape) == 2:\n",
    "                    query = query.squeeze(0)  # Remove batch dimension\n",
    "            \n",
    "            # Same for response tensor\n",
    "            if len(response.shape) > 1:\n",
    "                if len(response.shape) == 3:\n",
    "                    response = response.squeeze()\n",
    "                elif len(response.shape) == 2:\n",
    "                    response = response.squeeze(0)\n",
    "            \n",
    "            # Get the prompt text (full query)\n",
    "            prompt_text = self.tokenizer.decode(query, skip_special_tokens=True)\n",
    "            \n",
    "            # For the response, we need only the part after the prompt\n",
    "            full_response_text = self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "            \n",
    "            # Extract response portion by removing the prompt text prefix\n",
    "            # This handles token ID conversion issues better than token-based slicing\n",
    "            if full_response_text.startswith(prompt_text):\n",
    "                response_text = full_response_text[len(prompt_text):]\n",
    "            else:\n",
    "                # If we can't find the exact prefix, use the full response\n",
    "                # (better than empty response for training)\n",
    "                response_text = full_response_text\n",
    "            \n",
    "            # Add to the formatted data if both texts are non-empty\n",
    "            if prompt_text and response_text:\n",
    "                formatted_data[\"prompt\"].append(prompt_text)\n",
    "                formatted_data[\"response\"].append(response_text)\n",
    "                \n",
    "                # Get the corresponding reward\n",
    "                if i < len(rewards):\n",
    "                    reward_value = float(rewards[i])\n",
    "                else:\n",
    "                    reward_value = 0.0\n",
    "                formatted_data[\"reward\"].append(reward_value)\n",
    "            else:\n",
    "                print(f\"Skipping example {i} - empty prompt or response\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing experience {i}: {e}\")\n",
    "            # Skip this example\n",
    "    \n",
    "    # Make sure we have at least some data\n",
    "    if len(formatted_data[\"prompt\"]) == 0:\n",
    "        print(\"Warning: No valid examples found. Creating dummy data.\")\n",
    "        formatted_data[\"prompt\"] = [\"dummy prompt\"] * 2\n",
    "        formatted_data[\"response\"] = [\"dummy response\"] * 2\n",
    "        formatted_data[\"reward\"] = [0.0] * 2\n",
    "    \n",
    "    # Create a dataset from the formatted data\n",
    "    dataset = Dataset.from_dict(formatted_data)\n",
    "    print(f\"Created dataset with {len(dataset)} examples\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, state: Dict) -> Tuple[float, str, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate trading decision based on current state\n",
    "    \n",
    "    Returns consistent tensor shapes for PPO training\n",
    "    \n",
    "    Parameters:\n",
    "    - state: Current environment state\n",
    "    \n",
    "    Returns:\n",
    "    - position: Float value representing the position (-1 to 1)\n",
    "    - response: Decoded text of the response\n",
    "    - query_tensor: Token IDs of the query \n",
    "    - response_tensor: Token IDs of the full response (including query)\n",
    "    \"\"\"\n",
    "    # Format the state into a prompt\n",
    "    prompt = self.format_state(state)\n",
    "    \n",
    "    # Tokenize the input \n",
    "    inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.policy_model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = self.policy_model.generate(\n",
    "                inputs.input_ids,\n",
    "                **self.generation_kwargs\n",
    "            )\n",
    "        \n",
    "        # Decode response (everything after the prompt)\n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_text = full_response[len(prompt):]\n",
    "        \n",
    "        # Extract positioning value\n",
    "        position = self.extract_positioning(full_response)\n",
    "        position = np.clip(position, -1.0, 1.0)\n",
    "        \n",
    "        # Ensure we're returning 2D tensors (batch, sequence) for consistency\n",
    "        query_tensor = inputs.input_ids  # Already has shape [1, sequence_length]\n",
    "        response_tensor = outputs        # Should have shape [1, sequence_length]\n",
    "        \n",
    "        # Check and fix tensor shapes for debugging\n",
    "        if len(query_tensor.shape) != 2:\n",
    "            print(f\"Warning: query_tensor has unexpected shape {query_tensor.shape}\")\n",
    "            query_tensor = query_tensor.view(1, -1)  # Reshape to [1, sequence_length]\n",
    "            \n",
    "        if len(response_tensor.shape) != 2:\n",
    "            print(f\"Warning: response_tensor has unexpected shape {response_tensor.shape}\")\n",
    "            response_tensor = response_tensor.view(1, -1)  # Reshape to [1, sequence_length]\n",
    "        \n",
    "        return position, response_text, query_tensor, response_tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Return safe defaults\n",
    "        return 0.0, \"\", inputs.input_ids, inputs.input_ids  # Return same tensor as both query and response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
