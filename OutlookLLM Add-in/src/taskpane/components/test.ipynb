{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import Dataset\n",
    "from accelerate import PartialState\n",
    "\n",
    "# Set up logging\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "log_file = f\"logs/training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "rewards_file = f\"logs/rewards_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"GRPO-Trader\")\n",
    "\n",
    "# Keep your original system prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "You must respond in the following XML format:\n",
    "\n",
    "<macro state>\n",
    "...\n",
    "</macro state>\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<positioning>\n",
    "...\n",
    "</positioning>\n",
    "\"\"\"\n",
    "\n",
    "def is_main_process():\n",
    "    \"\"\"Check if current process is the main process\"\"\"\n",
    "    try:\n",
    "        state = PartialState()\n",
    "        return state.process_index == 0\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "def log_reward(data):\n",
    "    \"\"\"Log rewards from main process only\"\"\"\n",
    "    if not is_main_process():\n",
    "        return\n",
    "        \n",
    "    with open(rewards_file, 'a') as f:\n",
    "        f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare training data from DataFrame\"\"\"\n",
    "    if is_main_process():\n",
    "        logger.info(\"Preparing training data...\")\n",
    "    \n",
    "    def prepare_prompt(df):\n",
    "        df['prompt'] = df.apply(lambda row: [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT.strip()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Market Context:{', '.join(f'{k}:{v}' for k, v in row.drop('date', errors='ignore').items())}\"\n",
    "            }\n",
    "        ], axis=1)\n",
    "        return df\n",
    "\n",
    "    df = prepare_prompt(df)\n",
    "    df['returns'] = df['close'].pct_change().shift(-1)\n",
    "    train_dataset = df[['prompt', 'returns']]\n",
    "    data = Dataset.from_pandas(train_dataset, preserve_index=False)\n",
    "    \n",
    "    if is_main_process():\n",
    "        logger.info(f\"Prepared {len(data)} training examples\")\n",
    "    return data\n",
    "\n",
    "def extract_positioning(text):\n",
    "    \"\"\"Extract the positioning value from the XML format\"\"\"\n",
    "    try:\n",
    "        match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            value = match.group(1).strip()\n",
    "            return float(value)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        if is_main_process():\n",
    "            logger.error(f\"Error extracting positioning: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def format_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if completions follow the expected format\"\"\"\n",
    "    # Get step information if available\n",
    "    step = kwargs.get(\"step\", 0)\n",
    "    \n",
    "    # Fixed pattern with DOTALL flag to match across newlines\n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    \n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    # Log sample completions periodically (main process only)\n",
    "    if is_main_process() and step % 20 == 0 and len(completion_contents) > 0:\n",
    "        sample_idx = min(2, len(completion_contents) - 1)\n",
    "        sample = completion_contents[sample_idx]\n",
    "        logger.info(f\"Step {step} - Sample completion:\\n{sample[:200]}...\")\n",
    "    \n",
    "    # Check format with proper regex (using re.DOTALL)\n",
    "    matches = [re.search(pattern, content, re.DOTALL) is not None for content in completion_contents]\n",
    "    rewards = [3.0 if match else -1.0 for match in matches]\n",
    "    \n",
    "    # Log format match rate (main process only)\n",
    "    if is_main_process() and matches:\n",
    "        match_rate = sum(matches) / len(matches)\n",
    "        logger.info(f\"Step {step} - Format match rate: {match_rate:.2f}\")\n",
    "        log_reward({\n",
    "            \"step\": step,\n",
    "            \"time\": time.time(),\n",
    "            \"type\": \"format\",\n",
    "            \"match_rate\": match_rate,\n",
    "            \"avg_reward\": sum(rewards) / len(rewards)\n",
    "        })\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "def return_reward(prompts, completions, returns, **kwargs):\n",
    "    \"\"\"Return-based reward function\"\"\"\n",
    "    step = kwargs.get(\"step\", 0)\n",
    "    \n",
    "    rewards = []\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    # Track valid positioning values\n",
    "    positions = []\n",
    "    \n",
    "    for i, completion in enumerate(completion_contents):\n",
    "        try:\n",
    "            position = extract_positioning(completion)\n",
    "            positions.append(position)\n",
    "            reward = position * returns[i % len(returns)]\n",
    "            rewards.append(reward)\n",
    "        except Exception as e:\n",
    "            if is_main_process():\n",
    "                logger.error(f\"Error in return_reward: {e}\")\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    # Log stats about positions and rewards (main process only)\n",
    "    if is_main_process() and len(positions) > 0:\n",
    "        avg_position = sum(positions) / len(positions)\n",
    "        avg_reward = sum(rewards) / len(rewards)\n",
    "        logger.info(f\"Step {step} - Avg position: {avg_position:.2f}, Avg reward: {avg_reward:.2f}\")\n",
    "        log_reward({\n",
    "            \"step\": step,\n",
    "            \"time\": time.time(),\n",
    "            \"type\": \"returns\",\n",
    "            \"avg_position\": avg_position,\n",
    "            \"avg_reward\": avg_reward\n",
    "        })\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "def main():\n",
    "    if is_main_process():\n",
    "        logger.info(\"Starting GRPO Trader training with multi-GPU support\")\n",
    "    \n",
    "    # Model and output configuration\n",
    "    model_path = './huggingface_mirror/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775'\n",
    "    output_dir = \"outputs/Qwen-2.5-0.5B-GRPO-trader\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    if is_main_process():\n",
    "        logger.info(\"Loading dataset\")\n",
    "    train_df = pd.read_csv('../train.csv')\n",
    "    data = prepare_data(train_df)\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    if is_main_process():\n",
    "        logger.info(f\"Loading model from {model_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )  # Don't manually move to CUDA - accelerate will handle device placement\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        padding_side=\"left\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Set pad token if needed\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # GRPO Configuration for multi-GPU\n",
    "    grpo_config = GRPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        run_name=\"Qwen-2.5-0.5B-GRPO-trader\",\n",
    "        learning_rate=5e-6,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        logging_steps=10,\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=2,  # This is PER GPU\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_generations=4,  # Crucial for multi-GPU: ensure num_generations divides global batch size\n",
    "        max_prompt_length=768,\n",
    "        max_completion_length=256,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=100,\n",
    "        max_grad_norm=0.1,\n",
    "        temperature=0.7,\n",
    "        num_iterations=2,\n",
    "        reward_weights=[1.0, 1.0],\n",
    "        # Multi-GPU settings\n",
    "        ddp_find_unused_parameters=False,\n",
    "        dataloader_num_workers=4,\n",
    "        # Disable VLLM for multi-GPU - not compatible with DDP\n",
    "        use_vllm=False\n",
    "    )\n",
    "    \n",
    "    # Store current global step for logging\n",
    "    current_step = [0]\n",
    "    \n",
    "    # Create wrapped reward functions with step info\n",
    "    def format_reward_with_step(prompts, completions, **kwargs):\n",
    "        return format_reward_func(prompts, completions, step=current_step[0], **kwargs)\n",
    "    \n",
    "    def return_reward_with_step(prompts, completions, returns, **kwargs):\n",
    "        return return_reward(prompts, completions, returns, step=current_step[0], **kwargs)\n",
    "    \n",
    "    # Initialize trainer with multi-GPU settings\n",
    "    if is_main_process():\n",
    "        logger.info(\"Initializing trainer with multi-GPU support\")\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[\n",
    "            format_reward_with_step,\n",
    "            return_reward_with_step\n",
    "        ],\n",
    "        args=grpo_config,\n",
    "        train_dataset=data,\n",
    "    )\n",
    "    \n",
    "    # Update global step during training using a TrainerCallback\n",
    "    class StepTracker(TrainerCallback):\n",
    "        def on_step_end(self, args, state, control, **kwargs):\n",
    "            current_step[0] = state.global_step\n",
    "    \n",
    "    trainer.add_callback(StepTracker())\n",
    "    \n",
    "    # Start training\n",
    "    if is_main_process():\n",
    "        logger.info(\"Starting training\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        if is_main_process():\n",
    "            logger.info(\"Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        if is_main_process():\n",
    "            logger.error(f\"Training failed with error: {str(e)}\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
