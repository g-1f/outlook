{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_step(self, query_tensors, response_tensors, rewards):\n",
    "    \"\"\"Run PPO optimization step\n",
    "    \n",
    "    Parameters:\n",
    "    - query_tensors: List of tensors for queries\n",
    "    - response_tensors: List of tensors for responses\n",
    "    - rewards: List of reward values\n",
    "    \n",
    "    Returns:\n",
    "    - Statistics from the PPO update\n",
    "    \"\"\"\n",
    "    if self.trainer is None:\n",
    "        self.initialize_trainer()\n",
    "    \n",
    "    # Format inputs for PPO trainer\n",
    "    texts = []\n",
    "    for i in range(len(query_tensors)):\n",
    "        query = query_tensors[i]\n",
    "        response = response_tensors[i]\n",
    "        \n",
    "        # Make sure we're working with individual tensors, not batches\n",
    "        if len(query.shape) > 1 and query.shape[0] > 1:\n",
    "            # Handle batched tensors - we'll process each item separately\n",
    "            for j in range(query.shape[0]):\n",
    "                prompt_text = self.tokenizer.decode(query[j], skip_special_tokens=True)\n",
    "                # Calculate where the response starts in the full sequence\n",
    "                query_length = query[j].shape[0]\n",
    "                response_text = self.tokenizer.decode(\n",
    "                    response[j][query_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                texts.append({\n",
    "                    \"prompt\": prompt_text,\n",
    "                    \"response\": response_text,\n",
    "                })\n",
    "        else:\n",
    "            # Handle single tensors\n",
    "            prompt_text = self.tokenizer.decode(query, skip_special_tokens=True)\n",
    "            query_length = query.shape[0]\n",
    "            response_text = self.tokenizer.decode(\n",
    "                response[query_length:], \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            texts.append({\n",
    "                \"prompt\": prompt_text,\n",
    "                \"response\": response_text,\n",
    "            })\n",
    "    \n",
    "    # Ensure rewards match the number of texts\n",
    "    if isinstance(rewards, list) and len(rewards) != len(texts):\n",
    "        # If lengths don't match, we need to expand the rewards\n",
    "        if len(rewards) == 1:\n",
    "            # If we have a single reward, duplicate it\n",
    "            rewards = [rewards[0]] * len(texts)\n",
    "        else:\n",
    "            # Otherwise, we need to handle this mismatch more carefully\n",
    "            print(f\"Warning: Number of rewards ({len(rewards)}) doesn't match number of texts ({len(texts)})\")\n",
    "            # Simple approach: truncate or pad with the last value\n",
    "            if len(rewards) < len(texts):\n",
    "                last_reward = rewards[-1]\n",
    "                rewards = rewards + [last_reward] * (len(texts) - len(rewards))\n",
    "            else:\n",
    "                rewards = rewards[:len(texts)]\n",
    "    \n",
    "    # Log what we're passing to the PPO trainer\n",
    "    print(f\"Running PPO step with {len(texts)} text pairs and {len(rewards) if isinstance(rewards, list) else 'tensor'} rewards\")\n",
    "    \n",
    "    # Run PPO step with formatted texts and rewards\n",
    "    try:\n",
    "        stats = self.trainer.step(texts, rewards)\n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error in PPO step: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_step(self, query_tensors, response_tensors, rewards):\n",
    "    \"\"\"Run PPO optimization step with proper formatting for TRL PPOTrainer\n",
    "    \n",
    "    This method is specifically formatted to match the exact expectations\n",
    "    of Hugging Face's PPOTrainer.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_tensors: List of query tensors\n",
    "    - response_tensors: List of response tensors\n",
    "    - rewards: List of reward values\n",
    "    \n",
    "    Returns:\n",
    "    - Statistics from the PPO update\n",
    "    \"\"\"\n",
    "    if self.trainer is None:\n",
    "        self.initialize_trainer()\n",
    "    \n",
    "    try:\n",
    "        # Format inputs exactly as PPOTrainer expects\n",
    "        # PPOTrainer wants a list of dicts with \"prompt\" and \"response\" keys\n",
    "        texts = []\n",
    "        \n",
    "        for i in range(len(query_tensors)):\n",
    "            query = query_tensors[i]\n",
    "            response = response_tensors[i]\n",
    "            \n",
    "            # Remove batch dimension if present\n",
    "            if len(query.shape) > 1:\n",
    "                query = query.squeeze(0)\n",
    "            if len(response.shape) > 1:\n",
    "                response = response.squeeze(0)\n",
    "            \n",
    "            # Get the prompt text (full query)\n",
    "            prompt_text = self.tokenizer.decode(query, skip_special_tokens=True)\n",
    "            \n",
    "            # For the response, we need only the part after the prompt\n",
    "            # Determine the length of the query (in tokens)\n",
    "            query_length = query.shape[0]\n",
    "            \n",
    "            # Extract only the response portion (everything after the query)\n",
    "            if response.shape[0] > query_length:\n",
    "                response_only = response[query_length:]\n",
    "                response_text = self.tokenizer.decode(response_only, skip_special_tokens=True)\n",
    "            else:\n",
    "                # Handle case where response doesn't extend beyond query\n",
    "                response_text = \"\"\n",
    "            \n",
    "            # Add to the formatted texts\n",
    "            texts.append({\n",
    "                \"prompt\": prompt_text,\n",
    "                \"response\": response_text\n",
    "            })\n",
    "            \n",
    "        # Ensure rewards are a flat list of floats matching the texts\n",
    "        if isinstance(rewards, list):\n",
    "            # Convert any non-float rewards to floats\n",
    "            rewards = [float(r) for r in rewards]\n",
    "            \n",
    "            # Make sure lengths match\n",
    "            if len(rewards) != len(texts):\n",
    "                print(f\"Warning: rewards length ({len(rewards)}) doesn't match texts length ({len(texts)})\")\n",
    "                if len(rewards) > len(texts):\n",
    "                    rewards = rewards[:len(texts)]\n",
    "                else:\n",
    "                    last_reward = rewards[-1] if rewards else 0.0\n",
    "                    rewards = rewards + [last_reward] * (len(texts) - len(rewards))\n",
    "        else:\n",
    "            # Try to convert tensor to list if needed\n",
    "            try:\n",
    "                rewards_list = rewards.flatten().tolist()\n",
    "                rewards = rewards_list[:len(texts)] if len(rewards_list) >= len(texts) else rewards_list + [0.0] * (len(texts) - len(rewards_list))\n",
    "            except:\n",
    "                print(\"Warning: Could not process rewards properly\")\n",
    "                rewards = [0.0] * len(texts)\n",
    "        \n",
    "        print(f\"Running PPO step with {len(texts)} text pairs\")\n",
    "        \n",
    "        # Run the PPO step with properly formatted inputs\n",
    "        stats = self.trainer.step(texts, rewards)\n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in PPO step: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Print sample inputs for debugging\n",
    "        if len(query_tensors) > 0:\n",
    "            print(\"\\nSample query tensor shape:\", query_tensors[0].shape)\n",
    "        if len(response_tensors) > 0:\n",
    "            print(\"Sample response tensor shape:\", response_tensors[0].shape)\n",
    "        print(\"Rewards type:\", type(rewards))\n",
    "        if isinstance(rewards, list) and len(rewards) > 0:\n",
    "            print(\"Sample reward:\", rewards[0])\n",
    "            \n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
