{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "import copy\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from collections import deque\n",
    "from datasets import Dataset\n",
    "import re\n",
    "from trl.data_utils import is_conversational\n",
    "\n",
    "# Instead of monkey patching, we'll create a subclass\n",
    "class FixedGRPOTrainer(GRPOTrainer):\n",
    "    \"\"\"\n",
    "    A subclass of GRPOTrainer that ensures prompts aren't modified during training.\n",
    "    \"\"\"\n",
    "    def _prepare_inputs(self, inputs):\n",
    "        \"\"\"\n",
    "        Create deep copies of the prompts to prevent modification.\n",
    "        This wraps the standard _prepare_inputs method.\n",
    "        \"\"\"\n",
    "        # Create deep copies of the prompts for conversational data\n",
    "        if inputs and \"prompt\" in inputs[0] and is_conversational(inputs[0]):\n",
    "            # Make copies to avoid modifying originals\n",
    "            copied_inputs = []\n",
    "            for inp in inputs:\n",
    "                copied_inp = copy.deepcopy(inp)\n",
    "                copied_inputs.append(copied_inp)\n",
    "            \n",
    "            # Use the copies for processing\n",
    "            result = super()._prepare_inputs(copied_inputs)\n",
    "            return result\n",
    "        \n",
    "        # For non-conversational data, use the standard method\n",
    "        return super()._prepare_inputs(inputs)\n",
    "\n",
    "# System prompt for the portfolio manager\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a macro event driven portfolio manager, you make positioning decision of S&P500 index based on market context and news.\n",
    "\n",
    "Only edit macro state if there are changes in the macro regime that would impact returns of S&P500.\n",
    "\n",
    "Positioning should be a float that ranges from -1 (full short) to 1 (full long).\n",
    "\n",
    "Example:\n",
    "Market Context: PMI:52.6, inflation:5.1%, unemployment:3.8%\n",
    "\n",
    "<macro state>\n",
    "Economy is showing strength with PMI in expansion territory, but inflation remains elevated.\n",
    "</macro state>\n",
    "<reasoning>\n",
    "The PMI at 52.6 indicates expansion, which is positive for equities. However, inflation at 5.1% is above the Fed's target, suggesting possible rate hikes. Employment is strong at 3.8%, supporting consumer spending.\n",
    "</reasoning>\n",
    "<positioning>\n",
    "0.3\n",
    "</positioning>\n",
    "\n",
    "You must respond in the above XML format.\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data(df):\n",
    "    def prepare_prompt(df):\n",
    "        df['prompt'] = df.apply(lambda row: [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT.strip()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Market Context:{', '.join(f'{k}:{v}' for k, v in row.drop('date', errors='ignore').items())}\"\n",
    "            }\n",
    "        ], axis=1)\n",
    "        return df\n",
    "\n",
    "    df = prepare_prompt(df)\n",
    "    df['returns'] = df['close'].pct_change().shift(-1)\n",
    "    train_dataset = df[['prompt', 'returns']]\n",
    "    data = Dataset.from_pandas(train_dataset, preserve_index=False)\n",
    "    return data\n",
    "\n",
    "def extract_positioning(text):\n",
    "    try:\n",
    "        match = re.search(r\"<positioning>(.*?)</positioning>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            # Extract the value and clean any whitespace\n",
    "            value = match.group(1).strip()\n",
    "            return float(value)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Positioning extraction error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def format_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    # Debug output for first few completions\n",
    "    for i, content in enumerate(completion_contents[:3]):\n",
    "        print(f\"Completion {i}: {content[:100]}...\")\n",
    "    \n",
    "    matches = [re.search(pattern, content, re.DOTALL) is not None for content in completion_contents]\n",
    "    # Stronger reward/penalty for format\n",
    "    return [3.0 if match else -2.0 for match in matches]\n",
    "\n",
    "def return_reward(prompts, completions, returns, **kwargs):\n",
    "    \"\"\"Main reward function combining multiple factors\"\"\"\n",
    "    rewards = []\n",
    "    completion_contents = [\n",
    "        completion[0][\"content\"] if isinstance(completion[0], dict) else completion[0] \n",
    "        for completion in completions\n",
    "    ]\n",
    "    \n",
    "    pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "    for i, completion in enumerate(completion_contents):\n",
    "        try:\n",
    "            position = extract_positioning(completion)\n",
    "            # Only reward correct format and positioning\n",
    "            if re.search(pattern, completion, re.DOTALL):\n",
    "                rewards.append(position * returns[i % len(returns)])\n",
    "            else:\n",
    "                rewards.append(-1.0)  # Penalty for incorrect format\n",
    "        except Exception as e:\n",
    "            print(f\"Reward calculation error: {e} for completion: {completion[:100]}...\")\n",
    "            rewards.append(-1.0)\n",
    "    return rewards\n",
    "\n",
    "# Load and prepare data\n",
    "train = pd.read_csv('../train.csv')\n",
    "data = prepare_data(train)\n",
    "\n",
    "# Model configuration\n",
    "model_path = './huggingface_mirror/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775'\n",
    "output_dir = \"outputs/Qwen-2.5-0.5B-GRPO-trader\"\n",
    "run_name = \"Qwen-2.5-0.5B-GRPO-trader\"\n",
    "\n",
    "# GRPO Configuration with increased lengths and more exploration\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=512,  # Increased from 256\n",
    "    max_completion_length=512,  # Increased from 256\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    temperature=0.7,  # Increased from 0.1 for better exploration\n",
    "    num_iterations=2,  # Now safe to use iterations > 1\n",
    "    reward_weights=[1.0, 1.0],  # Equal weight for format and returns\n",
    "    log_completions=True  # Enable logging completions for debugging\n",
    ")\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True  # Important for Qwen models\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True  # Important for Qwen models\n",
    ")\n",
    "\n",
    "# Ensure tokenizer has pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add simple callback to print sample completions at various steps\n",
    "class CompletionDebugCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 50 == 0:\n",
    "            print(f\"\\n=== Step {state.global_step} Sample Completions ===\")\n",
    "            # Generate a sample completion with the current model state\n",
    "            sample = data[0]\n",
    "            messages = sample['prompt']\n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            output = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            print(f\"Output: {output}\\n\")\n",
    "            \n",
    "            # Check if it matches the format\n",
    "            pattern = r\"<macro state>.*?</macro state>.*?<reasoning>.*?</reasoning>.*?<positioning>.*?</positioning>\"\n",
    "            has_format = re.search(pattern, output, re.DOTALL) is not None\n",
    "            print(f\"Has correct format: {has_format}\")\n",
    "            \n",
    "            # Try to extract positioning\n",
    "            try:\n",
    "                position = extract_positioning(output)\n",
    "                print(f\"Extracted positioning: {position}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract positioning: {e}\")\n",
    "\n",
    "# Use our fixed trainer subclass\n",
    "trainer = FixedGRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        format_reward_func,\n",
    "        return_reward\n",
    "    ],\n",
    "    args=grpo_config,\n",
    "    train_dataset=data,\n",
    ")\n",
    "\n",
    "# Add the callback\n",
    "trainer.add_callback(CompletionDebugCallback())\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
